<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>2023-2025年科研成果展示</title>
    <url>/2024/12/11/%E7%A7%91%E7%A0%94%E5%BF%AB%E8%AE%AF/</url>
    <content><![CDATA[<h2 id="ICCV2023-When-Noisy-Labels-Meet-Long-Tail-Dilemmas-A-Representation-Calibration-Method"><a href="#ICCV2023-When-Noisy-Labels-Meet-Long-Tail-Dilemmas-A-Representation-Calibration-Method" class="headerlink" title="ICCV2023: When Noisy Labels Meet Long Tail Dilemmas: A Representation Calibration Method"></a><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_When_Noisy_Labels_Meet_Long_Tail_Dilemmas_A_Representation_Calibration_ICCV_2023_paper.html?trk=public_post_comment-text">ICCV2023: When Noisy Labels Meet Long Tail Dilemmas: A Representation Calibration Method</a></h2><h3 id="Author-张曼怡"><a href="#Author-张曼怡" class="headerlink" title="Author: 张曼怡"></a>Author: 张曼怡</h3><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>Real-world large-scale datasets are both noisily labeled and class-imbalanced. The issues seriously hurt the generalization of trained models. It is hence significant to address the simultaneous incorrect labeling and class-imbalance, i.e., the problem of learning with noisy labels on long-tailed data. Previous works develop several methods for the problem. However, they always rely on strong assumptions that are invalid or hard to be checked in practice. In this paper, to handle the problem and address the limitations of prior works, we propose a representation calibration method RCAL. Specifically, RCAL works with the representations extracted by unsupervised contrastive learning. We assume that without incorrect labeling and class imbalance, the representations of instances in each class conform to a multivariate Gaussian distribution, which is much milder and easier to be checked. Based on the assumption, we recover underlying representation distributions from polluted ones resulting from mislabeled and class-imbalanced data. Additional data points are then sampled from the recovered distributions to help generalization. Moreover, during classifier training, representation learning takes advantage of representation robustness brought by contrastive learning, which further improves the classifier performance. We derive theoretical results to discuss the effectiveness of our representation calibration. Experiments on multiple benchmarks justify our claims and confirm the superiority of the proposed method.</p>
<p><img src="https://raw.githubusercontent.com/THU-CVML/thu-cvml.github.io/refs/heads/main/images/RCAL.png"></p>
<h2 id="AAAI2025-Aligning-Composed-Query-with-Image-via-Discriminative-Perception-from-Negative-Correspondences"><a href="#AAAI2025-Aligning-Composed-Query-with-Image-via-Discriminative-Perception-from-Negative-Correspondences" class="headerlink" title="AAAI2025: Aligning Composed Query with Image via Discriminative Perception from Negative Correspondences"></a><a href="">AAAI2025: Aligning Composed Query with Image via Discriminative Perception from Negative Correspondences</a></h2><h3 id="Author-王依凡"><a href="#Author-王依凡" class="headerlink" title="Author: 王依凡"></a>Author: 王依凡</h3><h3 id="Abstract-1"><a href="#Abstract-1" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="AAAI2025-Rethinking-Pseudo-Label-Guided-Learning-for-Weakly-Supervised-Temporal-Action-Localization-from-the-Perspective-of-Noise-Correction"><a href="#AAAI2025-Rethinking-Pseudo-Label-Guided-Learning-for-Weakly-Supervised-Temporal-Action-Localization-from-the-Perspective-of-Noise-Correction" class="headerlink" title="AAAI2025: Rethinking Pseudo-Label Guided Learning for Weakly-Supervised Temporal Action Localization from the Perspective of Noise Correction"></a><a href="">AAAI2025: Rethinking Pseudo-Label Guided Learning for Weakly-Supervised Temporal Action Localization from the Perspective of Noise Correction</a></h2><h3 id="Author-张权"><a href="#Author-张权" class="headerlink" title="Author: 张权"></a>Author: 张权</h3><h3 id="Abstract-2"><a href="#Abstract-2" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="NeurIPS2025-SuperVLAD-Compact-and-Robust-Image-Descriptors-for-Visual-Place-Recognition"><a href="#NeurIPS2025-SuperVLAD-Compact-and-Robust-Image-Descriptors-for-Visual-Place-Recognition" class="headerlink" title="NeurIPS2025: SuperVLAD: Compact and Robust Image Descriptors for Visual Place Recognition"></a><a href="">NeurIPS2025: SuperVLAD: Compact and Robust Image Descriptors for Visual Place Recognition</a></h2><h3 id="Author-卢锋"><a href="#Author-卢锋" class="headerlink" title="Author: 卢锋"></a>Author: 卢锋</h3><h3 id="Abstract-3"><a href="#Abstract-3" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="ICPR2024-Benchmarking-AI-in-Mental-Health-A-Critical-Examination-of-LLMs-Across-Key-Performance-and-Ethical-Metrics"><a href="#ICPR2024-Benchmarking-AI-in-Mental-Health-A-Critical-Examination-of-LLMs-Across-Key-Performance-and-Ethical-Metrics" class="headerlink" title="ICPR2024: Benchmarking AI in Mental Health: A Critical Examination of LLMs Across Key Performance and Ethical Metrics"></a><a href="">ICPR2024: Benchmarking AI in Mental Health: A Critical Examination of LLMs Across Key Performance and Ethical Metrics</a></h2><h3 id="Author-袁睿"><a href="#Author-袁睿" class="headerlink" title="Author: 袁睿"></a>Author: 袁睿</h3><h3 id="Abstract-4"><a href="#Abstract-4" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="ICPR2024-AMC-OA-Adaptive-Multi-Scale-Convolutional-Networks-with-Optimized-Attention-for-Temporal-Action-Localization"><a href="#ICPR2024-AMC-OA-Adaptive-Multi-Scale-Convolutional-Networks-with-Optimized-Attention-for-Temporal-Action-Localization" class="headerlink" title="ICPR2024: AMC-OA: Adaptive Multi-Scale Convolutional Networks with Optimized Attention for Temporal Action Localization"></a><a href="">ICPR2024: AMC-OA: Adaptive Multi-Scale Convolutional Networks with Optimized Attention for Temporal Action Localization</a></h2><h3 id="Author-袁睿-1"><a href="#Author-袁睿-1" class="headerlink" title="Author: 袁睿"></a>Author: 袁睿</h3><h3 id="Abstract-5"><a href="#Abstract-5" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="ACMMM2024-Semantic-Distillation-from-Neighborhood-for-Composed-Image-Retrieval"><a href="#ACMMM2024-Semantic-Distillation-from-Neighborhood-for-Composed-Image-Retrieval" class="headerlink" title="ACMMM2024: Semantic Distillation from Neighborhood for Composed Image Retrieval"></a><a href="">ACMMM2024: Semantic Distillation from Neighborhood for Composed Image Retrieval</a></h2><h3 id="Author-王依凡-1"><a href="#Author-王依凡-1" class="headerlink" title="Author: 王依凡"></a>Author: 王依凡</h3><h3 id="Abstract-6"><a href="#Abstract-6" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="ACM-MM2024-CustomNet-Object-Customization-with-Variable-Viewpoints-in-Text-to-Image-Diffusion-Models"><a href="#ACM-MM2024-CustomNet-Object-Customization-with-Variable-Viewpoints-in-Text-to-Image-Diffusion-Models" class="headerlink" title="ACM MM2024: CustomNet: Object Customization with Variable-Viewpoints in Text-to-Image Diffusion Models"></a><a href="">ACM MM2024: CustomNet: Object Customization with Variable-Viewpoints in Text-to-Image Diffusion Models</a></h2><h3 id="Author-袁梓洋"><a href="#Author-袁梓洋" class="headerlink" title="Author: 袁梓洋"></a>Author: 袁梓洋</h3><h3 id="Abstract-7"><a href="#Abstract-7" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="ECCV2024-GVGEN-Text-to-3D-Generation-with-Volumetric-Representation"><a href="#ECCV2024-GVGEN-Text-to-3D-Generation-with-Volumetric-Representation" class="headerlink" title="ECCV2024: GVGEN: Text-to-3D Generation with Volumetric Representation"></a><a href="">ECCV2024: GVGEN: Text-to-3D Generation with Volumetric Representation</a></h2><h3 id="Author-何相龙"><a href="#Author-何相龙" class="headerlink" title="Author: 何相龙"></a>Author: 何相龙</h3><h3 id="Abstract-8"><a href="#Abstract-8" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="ECCV2024-A-Task-is-Worth-One-Word-Learning-with-Task-Prompts-for-High-Quality-Versatile-Image-Inpainting"><a href="#ECCV2024-A-Task-is-Worth-One-Word-Learning-with-Task-Prompts-for-High-Quality-Versatile-Image-Inpainting" class="headerlink" title="ECCV2024: A Task is Worth One Word: Learning with Task Prompts for High-Quality Versatile Image Inpainting"></a><a href="">ECCV2024: A Task is Worth One Word: Learning with Task Prompts for High-Quality Versatile Image Inpainting</a></h2><h3 id="Author-庄俊豪"><a href="#Author-庄俊豪" class="headerlink" title="Author: 庄俊豪"></a>Author: 庄俊豪</h3><h3 id="Abstract-9"><a href="#Abstract-9" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="ECCV2024-DreamDiffusion-High-Quality-EEG-to-Image-Generation-with-Temporal-Masked-Signal-Modeling-and-CLIP-Alignment"><a href="#ECCV2024-DreamDiffusion-High-Quality-EEG-to-Image-Generation-with-Temporal-Masked-Signal-Modeling-and-CLIP-Alignment" class="headerlink" title="ECCV2024: DreamDiffusion: High-Quality EEG-to-Image Generation with Temporal Masked Signal Modeling and CLIP Alignment"></a><a href="">ECCV2024: DreamDiffusion: High-Quality EEG-to-Image Generation with Temporal Masked Signal Modeling and CLIP Alignment</a></h2><h3 id="Author-白云鹏"><a href="#Author-白云鹏" class="headerlink" title="Author: 白云鹏"></a>Author: 白云鹏</h3><h3 id="Abstract-10"><a href="#Abstract-10" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="ECCV2024-MirrorGaussian-Reflecting-3D-Gaussians-for-Reconstructing-Mirror-Reflections"><a href="#ECCV2024-MirrorGaussian-Reflecting-3D-Gaussians-for-Reconstructing-Mirror-Reflections" class="headerlink" title="ECCV2024: MirrorGaussian: Reflecting 3D Gaussians for Reconstructing Mirror Reflections"></a><a href="">ECCV2024: MirrorGaussian: Reflecting 3D Gaussians for Reconstructing Mirror Reflections</a></h2><h3 id="Author-刘佳月"><a href="#Author-刘佳月" class="headerlink" title="Author: 刘佳月"></a>Author: 刘佳月</h3><h3 id="Abstract-11"><a href="#Abstract-11" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="IEEE-TCSVT-Meta-Learning-without-Data-via-Unconditional-Diffusion-Models"><a href="#IEEE-TCSVT-Meta-Learning-without-Data-via-Unconditional-Diffusion-Models" class="headerlink" title="IEEE TCSVT: Meta-Learning without Data via Unconditional Diffusion Models"></a><a href="">IEEE TCSVT: Meta-Learning without Data via Unconditional Diffusion Models</a></h2><h3 id="Author-韦永贤"><a href="#Author-韦永贤" class="headerlink" title="Author: 韦永贤"></a>Author: 韦永贤</h3><h3 id="Abstract-12"><a href="#Abstract-12" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="ICML2024-CrossGET-Cross-Guided-Ensemble-of-Tokens-for-Accelerating-Vision-Language-Transformers"><a href="#ICML2024-CrossGET-Cross-Guided-Ensemble-of-Tokens-for-Accelerating-Vision-Language-Transformers" class="headerlink" title="ICML2024: CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers"></a><a href="">ICML2024: CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers</a></h2><h3 id="Author-石大川"><a href="#Author-石大川" class="headerlink" title="Author: 石大川"></a>Author: 石大川</h3><h3 id="Abstract-13"><a href="#Abstract-13" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="ICML2024-Sparse-Model-Inversion-Efficient-Inversion-of-Vision-Transformers-with-Less-Hallucination"><a href="#ICML2024-Sparse-Model-Inversion-Efficient-Inversion-of-Vision-Transformers-with-Less-Hallucination" class="headerlink" title="ICML2024: Sparse Model Inversion: Efficient Inversion of Vision Transformers with Less Hallucination"></a><a href="">ICML2024: Sparse Model Inversion: Efficient Inversion of Vision Transformers with Less Hallucination</a></h2><h3 id="Author-胡梓轩"><a href="#Author-胡梓轩" class="headerlink" title="Author: 胡梓轩"></a>Author: 胡梓轩</h3><h3 id="Abstract-14"><a href="#Abstract-14" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="ICML2024-Task-Groupings-Regularization-Data-Free-Meta-Learning-with-Heterogeneous-Pre-trained-Models"><a href="#ICML2024-Task-Groupings-Regularization-Data-Free-Meta-Learning-with-Heterogeneous-Pre-trained-Models" class="headerlink" title="ICML2024: Task Groupings Regularization: Data-Free Meta-Learning with Heterogeneous Pre-trained Models"></a><a href="">ICML2024: Task Groupings Regularization: Data-Free Meta-Learning with Heterogeneous Pre-trained Models</a></h2><h3 id="Author-韦永贤-1"><a href="#Author-韦永贤-1" class="headerlink" title="Author: 韦永贤"></a>Author: 韦永贤</h3><h3 id="Abstract-15"><a href="#Abstract-15" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="ICML2024-DFD-Distillng-the-Feature-Disparity-Differently-for-Detectors"><a href="#ICML2024-DFD-Distillng-the-Feature-Disparity-Differently-for-Detectors" class="headerlink" title="ICML2024: DFD: Distillng the Feature Disparity Differently for Detectors"></a><a href="">ICML2024: DFD: Distillng the Feature Disparity Differently for Detectors</a></h2><h3 id="Author-刘康"><a href="#Author-刘康" class="headerlink" title="Author: 刘康"></a>Author: 刘康</h3><h3 id="Abstract-16"><a href="#Abstract-16" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="IJCNN2024-Integrating-Local-Global-Features-for-Estimating-Shortest-Path-Distance-in-Large-Scale-Graphs"><a href="#IJCNN2024-Integrating-Local-Global-Features-for-Estimating-Shortest-Path-Distance-in-Large-Scale-Graphs" class="headerlink" title="IJCNN2024: Integrating Local &amp; Global Features for Estimating Shortest-Path Distance in Large-Scale Graphs"></a><a href="">IJCNN2024: Integrating Local &amp; Global Features for Estimating Shortest-Path Distance in Large-Scale Graphs</a></h2><h3 id="Author-王浩宇"><a href="#Author-王浩宇" class="headerlink" title="Author: 王浩宇"></a>Author: 王浩宇</h3><h3 id="Abstract-17"><a href="#Abstract-17" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="IJCNN2024-MMR-Multi-Scale-Motion-Retargeting-Between-Skeleton-Agnostic-Characters"><a href="#IJCNN2024-MMR-Multi-Scale-Motion-Retargeting-Between-Skeleton-Agnostic-Characters" class="headerlink" title="IJCNN2024: MMR: Multi-Scale Motion Retargeting Between Skeleton-Agnostic Characters"></a><a href="">IJCNN2024: MMR: Multi-Scale Motion Retargeting Between Skeleton-Agnostic Characters</a></h2><h3 id="Author-王浩宇-1"><a href="#Author-王浩宇-1" class="headerlink" title="Author: 王浩宇"></a>Author: 王浩宇</h3><h3 id="Abstract-18"><a href="#Abstract-18" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="IJCNN2024-Error-Bound-Based-Noise-Schedule-Design-in-Diffusion-Models"><a href="#IJCNN2024-Error-Bound-Based-Noise-Schedule-Design-in-Diffusion-Models" class="headerlink" title="IJCNN2024: Error Bound Based Noise Schedule Design in Diffusion Models"></a><a href="">IJCNN2024: Error Bound Based Noise Schedule Design in Diffusion Models</a></h2><h3 id="Author-刘力源"><a href="#Author-刘力源" class="headerlink" title="Author: 刘力源"></a>Author: 刘力源</h3><h3 id="Abstract-19"><a href="#Abstract-19" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="IJCNN2024-Noise-Weighting-Phased-Prompt-Image-Editing"><a href="#IJCNN2024-Noise-Weighting-Phased-Prompt-Image-Editing" class="headerlink" title="IJCNN2024: Noise Weighting Phased Prompt Image Editing"></a><a href="">IJCNN2024: Noise Weighting Phased Prompt Image Editing</a></h2><h3 id="Author-徐国炜"><a href="#Author-徐国炜" class="headerlink" title="Author: 徐国炜"></a>Author: 徐国炜</h3><h3 id="Abstract-20"><a href="#Abstract-20" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="CVPR2024-Distilling-Semantic-Priors-from-SAM-to-Efficient-Image-Restoration-Models"><a href="#CVPR2024-Distilling-Semantic-Priors-from-SAM-to-Efficient-Image-Restoration-Models" class="headerlink" title="CVPR2024: Distilling Semantic Priors from SAM to Efficient Image Restoration Models"></a><a href="">CVPR2024: Distilling Semantic Priors from SAM to Efficient Image Restoration Models</a></h2><h3 id="Author-张权-1"><a href="#Author-张权-1" class="headerlink" title="Author: 张权"></a>Author: 张权</h3><h3 id="Abstract-21"><a href="#Abstract-21" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="CVPR2024-CricaVPR-Cross-image-Correlation-aware-Representation-Learning-for-Visual-Place-Recognition"><a href="#CVPR2024-CricaVPR-Cross-image-Correlation-aware-Representation-Learning-for-Visual-Place-Recognition" class="headerlink" title="CVPR2024: CricaVPR: Cross-image Correlation-aware Representation Learning for Visual Place Recognition"></a><a href="">CVPR2024: CricaVPR: Cross-image Correlation-aware Representation Learning for Visual Place Recognition</a></h2><h3 id="Author-卢锋-1"><a href="#Author-卢锋-1" class="headerlink" title="Author: 卢锋"></a>Author: 卢锋</h3><h3 id="Abstract-22"><a href="#Abstract-22" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="CVPR2024-A-Free-Lunch-for-Faster-and-Better-Data-Free-Meta-Learning"><a href="#CVPR2024-A-Free-Lunch-for-Faster-and-Better-Data-Free-Meta-Learning" class="headerlink" title="CVPR2024: A Free Lunch for Faster and Better Data-Free Meta-Learning"></a><a href="">CVPR2024: A Free Lunch for Faster and Better Data-Free Meta-Learning</a></h2><h3 id="Author-韦永贤-2"><a href="#Author-韦永贤-2" class="headerlink" title="Author: 韦永贤"></a>Author: 韦永贤</h3><h3 id="Abstract-23"><a href="#Abstract-23" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="IEEE-T-MM-Negative-Sensitive-Framework-with-Semantic-Enhancement-for-Composed-Image-Retrieval"><a href="#IEEE-T-MM-Negative-Sensitive-Framework-with-Semantic-Enhancement-for-Composed-Image-Retrieval" class="headerlink" title="IEEE T-MM: Negative-Sensitive Framework with Semantic Enhancement for Composed Image Retrieval"></a><a href="">IEEE T-MM: Negative-Sensitive Framework with Semantic Enhancement for Composed Image Retrieval</a></h2><h3 id="Author-王依凡-2"><a href="#Author-王依凡-2" class="headerlink" title="Author: 王依凡"></a>Author: 王依凡</h3><h3 id="Abstract-24"><a href="#Abstract-24" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="ICLR2024-Convolution-Meets-LoRA-Parameter-Efficient-Finetuning-for-Segment-Anything-Model"><a href="#ICLR2024-Convolution-Meets-LoRA-Parameter-Efficient-Finetuning-for-Segment-Anything-Model" class="headerlink" title="ICLR2024: Convolution Meets LoRA: Parameter Efficient Finetuning for Segment Anything Model"></a><a href="">ICLR2024: Convolution Meets LoRA: Parameter Efficient Finetuning for Segment Anything Model</a></h2><h3 id="Author-钟子涵"><a href="#Author-钟子涵" class="headerlink" title="Author: 钟子涵"></a>Author: 钟子涵</h3><h3 id="Abstract-25"><a href="#Abstract-25" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="ICLR2024-Towards-Seamless-Adaptation-of-Pre-trained-Models-for-Visual-Place-Recognition"><a href="#ICLR2024-Towards-Seamless-Adaptation-of-Pre-trained-Models-for-Visual-Place-Recognition" class="headerlink" title="ICLR2024: Towards Seamless Adaptation of Pre-trained Models for Visual Place Recognition"></a><a href="">ICLR2024: Towards Seamless Adaptation of Pre-trained Models for Visual Place Recognition</a></h2><h3 id="Author-卢锋-2"><a href="#Author-卢锋-2" class="headerlink" title="Author: 卢锋"></a>Author: 卢锋</h3><h3 id="Abstract-26"><a href="#Abstract-26" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="AAAI2024-Efficient-Conditional-Diffusion-Model-with-Probability-Flow-Sampling-for-Image-Super-resolution"><a href="#AAAI2024-Efficient-Conditional-Diffusion-Model-with-Probability-Flow-Sampling-for-Image-Super-resolution" class="headerlink" title="AAAI2024: Efficient Conditional Diffusion Model with Probability Flow Sampling for Image Super-resolution"></a><a href="">AAAI2024: Efficient Conditional Diffusion Model with Probability Flow Sampling for Image Super-resolution</a></h2><h3 id="Author-袁宇韬"><a href="#Author-袁宇韬" class="headerlink" title="Author: 袁宇韬"></a>Author: 袁宇韬</h3><h3 id="Abstract-27"><a href="#Abstract-27" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="AAAI2024-Blind-Face-Restoration-under-Extreme-Conditions-Leveraging-3D-2D-Prior-Fusion-for-Superior-Structural-and-Texture-Recovery"><a href="#AAAI2024-Blind-Face-Restoration-under-Extreme-Conditions-Leveraging-3D-2D-Prior-Fusion-for-Superior-Structural-and-Texture-Recovery" class="headerlink" title="AAAI2024: Blind Face Restoration under Extreme Conditions: Leveraging 3D-2D Prior Fusion for Superior Structural and Texture Recovery"></a><a href="">AAAI2024: Blind Face Restoration under Extreme Conditions: Leveraging 3D-2D Prior Fusion for Superior Structural and Texture Recovery</a></h2><h3 id="Author-袁梓洋-1"><a href="#Author-袁梓洋-1" class="headerlink" title="Author: 袁梓洋"></a>Author: 袁梓洋</h3><h3 id="Abstract-28"><a href="#Abstract-28" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="AAAI2024-Mean-Teacher-DETR-with-Masked-Feature-Alignment-A-Robust-Domain-Adaptive-Detection-Transformer-Framework"><a href="#AAAI2024-Mean-Teacher-DETR-with-Masked-Feature-Alignment-A-Robust-Domain-Adaptive-Detection-Transformer-Framework" class="headerlink" title="AAAI2024: Mean Teacher DETR with Masked Feature Alignment: A Robust Domain Adaptive Detection Transformer Framework"></a><a href="">AAAI2024: Mean Teacher DETR with Masked Feature Alignment: A Robust Domain Adaptive Detection Transformer Framework</a></h2><h3 id="Author-翁玮熙"><a href="#Author-翁玮熙" class="headerlink" title="Author: 翁玮熙"></a>Author: 翁玮熙</h3><h3 id="Abstract-29"><a href="#Abstract-29" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="AAAI2024-Deep-Homography-Estimation-for-Visual-Place-Recognition"><a href="#AAAI2024-Deep-Homography-Estimation-for-Visual-Place-Recognition" class="headerlink" title="AAAI2024: Deep Homography Estimation for Visual Place Recognition"></a><a href="">AAAI2024: Deep Homography Estimation for Visual Place Recognition</a></h2><h3 id="Author-卢锋-3"><a href="#Author-卢锋-3" class="headerlink" title="Author: 卢锋"></a>Author: 卢锋</h3><h3 id="Abstract-30"><a href="#Abstract-30" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="IEEE-T-MM-Towards-Effective-Collaborative-Learning-in-Long-Tailed-Recognition"><a href="#IEEE-T-MM-Towards-Effective-Collaborative-Learning-in-Long-Tailed-Recognition" class="headerlink" title="IEEE T-MM: Towards Effective Collaborative Learning in Long-Tailed Recognition"></a><a href="">IEEE T-MM: Towards Effective Collaborative Learning in Long-Tailed Recognition</a></h2><h3 id="Author-许正卓"><a href="#Author-许正卓" class="headerlink" title="Author: 许正卓"></a>Author: 许正卓</h3><h3 id="Abstract-31"><a href="#Abstract-31" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="ACL2023-LET-Leveraging-Error-Type-Information-for-Grammatical-Error-Correction"><a href="#ACL2023-LET-Leveraging-Error-Type-Information-for-Grammatical-Error-Correction" class="headerlink" title="ACL2023: LET: Leveraging Error Type Information for Grammatical Error Correction"></a><a href="">ACL2023: LET: Leveraging Error Type Information for Grammatical Error Correction</a></h2><h3 id="Author-李泓嘉"><a href="#Author-李泓嘉" class="headerlink" title="Author: 李泓嘉"></a>Author: 李泓嘉</h3><h3 id="Abstract-32"><a href="#Abstract-32" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="ACL2023-Tailoring-Instructions-to-Student’s-Learning-Levels-Boosts-Knowledge-Distillation"><a href="#ACL2023-Tailoring-Instructions-to-Student’s-Learning-Levels-Boosts-Knowledge-Distillation" class="headerlink" title="ACL2023: Tailoring Instructions to Student’s Learning Levels Boosts Knowledge Distillation"></a><a href="">ACL2023: Tailoring Instructions to Student’s Learning Levels Boosts Knowledge Distillation</a></h2><h3 id="Author-任昱鑫"><a href="#Author-任昱鑫" class="headerlink" title="Author: 任昱鑫"></a>Author: 任昱鑫</h3><h3 id="Abstract-33"><a href="#Abstract-33" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="IEEE-T-MM-HHF-Hashing-guided-Hinge-Function-for-Deep-Hashing-Retrieval"><a href="#IEEE-T-MM-HHF-Hashing-guided-Hinge-Function-for-Deep-Hashing-Retrieval" class="headerlink" title="IEEE T-MM: HHF: Hashing-guided Hinge Function for Deep Hashing Retrieval"></a><a href="">IEEE T-MM: HHF: Hashing-guided Hinge Function for Deep Hashing Retrieval</a></h2><h3 id="Author-徐呈寅"><a href="#Author-徐呈寅" class="headerlink" title="Author: 徐呈寅"></a>Author: 徐呈寅</h3><h3 id="Abstract-34"><a href="#Abstract-34" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="ACM-MM2023-Enhanced-Image-Deblurring-An-Efficient-Frequency-Exploitation-and-Preservation-Network"><a href="#ACM-MM2023-Enhanced-Image-Deblurring-An-Efficient-Frequency-Exploitation-and-Preservation-Network" class="headerlink" title="ACM MM2023: Enhanced Image Deblurring: An Efficient Frequency Exploitation and Preservation Network"></a><a href="">ACM MM2023: Enhanced Image Deblurring: An Efficient Frequency Exploitation and Preservation Network</a></h2><h3 id="Author-董姝婷"><a href="#Author-董姝婷" class="headerlink" title="Author: 董姝婷"></a>Author: 董姝婷</h3><h3 id="Abstract-35"><a href="#Abstract-35" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="IJCAI2023-DFVSR-Directional-Frequency-Video-Super-Resolution-via-Asymmetric-and-Enhancement-Alignment-Network"><a href="#IJCAI2023-DFVSR-Directional-Frequency-Video-Super-Resolution-via-Asymmetric-and-Enhancement-Alignment-Network" class="headerlink" title="IJCAI2023: DFVSR: Directional Frequency Video Super-Resolution via Asymmetric and Enhancement Alignment Network"></a><a href="">IJCAI2023: DFVSR: Directional Frequency Video Super-Resolution via Asymmetric and Enhancement Alignment Network</a></h2><h3 id="Author-董姝婷-1"><a href="#Author-董姝婷-1" class="headerlink" title="Author: 董姝婷"></a>Author: 董姝婷</h3><h3 id="Abstract-36"><a href="#Abstract-36" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="ACM-MM2023-Adaptive-Contrastive-Learning-for-Learning-Robust-Representations-under-Label-Noise"><a href="#ACM-MM2023-Adaptive-Contrastive-Learning-for-Learning-Robust-Representations-under-Label-Noise" class="headerlink" title="ACM MM2023: Adaptive Contrastive Learning for Learning Robust Representations under Label Noise"></a><a href="">ACM MM2023: Adaptive Contrastive Learning for Learning Robust Representations under Label Noise</a></h2><h3 id="Author-王子浩"><a href="#Author-王子浩" class="headerlink" title="Author: 王子浩"></a>Author: 王子浩</h3><h3 id="Abstract-37"><a href="#Abstract-37" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="ICCV2023-HiFace-High-Fidelity-3D-Face-Reconstruction-by-Learning-Static-and-Dynamic-Details"><a href="#ICCV2023-HiFace-High-Fidelity-3D-Face-Reconstruction-by-Learning-Static-and-Dynamic-Details" class="headerlink" title="ICCV2023: HiFace: High-Fidelity 3D Face Reconstruction by Learning Static and Dynamic Details"></a><a href="">ICCV2023: HiFace: High-Fidelity 3D Face Reconstruction by Learning Static and Dynamic Details</a></h2><h3 id="Author-柴增豪"><a href="#Author-柴增豪" class="headerlink" title="Author: 柴增豪"></a>Author: 柴增豪</h3><h3 id="Abstract-38"><a href="#Abstract-38" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="ICCV2023-Accurate-3D-Face-Reconstruction-with-Facial-Component-Tokens"><a href="#ICCV2023-Accurate-3D-Face-Reconstruction-with-Facial-Component-Tokens" class="headerlink" title="ICCV2023: Accurate 3D Face Reconstruction with Facial Component Tokens"></a><a href="">ICCV2023: Accurate 3D Face Reconstruction with Facial Component Tokens</a></h2><h3 id="Author-章天珂"><a href="#Author-章天珂" class="headerlink" title="Author: 章天珂"></a>Author: 章天珂</h3><h3 id="Abstract-39"><a href="#Abstract-39" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="ICCVW2023-Effective-Whole-body-Pose-Estimation-with-Two-stages-Distillation"><a href="#ICCVW2023-Effective-Whole-body-Pose-Estimation-with-Two-stages-Distillation" class="headerlink" title="ICCVW2023: Effective Whole-body Pose Estimation with Two-stages Distillation"></a><a href="">ICCVW2023: Effective Whole-body Pose Estimation with Two-stages Distillation</a></h2><h3 id="Author-杨震东"><a href="#Author-杨震东" class="headerlink" title="Author: 杨震东"></a>Author: 杨震东</h3><h3 id="Abstract-40"><a href="#Abstract-40" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="ICCV2023-From-Knowledge-Distillation-to-Self-Knowledge-Distillation-A-Unified-Approach-with-Normalized-Loss-and-Customized-Soft-Labels"><a href="#ICCV2023-From-Knowledge-Distillation-to-Self-Knowledge-Distillation-A-Unified-Approach-with-Normalized-Loss-and-Customized-Soft-Labels" class="headerlink" title="ICCV2023: From Knowledge Distillation to Self-Knowledge Distillation: A Unified Approach with Normalized Loss and Customized Soft Labels"></a><a href="">ICCV2023: From Knowledge Distillation to Self-Knowledge Distillation: A Unified Approach with Normalized Loss and Customized Soft Labels</a></h2><h3 id="Author-杨震东-1"><a href="#Author-杨震东-1" class="headerlink" title="Author: 杨震东"></a>Author: 杨震东</h3><h3 id="Abstract-41"><a href="#Abstract-41" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="ICCV2023-Make-Encoder-Great-Again-in-3D-GAN-Inversion-through-Geometry-and-Occlusion-Aware-Encoding"><a href="#ICCV2023-Make-Encoder-Great-Again-in-3D-GAN-Inversion-through-Geometry-and-Occlusion-Aware-Encoding" class="headerlink" title="ICCV2023: Make Encoder Great Again in 3D GAN Inversion through Geometry and Occlusion-Aware Encoding"></a><a href="">ICCV2023: Make Encoder Great Again in 3D GAN Inversion through Geometry and Occlusion-Aware Encoding</a></h2><h3 id="Author-袁梓洋-2"><a href="#Author-袁梓洋-2" class="headerlink" title="Author: 袁梓洋"></a>Author: 袁梓洋</h3><h3 id="Abstract-42"><a href="#Abstract-42" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="ICML2023-UPop-Unified-and-Progressive-Pruning-for-Compressing-Vision-Language-Transformers"><a href="#ICML2023-UPop-Unified-and-Progressive-Pruning-for-Compressing-Vision-Language-Transformers" class="headerlink" title="ICML2023: UPop: Unified and Progressive Pruning for Compressing Vision-Language Transformers"></a><a href="">ICML2023: UPop: Unified and Progressive Pruning for Compressing Vision-Language Transformers</a></h2><h3 id="Author-石大川-1"><a href="#Author-石大川-1" class="headerlink" title="Author: 石大川"></a>Author: 石大川</h3><h3 id="Abstract-43"><a href="#Abstract-43" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="ICML2023-Learning-to-Learn-from-APIs-Black-Box-Data-Free-Meta-Learning"><a href="#ICML2023-Learning-to-Learn-from-APIs-Black-Box-Data-Free-Meta-Learning" class="headerlink" title="ICML2023: Learning to Learn from APIs: Black-Box Data-Free Meta-Learning"></a><a href="">ICML2023: Learning to Learn from APIs: Black-Box Data-Free Meta-Learning</a></h2><h3 id="Author-胡梓轩-1"><a href="#Author-胡梓轩-1" class="headerlink" title="Author: 胡梓轩"></a>Author: 胡梓轩</h3><h3 id="Abstract-44"><a href="#Abstract-44" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="ICRA2023-AANet-Aggregation-and-Alignment-Network-with-Semi-hard-Positive-Sample-Mining-for-Hierarchical-Place-Recognition"><a href="#ICRA2023-AANet-Aggregation-and-Alignment-Network-with-Semi-hard-Positive-Sample-Mining-for-Hierarchical-Place-Recognition" class="headerlink" title="ICRA2023: AANet: Aggregation and Alignment Network with Semi-hard Positive Sample Mining for Hierarchical Place Recognition"></a><a href="">ICRA2023: AANet: Aggregation and Alignment Network with Semi-hard Positive Sample Mining for Hierarchical Place Recognition</a></h2><h3 id="Author-卢锋-4"><a href="#Author-卢锋-4" class="headerlink" title="Author: 卢锋"></a>Author: 卢锋</h3><h3 id="Abstract-45"><a href="#Abstract-45" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="CVPR2023-High-fidelity-Facial-Avatar-Reconstruction-from-Monocular-Video-with-Generative-Priors"><a href="#CVPR2023-High-fidelity-Facial-Avatar-Reconstruction-from-Monocular-Video-with-Generative-Priors" class="headerlink" title="CVPR2023: High-fidelity Facial Avatar Reconstruction from Monocular Video with Generative Priors"></a><a href="">CVPR2023: High-fidelity Facial Avatar Reconstruction from Monocular Video with Generative Priors</a></h2><h3 id="Author-白云鹏-1"><a href="#Author-白云鹏-1" class="headerlink" title="Author: 白云鹏"></a>Author: 白云鹏</h3><h3 id="Abstract-46"><a href="#Abstract-46" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="CVPR2023-Learning-Imbalanced-Data-with-Vision-Transformers"><a href="#CVPR2023-Learning-Imbalanced-Data-with-Vision-Transformers" class="headerlink" title="CVPR2023: Learning Imbalanced Data with Vision Transformers"></a><a href="">CVPR2023: Learning Imbalanced Data with Vision Transformers</a></h2><h3 id="Author-许正卓-1"><a href="#Author-许正卓-1" class="headerlink" title="Author: 许正卓"></a>Author: 许正卓</h3><h3 id="Abstract-47"><a href="#Abstract-47" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="IEEE-TCSVT-Task-adaptive-Feature-Disentanglement-and-Hallucination-for-Few-shot-Classification"><a href="#IEEE-TCSVT-Task-adaptive-Feature-Disentanglement-and-Hallucination-for-Few-shot-Classification" class="headerlink" title="IEEE TCSVT: Task-adaptive Feature Disentanglement and Hallucination for Few-shot Classification"></a><a href="">IEEE TCSVT: Task-adaptive Feature Disentanglement and Hallucination for Few-shot Classification</a></h2><h3 id="Author-胡梓轩-2"><a href="#Author-胡梓轩-2" class="headerlink" title="Author: 胡梓轩"></a>Author: 胡梓轩</h3><h3 id="Abstract-48"><a href="#Abstract-48" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="CVPR2023-Architecture-Dataset-and-Model-Scale-Agnostic-Data-free-Meta-Learning"><a href="#CVPR2023-Architecture-Dataset-and-Model-Scale-Agnostic-Data-free-Meta-Learning" class="headerlink" title="CVPR2023: Architecture, Dataset and Model-Scale Agnostic Data-free Meta-Learning"></a><a href="">CVPR2023: Architecture, Dataset and Model-Scale Agnostic Data-free Meta-Learning</a></h2><h3 id="Author-胡梓轩-3"><a href="#Author-胡梓轩-3" class="headerlink" title="Author: 胡梓轩"></a>Author: 胡梓轩</h3><h3 id="Abstract-49"><a href="#Abstract-49" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="CVPR2023-Uni-Perceiver-v2-A-Generalist-Model-for-Large-Scale-Vision-and-Vision-Language-Tasks"><a href="#CVPR2023-Uni-Perceiver-v2-A-Generalist-Model-for-Large-Scale-Vision-and-Vision-Language-Tasks" class="headerlink" title="CVPR2023: Uni-Perceiver v2: A Generalist Model for Large-Scale Vision and Vision-Language Tasks"></a><a href="">CVPR2023: Uni-Perceiver v2: A Generalist Model for Large-Scale Vision and Vision-Language Tasks</a></h2><h3 id="Author-江晓湖"><a href="#Author-江晓湖" class="headerlink" title="Author: 江晓湖"></a>Author: 江晓湖</h3><h3 id="Abstract-50"><a href="#Abstract-50" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="ICASSP2023-FREQUENCY-RECIPROCAL-ACTION-AND-FUSION-FOR-SINGLE-IMAGE-SUPER-RESOLUTION"><a href="#ICASSP2023-FREQUENCY-RECIPROCAL-ACTION-AND-FUSION-FOR-SINGLE-IMAGE-SUPER-RESOLUTION" class="headerlink" title="ICASSP2023: FREQUENCY RECIPROCAL ACTION AND FUSION FOR SINGLE IMAGE SUPER-RESOLUTION"></a><a href="">ICASSP2023: FREQUENCY RECIPROCAL ACTION AND FUSION FOR SINGLE IMAGE SUPER-RESOLUTION</a></h2><h3 id="Author-董姝婷-2"><a href="#Author-董姝婷-2" class="headerlink" title="Author: 董姝婷"></a>Author: 董姝婷</h3><h3 id="Abstract-51"><a href="#Abstract-51" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="ICASSP2023-Rethink-Long-Tailed-Recognition-With-Vision-Transforms"><a href="#ICASSP2023-Rethink-Long-Tailed-Recognition-With-Vision-Transforms" class="headerlink" title="ICASSP2023: Rethink Long-Tailed Recognition With Vision Transforms"></a><a href="">ICASSP2023: Rethink Long-Tailed Recognition With Vision Transforms</a></h2><h3 id="Author-许正卓-2"><a href="#Author-许正卓-2" class="headerlink" title="Author: 许正卓"></a>Author: 许正卓</h3><h3 id="Abstract-52"><a href="#Abstract-52" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="IEEE-TNNLS-PatchNet-Maximize-the-Exploration-of-Congeneric-Semantics-for-Weakly-Supervised-Semantic-Segmentation"><a href="#IEEE-TNNLS-PatchNet-Maximize-the-Exploration-of-Congeneric-Semantics-for-Weakly-Supervised-Semantic-Segmentation" class="headerlink" title="IEEE TNNLS: PatchNet: Maximize the Exploration of Congeneric Semantics for Weakly Supervised Semantic Segmentation"></a><a href="">IEEE TNNLS: PatchNet: Maximize the Exploration of Congeneric Semantics for Weakly Supervised Semantic Segmentation</a></h2><h3 id="Author-张可"><a href="#Author-张可" class="headerlink" title="Author: 张可"></a>Author: 张可</h3><h3 id="Abstract-53"><a href="#Abstract-53" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="AAAI2023-Truncate-Split-Contrast-A-Framework-for-Learning-from-Mislabeled-Videos"><a href="#AAAI2023-Truncate-Split-Contrast-A-Framework-for-Learning-from-Mislabeled-Videos" class="headerlink" title="AAAI2023: Truncate-Split-Contrast: A Framework for Learning from Mislabeled Videos"></a><a href="">AAAI2023: Truncate-Split-Contrast: A Framework for Learning from Mislabeled Videos</a></h2><h3 id="Author-王子啸"><a href="#Author-王子啸" class="headerlink" title="Author: 王子啸"></a>Author: 王子啸</h3><h3 id="Abstract-54"><a href="#Abstract-54" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="AAAI2023-Darwinian-Model-Upgrades-Model-Evolving-with-Selective-Compatibility"><a href="#AAAI2023-Darwinian-Model-Upgrades-Model-Evolving-with-Selective-Compatibility" class="headerlink" title="AAAI2023: Darwinian Model Upgrades: Model Evolving with Selective Compatibility"></a><a href="https://ojs.aaai.org/index.php/AAAI/article/view/25447">AAAI2023: Darwinian Model Upgrades: Model Evolving with Selective Compatibility</a></h2><h3 id="Author-张斌杰"><a href="#Author-张斌杰" class="headerlink" title="Author: 张斌杰"></a>Author: 张斌杰</h3><h3 id="Abstract-55"><a href="#Abstract-55" class="headerlink" title="Abstract"></a>Abstract</h3><p>The traditional model upgrading paradigm for retrieval requires recomputing all gallery embeddings before deploying the new model (dubbed as “backfilling”), which is quite expensive and time-consuming considering billions of instances in industrial applications. BCT presents the first step towards backward-compatible model upgrades to get rid of backfilling. It is workable but leaves the new model in a dilemma between new feature discriminativeness and new-to-old compatibility due to the undifferentiated compatibility constraints. In this work, we propose Darwinian Model Upgrades (DMU), which disentangle the inheritance and variation in the model evolving with selective backward compatibility and forward adaptation, respectively. The old-to-new heritable knowledge is measured by old feature discriminativeness, and the gallery features, especially those of poor quality, are evolved in a lightweight manner to become more adaptive in the new latent space. We demonstrate the superiority of DMU through comprehensive experiments on large-scale landmark retrieval and face recognition benchmarks. DMU effectively alleviates the new-to-new degradation at the same time improving new-to-old compatibility, rendering a more proper model upgrading paradigm in large-scale retrieval systems.Code: <a href="https://github.com/TencentARC/OpenCompatible">https://github.com/TencentARC/OpenCompatible</a>.</p>
]]></content>
  </entry>
</search>
