<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#660874"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#660874">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha256-5eIC48iZUHmSlSUz9XtjRyK2mzQkHScZY1WdMaoz74E=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"thu-cvml.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.21.1","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":-1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="教授，博士生导师">
<meta property="og:type" content="website">
<meta property="og:title" content="清华大学&lt;br&gt;深圳国际研究生院&lt;br&gt;袁春教授课题组">
<meta property="og:url" content="https://thu-cvml.github.io/index.html">
<meta property="og:site_name" content="清华大学&lt;br&gt;深圳国际研究生院&lt;br&gt;袁春教授课题组">
<meta property="og:description" content="教授，博士生导师">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="袁春">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://thu-cvml.github.io/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"en","comments":"","permalink":"","path":"index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>清华大学<br>深圳国际研究生院<br>袁春教授课题组</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">清华大学<br>深圳国际研究生院<br>袁春教授课题组</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">计算机视觉与机器学习<br>Computer Vision and Machine Learning<br>CVML</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-research"><a href="/" rel="section"><i class="fa fa-medal fa-fw"></i>Research</a></li><li class="menu-item menu-item-overview"><a href="/Overview/" rel="section"><i class="fa fa-circle-nodes fa-fw"></i>Overview</a></li><li class="menu-item menu-item-archive"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archive</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="Searching..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="袁春"
      src="/images/Chunyuan.jpg">
  <p class="site-author-name" itemprop="name">袁春</p>
  <div class="site-description" itemprop="description">教授，博士生导师</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">1</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://scholar.google.com/citations?hl=en&user=fYdxi2sAAAAJ" title="Google Scholar → https:&#x2F;&#x2F;scholar.google.com&#x2F;citations?hl&#x3D;en&amp;user&#x3D;fYdxi2sAAAAJ" rel="noopener me" target="_blank">Google Scholar</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://thu-cvml.github.io/2024/12/11/%E7%A7%91%E7%A0%94%E5%BF%AB%E8%AE%AF/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/Chunyuan.jpg">
      <meta itemprop="name" content="袁春">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="清华大学<br>深圳国际研究生院<br>袁春教授课题组">
      <meta itemprop="description" content="教授，博士生导师">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 清华大学<br>深圳国际研究生院<br>袁春教授课题组">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/12/11/%E7%A7%91%E7%A0%94%E5%BF%AB%E8%AE%AF/" class="post-title-link" itemprop="url">2023-2025年CVML科研成果展示</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-12-11 16:34:09" itemprop="dateCreated datePublished" datetime="2024-12-11T16:34:09+08:00">2024-12-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-12-18 13:16:13" itemprop="dateModified" datetime="2024-12-18T13:16:13+08:00">2024-12-18</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <div class="note primary"><h2 id="ICCV2023-When-Noisy-Labels-Meet-Long-Tail-Dilemmas-A-Representation-Calibration-Method"><a href="#ICCV2023-When-Noisy-Labels-Meet-Long-Tail-Dilemmas-A-Representation-Calibration-Method" class="headerlink" title="ICCV2023: When Noisy Labels Meet Long Tail Dilemmas: A Representation Calibration Method"></a><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_When_Noisy_Labels_Meet_Long_Tail_Dilemmas_A_Representation_Calibration_ICCV_2023_paper.html?trk=public_post_comment-text">ICCV2023: When Noisy Labels Meet Long Tail Dilemmas: A Representation Calibration Method</a></h2><h3 id="Author-张曼怡"><a href="#Author-张曼怡" class="headerlink" title="Author: 张曼怡"></a>Author: 张曼怡</h3><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>Real-world large-scale datasets are both noisily labeled and class-imbalanced. The issues seriously hurt the generalization of trained models. It is hence significant to address the simultaneous incorrect labeling and class-imbalance, i.e., the problem of learning with noisy labels on long-tailed data. Previous works develop several methods for the problem. However, they always rely on strong assumptions that are invalid or hard to be checked in practice. In this paper, to handle the problem and address the limitations of prior works, we propose a representation calibration method RCAL. Specifically, RCAL works with the representations extracted by unsupervised contrastive learning. We assume that without incorrect labeling and class imbalance, the representations of instances in each class conform to a multivariate Gaussian distribution, which is much milder and easier to be checked. Based on the assumption, we recover underlying representation distributions from polluted ones resulting from mislabeled and class-imbalanced data. Additional data points are then sampled from the recovered distributions to help generalization. Moreover, during classifier training, representation learning takes advantage of representation robustness brought by contrastive learning, which further improves the classifier performance. We derive theoretical results to discuss the effectiveness of our representation calibration. Experiments on multiple benchmarks justify our claims and confirm the superiority of the proposed method.</p>
<p><img src="https://raw.githubusercontent.com/THU-CVML/thu-cvml.github.io/refs/heads/main/images/RCAL.png"></p>
</div>

<div class="note primary"><h2 id="AAAI2025-Aligning-Composed-Query-with-Image-via-Discriminative-Perception-from-Negative-Correspondences"><a href="#AAAI2025-Aligning-Composed-Query-with-Image-via-Discriminative-Perception-from-Negative-Correspondences" class="headerlink" title="AAAI2025: Aligning Composed Query with Image via Discriminative Perception from Negative Correspondences"></a><a href="">AAAI2025: Aligning Composed Query with Image via Discriminative Perception from Negative Correspondences</a></h2><h3 id="Author-王依凡"><a href="#Author-王依凡" class="headerlink" title="Author: 王依凡"></a>Author: 王依凡</h3><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3></div>

<div class="note primary"><h2 id="AAAI2025-Rethinking-Pseudo-Label-Guided-Learning-for-Weakly-Supervised-Temporal-Action-Localization-from-the-Perspective-of-Noise-Correction"><a href="#AAAI2025-Rethinking-Pseudo-Label-Guided-Learning-for-Weakly-Supervised-Temporal-Action-Localization-from-the-Perspective-of-Noise-Correction" class="headerlink" title="AAAI2025: Rethinking Pseudo-Label Guided Learning for Weakly-Supervised Temporal Action Localization from the Perspective of Noise Correction"></a><a href="">AAAI2025: Rethinking Pseudo-Label Guided Learning for Weakly-Supervised Temporal Action Localization from the Perspective of Noise Correction</a></h2><h3 id="Author-张权"><a href="#Author-张权" class="headerlink" title="Author: 张权"></a>Author: 张权</h3><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3></div>

<div class="note primary"><h2 id="NeurIPS2025-SuperVLAD-Compact-and-Robust-Image-Descriptors-for-Visual-Place-Recognition"><a href="#NeurIPS2025-SuperVLAD-Compact-and-Robust-Image-Descriptors-for-Visual-Place-Recognition" class="headerlink" title="NeurIPS2025: SuperVLAD: Compact and Robust Image Descriptors for Visual Place Recognition"></a><a href="">NeurIPS2025: SuperVLAD: Compact and Robust Image Descriptors for Visual Place Recognition</a></h2><h3 id="Author-卢锋"><a href="#Author-卢锋" class="headerlink" title="Author: 卢锋"></a>Author: 卢锋</h3><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3></div>

<div class="note primary"><h2 id="ICPR2024-Benchmarking-AI-in-Mental-Health-A-Critical-Examination-of-LLMs-Across-Key-Performance-and-Ethical-Metrics"><a href="#ICPR2024-Benchmarking-AI-in-Mental-Health-A-Critical-Examination-of-LLMs-Across-Key-Performance-and-Ethical-Metrics" class="headerlink" title="ICPR2024: Benchmarking AI in Mental Health: A Critical Examination of LLMs Across Key Performance and Ethical Metrics"></a><a target="_blank" rel="noopener" href="https://link.springer.com/chapter/10.1007/978-3-031-78104-9_24">ICPR2024: Benchmarking AI in Mental Health: A Critical Examination of LLMs Across Key Performance and Ethical Metrics</a></h2><h3 id="Author-袁睿"><a href="#Author-袁睿" class="headerlink" title="Author: 袁睿"></a>Author: 袁睿</h3><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>The rapid advancement of artificial intelligence (AI) has led to an increasing application of Large Language Models (LLMs) in psychological counseling. This study focuses on a comprehensive evaluation of LLMs in this domain, moving beyond traditional case-based reasoning. We introduce a novel multi-agent LLM framework that enhances the analysis of psychological case interactions. Our approach involves expanding the Emotional First Aid dataset with diverse client backgrounds, enhancing its applicability and generalizability. A sophisticated user profile model, incorporating eight critical dimensions, is developed and applied within a multi-agent system to examine counseling scenarios. The system’s performance is extensively evaluated based on accuracy, robustness, consistency, and fairness. The findings reveal significant differences among LLMs in these areas, highlighting their strengths and limitations in psychological interventions. This research underscores the need for ongoing refinement in LLM applications to ensure equitable and reliable support in psychological counseling. The detailed results and methodologies are available on the GitHub platform for further academic scrutiny and development.</p>
</div>

<div class="note primary"><h2 id="ICPR2024-AMC-OA-Adaptive-Multi-Scale-Convolutional-Networks-with-Optimized-Attention-for-Temporal-Action-Localization"><a href="#ICPR2024-AMC-OA-Adaptive-Multi-Scale-Convolutional-Networks-with-Optimized-Attention-for-Temporal-Action-Localization" class="headerlink" title="ICPR2024: AMC-OA: Adaptive Multi-Scale Convolutional Networks with Optimized Attention for Temporal Action Localization"></a><a target="_blank" rel="noopener" href="https://link.springer.com/chapter/10.1007/978-3-031-78107-0_9">ICPR2024: AMC-OA: Adaptive Multi-Scale Convolutional Networks with Optimized Attention for Temporal Action Localization</a></h2><h3 id="Author-袁睿"><a href="#Author-袁睿" class="headerlink" title="Author: 袁睿"></a>Author: 袁睿</h3><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>Temporal Action Localization (TAL) is crucial in video understanding, focusing on identifying and timestamping actions within raw video footage. A critical challenge in TAL is processing the rich spatiotemporal details inherent in videos, traditionally addressed through methods adapted from image processing. The Vision Transformer (VIT) model marked a significant evolution, using a self-attention mechanism for enhanced temporal information blending. Despite these advancements, two key issues remain: insufficient extraction of spatial semantic information at lower levels of feature pyramids and inadequate capture of temporal semantic information at higher levels. To address these challenges, we introduce Adaptive Multi-Scale Convolutional Networks with Optimized Attention (AMC-OA). AMC-OA enhances lower-level features within the pyramid using multi-scale convolutional kernels, enriching spatial contextual semantics. Simultaneously, upper-level features are refined with a temporally-focused contextual enhancement network utilizing residual structures for better temporal understanding. To further improve the model’s capability in handling extensive temporal spans, we integrate an advanced multi-head attention mechanism. Empirical results on benchmarks like THUMOS14 and ActivityNet1.3 demonstrate AMC-OA’s superiority in TAL tasks, significantly improving both spatial and temporal information extraction compared to state-of-the-art models.</p>
</div>

<div class="note primary"><h2 id="ACM-MM2024-Semantic-Distillation-from-Neighborhood-for-Composed-Image-Retrieval"><a href="#ACM-MM2024-Semantic-Distillation-from-Neighborhood-for-Composed-Image-Retrieval" class="headerlink" title="ACM MM2024: Semantic Distillation from Neighborhood for Composed Image Retrieval"></a><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/abs/10.1145/3664647.3681493">ACM MM2024: Semantic Distillation from Neighborhood for Composed Image Retrieval</a></h2><h3 id="Author-王依凡"><a href="#Author-王依凡" class="headerlink" title="Author: 王依凡"></a>Author: 王依凡</h3><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>The challenging task composed image retrieval targets at identifying the matched image from the multi-modal query with a reference image and a textual modifier. Most existing methods are devoted to composing the unified query representations from the query images and texts, yet the distribution gaps between the hybrid-modal query representations and visual target representations are neglected. However, directly incorporating target features on the query may cause ambiguous rankings and poor robustness due to the insufficient exploration of the distinguishments and overfitting issues. To address the above concerns, we propose a novel framework termed <em>SemAntic Distillation from Neighborhood (SADN)</em> for composed image retrieval. For mitigating the distribution divergences, we construct neighborhood sampling from the target domain for each query and aggregate neighborhood features with adaptive weights to restructure the query representations. Specifically, the adaptive weights are determined by the collaboration of two individual modules, as correspondence-induced adaption and divergence-based correction. Correspondence-induced adaption accounts for capturing the correlation alignments from neighbor features under the guidance of the positive representations, and the divergence-based correction regulates the weights based on the embedding distances between hard negatives and the query in the latent space. Extensive results and ablation studies on CIRR and FashionIQ validate that the proposed semantic distillation from neighborhood significantly outperforms baseline methods.</p>
</div>

<div class="note primary"><h2 id="ACM-MM2024-CustomNet-Object-Customization-with-Variable-Viewpoints-in-Text-to-Image-Diffusion-Models"><a href="#ACM-MM2024-CustomNet-Object-Customization-with-Variable-Viewpoints-in-Text-to-Image-Diffusion-Models" class="headerlink" title="ACM MM2024: CustomNet: Object Customization with Variable-Viewpoints in Text-to-Image Diffusion Models"></a><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/abs/10.1145/3664647.3681396">ACM MM2024: CustomNet: Object Customization with Variable-Viewpoints in Text-to-Image Diffusion Models</a></h2><h3 id="Author-袁梓洋"><a href="#Author-袁梓洋" class="headerlink" title="Author: 袁梓洋"></a>Author: 袁梓洋</h3><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>Incorporating a customized object into image generation presents an attractive feature in text-to-image (T2I) generation. Some methods finetune T2I models for each object individually at test-time, which tend to be overfitted and time-consuming. Others train an extra encoder to extract object visual information for customization efficiently but struggle to preserve the object’s identity. To address these limitations, we present CustomNet, a unified encoder-based object customization framework that explicitly incorporates 3D novel view synthesis capabilities into the customization process. This integration facilitates the adjustment of spatial positions and viewpoints, producing diverse outputs while effectively preserving the object’s identity. To train our model effectively, we propose a dataset construction pipeline to better handle real-world objects and complex backgrounds. Additionally, we introduce delicate designs that enable location control and flexible background control through textual descriptions or user-defined backgrounds. Our method allows for object customization without the need of test-time optimization, providing simultaneous control over viewpoints, location, and text. Experimental results show that our method outperforms other customization methods regarding identity preservation, diversity, and harmony. Codes are available at <a target="_blank" rel="noopener" href="https://github.com/TencentARC/CustomNet">https://github.com/TencentARC/CustomNet</a>.</p>
</div>

<div class="note primary"><h2 id="ECCV2024-GVGEN-Text-to-3D-Generation-with-Volumetric-Representation"><a href="#ECCV2024-GVGEN-Text-to-3D-Generation-with-Volumetric-Representation" class="headerlink" title="ECCV2024: GVGEN: Text-to-3D Generation with Volumetric Representation"></a><a target="_blank" rel="noopener" href="https://link.springer.com/chapter/10.1007/978-3-031-73242-3_26">ECCV2024: GVGEN: Text-to-3D Generation with Volumetric Representation</a></h2><h3 id="Author-何相龙"><a href="#Author-何相龙" class="headerlink" title="Author: 何相龙"></a>Author: 何相龙</h3><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>In recent years, 3D Gaussian splatting has emerged as a powerful technique for 3D reconstruction and generation, known for its fast and high-quality rendering capabilities. Nevertheless, these methods often come with limitations, either lacking the ability to produce diverse samples or requiring prolonged inference times. To address these shortcomings, this paper introduces a novel diffusion-based framework, GVGEN, designed to efficiently generate 3D Gaussian representations from text input. We propose two innovative techniques: (1) <em>Structured Volumetric Representation</em>. We first arrange disorganized 3D Gaussian points as a structured form GaussianVolume. This transformation allows the capture of intricate texture details within a volume composed of a fixed number of Gaussians. To better optimize the representation of these details, we propose a unique pruning and densifying method named the Candidate Pool Strategy, enhancing detail fidelity through selective optimization. (2) <em>Coarse-to-fine Generation Pipeline</em>. To simplify the generation of GaussianVolume and empower the model to generate instances with detailed 3D geometry, we propose a coarse-to-fine pipeline. It initially constructs a basic geometric structure, followed by the prediction of complete Gaussian attributes. Our framework, GVGEN, demonstrates superior performance in qualitative and quantitative assessments compared to existing 3D generation methods. Simultaneously, it maintains a fast generation speed (∼7 s), effectively striking a balance between quality and efficiency. Our project page is <a target="_blank" rel="noopener" href="https://gvgen.github.io/">https://gvgen.github.io/</a>.</p>
</div>

<div class="note primary"><h2 id="ECCV2024-A-Task-is-Worth-One-Word-Learning-with-Task-Prompts-for-High-Quality-Versatile-Image-Inpainting"><a href="#ECCV2024-A-Task-is-Worth-One-Word-Learning-with-Task-Prompts-for-High-Quality-Versatile-Image-Inpainting" class="headerlink" title="ECCV2024: A Task is Worth One Word: Learning with Task Prompts for High-Quality Versatile Image Inpainting"></a><a target="_blank" rel="noopener" href="https://link.springer.com/chapter/10.1007/978-3-031-73636-0_12">ECCV2024: A Task is Worth One Word: Learning with Task Prompts for High-Quality Versatile Image Inpainting</a></h2><h3 id="Author-庄俊豪"><a href="#Author-庄俊豪" class="headerlink" title="Author: 庄俊豪"></a>Author: 庄俊豪</h3><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>Advancing image inpainting is challenging as it requires filling user-specified regions for various intents, such as background filling and object synthesis. Existing approaches focus on either context-aware filling or object synthesis using text descriptions. However, achieving both tasks simultaneously is challenging due to differing training strategies. To overcome this challenge, we introduce <strong>PowerPaint</strong>, the first high-quality and versatile inpainting model that excels in multiple inpainting tasks. First, we introduce learnable task prompts along with tailored fine-tuning strategies to guide the model’s focus on different inpainting targets explicitly. This enables PowerPaint to accomplish various inpainting tasks by utilizing different task prompts, resulting in state-of-the-art performance. Second, we demonstrate the versatility of the task prompt in PowerPaint by showcasing its effectiveness as a negative prompt for object removal. Moreover, we leverage prompt interpolation techniques to enable controllable shape-guided object inpainting, enhancing the model’s applicability in shape-guided applications. Finally, we conduct extensive experiments and applications to verify the effectiveness of PowerPaint. We release our codes and models on our project page: <a target="_blank" rel="noopener" href="https://powerpaint.github.io/">https://powerpaint.github.io/</a>.</p>
</div>

<div class="note primary"><h2 id="ECCV2024-DreamDiffusion-High-Quality-EEG-to-Image-Generation-with-Temporal-Masked-Signal-Modeling-and-CLIP-Alignment"><a href="#ECCV2024-DreamDiffusion-High-Quality-EEG-to-Image-Generation-with-Temporal-Masked-Signal-Modeling-and-CLIP-Alignment" class="headerlink" title="ECCV2024: DreamDiffusion: High-Quality EEG-to-Image Generation with Temporal Masked Signal Modeling and CLIP Alignment"></a><a target="_blank" rel="noopener" href="https://link.springer.com/chapter/10.1007/978-3-031-72751-1_27">ECCV2024: DreamDiffusion: High-Quality EEG-to-Image Generation with Temporal Masked Signal Modeling and CLIP Alignment</a></h2><h3 id="Author-白云鹏"><a href="#Author-白云鹏" class="headerlink" title="Author: 白云鹏"></a>Author: 白云鹏</h3><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>This paper introduces DreamDiffusion, a novel method for generating high-quality images directly from brain electroencephalogram (EEG) signals, without the need to translate thoughts into text. DreamDiffusion leverages pre-trained text-to-image models and employs temporal masked signal modeling to pre-train the EEG encoder for effective and robust EEG representations. Additionally, the method further leverages the CLIP image encoder to provide extra supervision to better align EEG, text, and image embeddings with limited EEG-image pairs. Overall, the proposed method overcomes the challenges of using EEG signals for image generation, such as noise, limited information, and individual differences, and achieves promising results. Quantitative and qualitative results demonstrate the effectiveness of the proposed method as a significant step towards portable and low-cost “thoughts-to-image”, with potential applications in neuroscience and computer vision.</p>
</div>

<div class="note primary"><h2 id="ECCV2024-MirrorGaussian-Reflecting-3D-Gaussians-for-Reconstructing-Mirror-Reflections"><a href="#ECCV2024-MirrorGaussian-Reflecting-3D-Gaussians-for-Reconstructing-Mirror-Reflections" class="headerlink" title="ECCV2024: MirrorGaussian: Reflecting 3D Gaussians for Reconstructing Mirror Reflections"></a><a target="_blank" rel="noopener" href="https://link.springer.com/chapter/10.1007/978-3-031-73220-1_22">ECCV2024: MirrorGaussian: Reflecting 3D Gaussians for Reconstructing Mirror Reflections</a></h2><h3 id="Author-刘佳月"><a href="#Author-刘佳月" class="headerlink" title="Author: 刘佳月"></a>Author: 刘佳月</h3><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>3D Gaussian Splatting showcases notable advancements in photo-realistic and real-time novel view synthesis. However, it faces challenges in modeling mirror reflections, which exhibit substantial appearance variations from different viewpoints. To tackle this problem, we present MirrorGaussian, the first method for mirror scene reconstruction with real-time rendering based on 3D Gaussian Splatting. The key insight is grounded on the mirror symmetry between the real-world space and the virtual mirror space. We introduce an intuitive dual-rendering strategy that enables differentiable rasterization of both the real-world 3D Gaussians and the mirrored counterpart obtained by reflecting the former about the mirror plane. All 3D Gaussians are jointly optimized with the mirror plane in an end-to-end framework. MirrorGaussian achieves high-quality and real-time rendering in scenes with mirrors, empowering scene editing like adding new mirrors and objects. Comprehensive experiments on multiple datasets demonstrate that our approach significantly outperforms existing methods, achieving state-of-the-art results. Project page: <a target="_blank" rel="noopener" href="https://mirror-gaussian.github.io/">https://mirror-gaussian.github.io/</a>.</p>
</div>

<div class="note primary"><h2 id="IEEE-TCSVT-Meta-Learning-without-Data-via-Unconditional-Diffusion-Models"><a href="#IEEE-TCSVT-Meta-Learning-without-Data-via-Unconditional-Diffusion-Models" class="headerlink" title="IEEE TCSVT: Meta-Learning without Data via Unconditional Diffusion Models"></a><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/10587268">IEEE TCSVT: Meta-Learning without Data via Unconditional Diffusion Models</a></h2><h3 id="Author-韦永贤"><a href="#Author-韦永贤" class="headerlink" title="Author: 韦永贤"></a>Author: 韦永贤</h3><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>Although few-shot learning aims to address data scarcity, it still requires large, annotated datasets for training, which are often unavailable due to cost and privacy concerns. Previous studies have utilized pre-trained diffusion models, either to synthesize auxiliary data besides limited labeled samples, or to employ diffusion models as zero-shot classifiers. However, they are limited to conditional diffusion models needing class prior information (e.g., carefully crafted text prompts) about unseen tasks. To overcome this, we leverage unconditional diffusion models without needs for class information to train a meta-model capable of generalizing to unseen tasks. The framework contains (1) a meta-learning without data approach that uses synthetic data during training; and (2) a diffusion model-based data augmentation to calibrate the distribution shift during testing. During meta-training, we implement a self-taught class-learner to gradually capture class concepts, guiding unconditional diffusion models to generate a labeled pseudo dataset. This pseudo dataset is then used to jointly train the class-learner and the meta-model, allowing for iterative refinement and clear differentiation between classes. During meta-testing, we introduce a data augmentation that employs the diffusion models used in meta-training, to narrow the gap between meta-training and meta-testing task distribution. This enables the meta-model trained on synthetic images to effectively classify real images in unseen tasks. Comprehensive experiments showcase the superiority and adaptability of our approach in four real-world scenarios. Code available at <a target="_blank" rel="noopener" href="https://github.com/WalkerWorldPeace/MLWDUDM">https://github.com/WalkerWorldPeace/MLWDUDM</a> .</p>
</div>

<div class="note primary"><h2 id="ICML2024-CrossGET-Cross-Guided-Ensemble-of-Tokens-for-Accelerating-Vision-Language-Transformers"><a href="#ICML2024-CrossGET-Cross-Guided-Ensemble-of-Tokens-for-Accelerating-Vision-Language-Transformers" class="headerlink" title="ICML2024: CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers"></a><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.17455">ICML2024: CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers</a></h2><h3 id="Author-石大川"><a href="#Author-石大川" class="headerlink" title="Author: 石大川"></a>Author: 石大川</h3><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>Recent vision-language models have achieved tremendous advances. However, their computational costs are also escalating dramatically, making model acceleration exceedingly critical. To pursue more efficient vision-language Transformers, this paper introduces Cross-Guided Ensemble of Tokens (CrossGET), a general acceleration framework for vision-language Transformers. This framework adaptively combines tokens in real-time during inference, significantly reducing computational costs while maintaining high performance. CrossGET features two primary innovations: 1) Cross-Guided Matching and Ensemble. CrossGET leverages cross-modal guided token matching and ensemble to effectively utilize cross-modal information, achieving wider applicability across both modality-independent models, e.g., CLIP, and modality-dependent ones, e.g., BLIP2. 2) Complete-Graph Soft Matching. CrossGET introduces an algorithm for the token-matching mechanism, ensuring reliable matching results while facilitating parallelizability and high efficiency. Extensive experiments have been conducted on various vision-language tasks, such as image-text retrieval, visual reasoning, image captioning, and visual question answering. The performance on both classic multimodal architectures and emerging multimodal LLMs demonstrates the framework’s effectiveness and versatility. The code is available at <a target="_blank" rel="noopener" href="https://github.com/sdc17/CrossGET">https://github.com/sdc17/CrossGET</a> .</p>
</div>

<div class="note primary"><h2 id="ICML2024-Sparse-Model-Inversion-Efficient-Inversion-of-Vision-Transformers-with-Less-Hallucination"><a href="#ICML2024-Sparse-Model-Inversion-Efficient-Inversion-of-Vision-Transformers-with-Less-Hallucination" class="headerlink" title="ICML2024: Sparse Model Inversion: Efficient Inversion of Vision Transformers with Less Hallucination"></a><a target="_blank" rel="noopener" href="https://openreview.net/forum?id=T0lFfO8HaK">ICML2024: Sparse Model Inversion: Efficient Inversion of Vision Transformers with Less Hallucination</a></h2><h3 id="Author-胡梓轩"><a href="#Author-胡梓轩" class="headerlink" title="Author: 胡梓轩"></a>Author: 胡梓轩</h3><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>Model inversion, which aims to reconstruct the original training data from pre-trained discriminative models, is especially useful when the original training data is unavailable due to privacy, usage rights, or size constraints. However, existing dense inversion methods attempt to reconstruct the entire image area, making them extremely inefficient when inverting high-resolution images from large-scale Vision Transformers (ViTs). We further identify two underlying causes of this inefficiency: the redundant inversion of noisy backgrounds and the unintended inversion of spurious correlations—a phenomenon we term &#96;&#96;hallucination’’ in model inversion. To address these limitations, we propose a novel sparse model inversion strategy, as a plug-and-play extension to speed up existing dense inversion methods with no need for modifying their original loss functions. Specifically, we selectively invert semantic foregrounds while stopping the inversion of noisy backgrounds and potential spurious correlations. Through both theoretical and empirical studies, we validate the efficacy of our approach in achieving significant inversion acceleration (up to ×3.79) while maintaining comparable or even enhanced downstream performance in data-free model quantization and data-free knowledge transfer. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Egg-Hu/SMI">https://github.com/Egg-Hu/SMI</a>.</p>
</div>

<div class="note primary"><h2 id="ICML2024-Task-Groupings-Regularization-Data-Free-Meta-Learning-with-Heterogeneous-Pre-trained-Models"><a href="#ICML2024-Task-Groupings-Regularization-Data-Free-Meta-Learning-with-Heterogeneous-Pre-trained-Models" class="headerlink" title="ICML2024: Task Groupings Regularization: Data-Free Meta-Learning with Heterogeneous Pre-trained Models"></a><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.16560">ICML2024: Task Groupings Regularization: Data-Free Meta-Learning with Heterogeneous Pre-trained Models</a></h2><h3 id="Author-韦永贤"><a href="#Author-韦永贤" class="headerlink" title="Author: 韦永贤"></a>Author: 韦永贤</h3><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>Data-Free Meta-Learning (DFML) aims to derive knowledge from a collection of pre-trained models without accessing their original data, enabling the rapid adaptation to new unseen tasks. Current methods often overlook the heterogeneity among pre-trained models, which leads to performance degradation due to task conflicts. In this paper, we empirically and theoretically identify and analyze the model heterogeneity in DFML. We find that model heterogeneity introduces a heterogeneity-homogeneity trade-off, where homogeneous models reduce task conflicts but also increase the overfitting risk. Balancing this trade-off is crucial for learning shared representations across tasks. Based on our findings, we propose Task Groupings Regularization that benefits from model heterogeneity by grouping and aligning conflicting tasks. Specifically, we embed pre-trained models into a task space to compute dissimilarity, and group heterogeneous models together based on this measure. Then, we introduce implicit gradient regularization within each group to mitigate potential conflicts. By encouraging a gradient direction suitable for all tasks, the meta-model captures shared representations that generalize across tasks. Comprehensive experiments showcase the superiority of our approach in multiple benchmarks, effectively tackling the model heterogeneity in challenging multi-domain and multi-architecture scenarios.</p>
</div>

<div class="note primary"><h2 id="ICML2024-DFD-Distillng-the-Feature-Disparity-Differently-for-Detectors"><a href="#ICML2024-DFD-Distillng-the-Feature-Disparity-Differently-for-Detectors" class="headerlink" title="ICML2024: DFD: Distillng the Feature Disparity Differently for Detectors"></a><a target="_blank" rel="noopener" href="https://openreview.net/forum?id=KI3JKFKciG">ICML2024: DFD: Distillng the Feature Disparity Differently for Detectors</a></h2><h3 id="Author-刘康"><a href="#Author-刘康" class="headerlink" title="Author: 刘康"></a>Author: 刘康</h3><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>Knowledge distillation is a widely adopted model compression technique that has been successfully applied to object detection. In feature distillation, it is common practice for the student model to imitate the feature responses of the teacher model, with the underlying objective of improving its own abilities by reducing the disparity with the teacher. However, it is crucial to recognize that the disparities between the student and teacher are inconsistent, highlighting their varying abilities. In this paper, we explore the inconsistency in the disparity between teacher and student feature maps and analyze their impact on the efficiency of the distillation. We find that regions with varying degrees of difference should be treated separately, with different distillation constraints applied accordingly. We introduce our distillation method called Disparity Feature Distillation(DFD). The core idea behind DFD is to apply different treatments to regions with varying learning difficulties, simultaneously incorporating leniency and strictness. It enables the student to better assimilate the teacher’s knowledge. Through extensive experiments, we demonstrate the effectiveness of our proposed DFD in achieving significant improvements. For instance, when applied to detectors based on ResNet50 such as RetinaNet, FasterRCNN, and RepPoints, our method enhances their mAP from 37.4%, 38.4%, 38.6% to 41.7%, 42.4%, 42.7%, respectively. Our approach also demonstrates substantial improvements on YOLO and ViT-based models. The code is available at <a target="_blank" rel="noopener" href="https://github.com/luckin99/DFD">https://github.com/luckin99/DFD</a>.</p>
</div>

<div class="note primary"><h2 id="IJCNN2024-Integrating-Local-Global-Features-for-Estimating-Shortest-Path-Distance-in-Large-Scale-Graphs"><a href="#IJCNN2024-Integrating-Local-Global-Features-for-Estimating-Shortest-Path-Distance-in-Large-Scale-Graphs" class="headerlink" title="IJCNN2024: Integrating Local &amp; Global Features for Estimating Shortest-Path Distance in Large-Scale Graphs"></a><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/10650344">IJCNN2024: Integrating Local &amp; Global Features for Estimating Shortest-Path Distance in Large-Scale Graphs</a></h2><h3 id="Author-王浩宇"><a href="#Author-王浩宇" class="headerlink" title="Author: 王浩宇"></a>Author: 王浩宇</h3><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>We propose an effective hybrid approach jointly leveraging local and global features for shortest-path (SP) distance estimation in domain-agnostic large-scale graphs. Previous works struggle to make estimations either from node-wise local embeddings or by compressing a global SP distance matrix, causing insufficient learning at some distance and loss of accuracy. Unlike them, we find a way to better preserve local distance on node embeddings, and then integrate them with a global process for accurate estimation at every distance. First, we propose a distance-consistent embedding method that better preserves the distance between each node and its local neighbors due to resampling node occurrence on random walks. Second, we train a feed-forward network with boosting techniques (FFN-BT) to estimate SP distance from these embeddings plus existing global features. Experimental results show that our approach averagely yields 10% improved accuracy and 20% reduced time when compared to existing methods on a broad class of graphs.</p>
</div>

<div class="note primary"><h2 id="IJCNN2024-MMR-Multi-Scale-Motion-Retargeting-Between-Skeleton-Agnostic-Characters"><a href="#IJCNN2024-MMR-Multi-Scale-Motion-Retargeting-Between-Skeleton-Agnostic-Characters" class="headerlink" title="IJCNN2024: MMR: Multi-Scale Motion Retargeting Between Skeleton-Agnostic Characters"></a><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/10650339">IJCNN2024: MMR: Multi-Scale Motion Retargeting Between Skeleton-Agnostic Characters</a></h2><h3 id="Author-王浩宇"><a href="#Author-王浩宇" class="headerlink" title="Author: 王浩宇"></a>Author: 王浩宇</h3><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>We present a simple yet effective method for skeleton-agnostic motion retargeting. Previous methods transfer motion between high-resolution meshes, failing to preserve the inherent local-part motions in the mesh. Addressing this issue, our proposed method learns the correspondence in a coarse-to-fine fashion by disentangling the retargeting process within multi-scale meshes. First, we propose a mesh-pooling module that pools the mesh representations for better motion transfer. This module improves the ability to handle small-part motion and preserves the local motion interdependence between neighboring mesh vertices. Furthermore, we leverage a multi-scale refinement procedure to complement missing mesh details by gradually refining the low-resolution mesh output with a higher-resolution one. We evaluate our method on several well-known 3D character datasets, and it yields an average improvement of 25% on point-wise mesh Euclidean distance (PMD) against the start-of-art method. Qualitative results show that our method is significantly helpful in preserving the moving consistency of different body parts on the target character due to disentangling body-part structures and mesh details in a multi-scale way.</p>
</div>

<div class="note primary"><h2 id="IJCNN2024-Error-Bound-Based-Noise-Schedule-Design-in-Diffusion-Models"><a href="#IJCNN2024-Error-Bound-Based-Noise-Schedule-Design-in-Diffusion-Models" class="headerlink" title="IJCNN2024: Error Bound Based Noise Schedule Design in Diffusion Models"></a><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/10650926">IJCNN2024: Error Bound Based Noise Schedule Design in Diffusion Models</a></h2><h3 id="Author-刘力源"><a href="#Author-刘力源" class="headerlink" title="Author: 刘力源"></a>Author: 刘力源</h3><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>Diffusion-based generative model currently serves as a mainstream generative method. The noise schedule has a significant impact on the training process of diffusion model, as it affects both the distribution of the noisy training set and the weights of the objective function at each noise level. In this paper, we design the noise schedule from the scope of reducing the final error upper bound of the reverse denoising process. By examining Monte Carlo training from a theoretical perspective, we establish an association between noise schedule and the upper bound of network output error. Furthermore, we derive the connection between network output and final error through reverse process. We design our noise schedule with the goal of reducing the upper bound of error combined with the correlation analysis of network output. Experimental results demonstrate that our noise schedule enhances perceptual quality on CIFAR-10, FFHQ-64x64 and AFHQv2-64x64. Our noise schedule achieves state-of-the-art FID score of 1.70 on CIFAR-10 unconditional generation task using discriminator guidance method. On FFHQ&#x2F;AFHQv2, using our noise schedule to retrain the pre-trained model can improve the sample quality at little training cost.</p>
</div>

<div class="note primary"><h2 id="IJCNN2024-Noise-Weighting-Phased-Prompt-Image-Editing"><a href="#IJCNN2024-Noise-Weighting-Phased-Prompt-Image-Editing" class="headerlink" title="IJCNN2024: Noise Weighting Phased Prompt Image Editing"></a><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/10651075">IJCNN2024: Noise Weighting Phased Prompt Image Editing</a></h2><h3 id="Author-徐国炜"><a href="#Author-徐国炜" class="headerlink" title="Author: 徐国炜"></a>Author: 徐国炜</h3><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>The remarkable performance of large-scale Text-to-Image generation(TI) models is evident in their ability to produce high-quality and diverse images. However, despite advancements, the field of image editing still faces challenges. Current methods struggle to strike a balance between fidelity and powerful editing capabilities. Moreover, approaches that do not involve fine-tuning fail to produce diverse editing results. We introduce Noise Weighting Phased Prompt Image Editing (NWPP), a method that excels in powerful editing, high fidelity, and diverse results without fine-tuning. Our approach involves a two-phase generation process. The first phase employs the original prompt to guide initial image editing, ensuring a layout resembling the original image. In the second phase, a noise-weighting technique based on the Cross-Attention map minimizes the impact of the target text on non-editing regions. Further enhancement is achieved through the integration of the KV injection module, expanding the editing capabilities and enabling diverse result generation. Experimental evaluations, conducted on both generated images and the COCO dataset, affirm the efficacy of our method.</p>
</div>

<div class="note primary"><h2 id="CVPR2024-Distilling-Semantic-Priors-from-SAM-to-Efficient-Image-Restoration-Models"><a href="#CVPR2024-Distilling-Semantic-Priors-from-SAM-to-Efficient-Image-Restoration-Models" class="headerlink" title="CVPR2024: Distilling Semantic Priors from SAM to Efficient Image Restoration Models"></a><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_Distilling_Semantic_Priors_from_SAM_to_Efficient_Image_Restoration_Models_CVPR_2024_paper.html">CVPR2024: Distilling Semantic Priors from SAM to Efficient Image Restoration Models</a></h2><h3 id="Author-张权"><a href="#Author-张权" class="headerlink" title="Author: 张权"></a>Author: 张权</h3><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>In image restoration (IR) leveraging semantic priors from segmentation models has been a common approach to improve performance. The recent segment anything model (SAM) has emerged as a powerful tool for extracting advanced semantic priors to enhance IR tasks. However the computational cost of SAM is prohibitive for IR compared to existing smaller IR models. The incorporation of SAM for extracting semantic priors considerably hampers the model inference efficiency. To address this issue we propose a general framework to distill SAM’s semantic knowledge to boost exiting IR models without interfering with their inference process. Specifically our proposed framework consists of the semantic priors fusion (SPF) scheme and the semantic priors distillation (SPD) scheme. SPF fuses two kinds of information between the restored image predicted by the original IR model and the semantic mask predicted by SAM for the refined restored image. SPD leverages a self-distillation manner to distill the fused semantic priors to boost the performance of original IR models. Additionallywe design a semantic-guided relation (SGR) module for SPD which ensures semantic feature representation space consistency to fully distill the priors. We demonstrate the effectiveness of our framework across multiple IR models and tasks including deraining deblurring and denoising.</p>
</div>

<div class="note primary"><h2 id="CVPR2024-CricaVPR-Cross-image-Correlation-aware-Representation-Learning-for-Visual-Place-Recognition"><a href="#CVPR2024-CricaVPR-Cross-image-Correlation-aware-Representation-Learning-for-Visual-Place-Recognition" class="headerlink" title="CVPR2024: CricaVPR: Cross-image Correlation-aware Representation Learning for Visual Place Recognition"></a><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2024/html/Lu_CricaVPR_Cross-image_Correlation-aware_Representation_Learning_for_Visual_Place_Recognition_CVPR_2024_paper.html">CVPR2024: CricaVPR: Cross-image Correlation-aware Representation Learning for Visual Place Recognition</a></h2><h3 id="Author-卢锋"><a href="#Author-卢锋" class="headerlink" title="Author: 卢锋"></a>Author: 卢锋</h3><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>Over the past decade most methods in visual place recognition (VPR) have used neural networks to produce feature representations. These networks typically produce a global representation of a place image using only this image itself and neglect the cross-image variations (e.g. viewpoint and illumination) which limits their robustness in challenging scenes. In this paper we propose a robust global representation method with cross-image correlation awareness for VPR named CricaVPR. Our method uses the attention mechanism to correlate multiple images within a batch. These images can be taken in the same place with different conditions or viewpoints or even captured from different places. Therefore our method can utilize the cross-image variations as a cue to guide the representation learning which ensures more robust features are produced. To further facilitate the robustness we propose a multi-scale convolution-enhanced adaptation method to adapt pre-trained visual foundation models to the VPR task which introduces the multi-scale local information to further enhance the cross-image correlation-aware representation. Experimental results show that our method outperforms state-of-the-art methods by a large margin with significantly less training time. The code is released at <a target="_blank" rel="noopener" href="https://github.com/Lu-Feng/CricaVPR">https://github.com/Lu-Feng/CricaVPR</a>.</p>
</div>

<div class="note primary"><h2 id="CVPR2024-A-Free-Lunch-for-Faster-and-Better-Data-Free-Meta-Learning"><a href="#CVPR2024-A-Free-Lunch-for-Faster-and-Better-Data-Free-Meta-Learning" class="headerlink" title="CVPR2024: A Free Lunch for Faster and Better Data-Free Meta-Learning"></a><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2024/html/Wei_FREE_Faster_and_Better_Data-Free_Meta-Learning_CVPR_2024_paper.html">CVPR2024: A Free Lunch for Faster and Better Data-Free Meta-Learning</a></h2><h3 id="Author-韦永贤"><a href="#Author-韦永贤" class="headerlink" title="Author: 韦永贤"></a>Author: 韦永贤</h3><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>Data-Free Meta-Learning (DFML) aims to extract knowledge from a collection of pre-trained models without requiring the original data presenting practical benefits in contexts constrained by data privacy concerns. Current DFML methods primarily focus on the data recovery from these pre-trained models. However they suffer from slow recovery speed and overlook gaps inherent in heterogeneous pre-trained models. In response to these challenges we introduce the Faster and Better Data-Free Meta-Learning (FREE) framework which contains: (i) a meta-generator for rapidly recovering training tasks from pre-trained models; and (ii) a meta-learner for generalizing to new unseen tasks. Specifically within the module Faster Inversion via Meta-Generator each pre-trained model is perceived as a distinct task. The meta-generator can rapidly adapt to a specific task in just five steps significantly accelerating the data recovery. Furthermore we propose Better Generalization via Meta-Learner and introduce an implicit gradient alignment algorithm to optimize the meta-learner. This is achieved as aligned gradient directions alleviate potential conflicts among tasks from heterogeneous pre-trained models. Empirical experiments on multiple benchmarks affirm the superiority of our approach marking a notable speed-up (20x) and performance enhancement (1.42% 4.78%) in comparison to the state-of-the-art.</p>
</div>

<div class="note primary"><h2 id="IEEE-T-MM-Negative-Sensitive-Framework-with-Semantic-Enhancement-for-Composed-Image-Retrieval"><a href="#IEEE-T-MM-Negative-Sensitive-Framework-with-Semantic-Enhancement-for-Composed-Image-Retrieval" class="headerlink" title="IEEE T-MM: Negative-Sensitive Framework with Semantic Enhancement for Composed Image Retrieval"></a><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/10493853">IEEE T-MM: Negative-Sensitive Framework with Semantic Enhancement for Composed Image Retrieval</a></h2><h3 id="Author-王依凡"><a href="#Author-王依凡" class="headerlink" title="Author: 王依凡"></a>Author: 王依凡</h3><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>Composed image retrieval is a challenging task in the field of multi-modal learning, aiming at measuring the similarities between target images and query images with modification sentences. Most previous methods either construct feature composition for the query image and modification text or concentrate on extracting cross-modal alignments. However, these methods are prone to neglect the negative impacts of the mismatched correspondences between the hybrid-modal query and target, which could be discriminative when comparing similar instances. Besides, localized textual representations are not fully explored when learning similarities between the query and the target. To overcome the above issues, we propose a Negative-Sensitive Framework with Semantic Enhancement (NSFSE) for mining the adaptive boundaries between matched and mismatched samples with comprehensive consideration of positive and negative correspondences. It can optimize the threshold dynamically based on distributions to explore the intrinsic characteristics of positive and negative correlations, which could further facilitate accurate similarity learning. A text-guided attention mechanism after infusing cross-modal affinities on localized word features is exploited in NSFSE to explore latent semantic-related visual similarity and cross-modal similarity simultaneously. The performance of extensive experiments and comprehensive analysis on three representative datasets CIRR, FashionIQ, and Fashion200 K demonstrate the effectiveness of negative mining of similarity with semantic enhancement in the proposed NSFSE.</p>
</div>

<div class="note primary"><h2 id="ICLR2024-Convolution-Meets-LoRA-Parameter-Efficient-Finetuning-for-Segment-Anything-Model"><a href="#ICLR2024-Convolution-Meets-LoRA-Parameter-Efficient-Finetuning-for-Segment-Anything-Model" class="headerlink" title="ICLR2024: Convolution Meets LoRA: Parameter Efficient Finetuning for Segment Anything Model"></a><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2401.17868">ICLR2024: Convolution Meets LoRA: Parameter Efficient Finetuning for Segment Anything Model</a></h2><h3 id="Author-钟子涵"><a href="#Author-钟子涵" class="headerlink" title="Author: 钟子涵"></a>Author: 钟子涵</h3><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>The Segment Anything Model (SAM) stands as a foundational framework for image segmentation. While it exhibits remarkable zero-shot generalization in typical scenarios, its advantage diminishes when applied to specialized domains like medical imagery and remote sensing. To address this limitation, this paper introduces Conv-LoRA, a simple yet effective parameter-efficient fine-tuning approach. By integrating ultra-lightweight convolutional parameters into Low-Rank Adaptation (LoRA), Conv-LoRA can inject image-related inductive biases into the plain ViT encoder, further reinforcing SAM’s local prior assumption. Notably, Conv-LoRA not only preserves SAM’s extensive segmentation knowledge but also revives its capacity of learning high-level image semantics, which is constrained by SAM’s foreground-background segmentation pretraining. Comprehensive experimentation across diverse benchmarks spanning multiple domains underscores Conv-LoRA’s superiority in adapting SAM to real-world semantic segmentation tasks.</p>
</div>

<div class="note primary"><h2 id="ICLR2024-Towards-Seamless-Adaptation-of-Pre-trained-Models-for-Visual-Place-Recognition"><a href="#ICLR2024-Towards-Seamless-Adaptation-of-Pre-trained-Models-for-Visual-Place-Recognition" class="headerlink" title="ICLR2024: Towards Seamless Adaptation of Pre-trained Models for Visual Place Recognition"></a><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2402.14505">ICLR2024: Towards Seamless Adaptation of Pre-trained Models for Visual Place Recognition</a></h2><h3 id="Author-卢锋"><a href="#Author-卢锋" class="headerlink" title="Author: 卢锋"></a>Author: 卢锋</h3><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>Recent studies show that vision models pre-trained in generic visual learning tasks with large-scale data can provide useful feature representations for a wide range of visual perception problems. However, few attempts have been made to exploit pre-trained foundation models in visual place recognition (VPR). Due to the inherent difference in training objectives and data between the tasks of model pre-training and VPR, how to bridge the gap and fully unleash the capability of pre-trained models for VPR is still a key issue to address. To this end, we propose a novel method to realize seamless adaptation of pre-trained models for VPR. Specifically, to obtain both global and local features that focus on salient landmarks for discriminating places, we design a hybrid adaptation method to achieve both global and local adaptation efficiently, in which only lightweight adapters are tuned without adjusting the pre-trained model. Besides, to guide effective adaptation, we propose a mutual nearest neighbor local feature loss, which ensures proper dense local features are produced for local matching and avoids time-consuming spatial verification in re-ranking. Experimental results show that our method outperforms the state-of-the-art methods with less training data and training time, and uses about only 3% retrieval runtime of the two-stage VPR methods with RANSAC-based spatial verification. It ranks 1st on the MSLS challenge leaderboard (at the time of submission). The code is released at <a target="_blank" rel="noopener" href="https://github.com/Lu-Feng/SelaVPR">https://github.com/Lu-Feng/SelaVPR</a>.</p>
</div>

<div class="note primary"><h2 id="AAAI2024-Efficient-Conditional-Diffusion-Model-with-Probability-Flow-Sampling-for-Image-Super-resolution"><a href="#AAAI2024-Efficient-Conditional-Diffusion-Model-with-Probability-Flow-Sampling-for-Image-Super-resolution" class="headerlink" title="AAAI2024: Efficient Conditional Diffusion Model with Probability Flow Sampling for Image Super-resolution"></a><a target="_blank" rel="noopener" href="https://ojs.aaai.org/index.php/AAAI/article/view/28511">AAAI2024: Efficient Conditional Diffusion Model with Probability Flow Sampling for Image Super-resolution</a></h2><h3 id="Author-袁宇韬"><a href="#Author-袁宇韬" class="headerlink" title="Author: 袁宇韬"></a>Author: 袁宇韬</h3><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>Image super-resolution is a fundamentally ill-posed problem because multiple valid high-resolution images exist for one low-resolution image. Super-resolution methods based on diffusion probabilistic models can deal with the ill-posed nature by learning the distribution of high-resolution images conditioned on low-resolution images, avoiding the problem of blurry images in PSNR-oriented methods. However, existing diffusion-based super-resolution methods have high time consumption with the use of iterative sampling, while the quality and consistency of generated images are less than ideal due to problems like color shifting. In this paper, we propose Efficient Conditional Diffusion Model with Probability Flow Sampling (ECDP) for image super-resolution. To reduce the time consumption, we design a continuous-time conditional diffusion model for image super-resolution, which enables the use of probability flow sampling for efficient generation. Additionally, to improve the consistency of generated images, we propose a hybrid parametrization for the denoiser network, which interpolates between the data-predicting parametrization and the noise-predicting parametrization for different noise scales. Moreover, we design an image quality loss as a complement to the score matching loss of diffusion models, further improving the consistency and quality of super-resolution. Extensive experiments on DIV2K, ImageNet, and CelebA demonstrate that our method achieves higher super-resolution quality than existing diffusion-based image super-resolution methods while having lower time consumption. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Yuan-Yutao/ECDP">https://github.com/Yuan-Yutao/ECDP</a>.</p>
</div>

<div class="note primary"><h2 id="AAAI2024-Blind-Face-Restoration-under-Extreme-Conditions-Leveraging-3D-2D-Prior-Fusion-for-Superior-Structural-and-Texture-Recovery"><a href="#AAAI2024-Blind-Face-Restoration-under-Extreme-Conditions-Leveraging-3D-2D-Prior-Fusion-for-Superior-Structural-and-Texture-Recovery" class="headerlink" title="AAAI2024: Blind Face Restoration under Extreme Conditions: Leveraging 3D-2D Prior Fusion for Superior Structural and Texture Recovery"></a><a target="_blank" rel="noopener" href="https://ojs.aaai.org/index.php/AAAI/article/view/27889">AAAI2024: Blind Face Restoration under Extreme Conditions: Leveraging 3D-2D Prior Fusion for Superior Structural and Texture Recovery</a></h2><h3 id="Author-袁梓洋"><a href="#Author-袁梓洋" class="headerlink" title="Author: 袁梓洋"></a>Author: 袁梓洋</h3><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>Blind face restoration under extreme conditions involves reconstructing high-quality face images from severely degraded inputs. These input images are often in poor quality and have extreme facial poses, leading to errors in facial structure and unnatural artifacts within the restored images. In this paper, we show that utilizing 3D priors effectively compensates for structure knowledge deficiencies in 2D priors while preserving the texture details. Based on this, we introduce FREx (Face Restoration under Extreme conditions) that combines structure-accurate 3D priors and texture-rich 2D priors in pretrained generative networks for blind face restoration under extreme conditions. To fuse the different information in 3D and 2D priors, we introduce an adaptive weight module that adjusts the importance of features based on the input image’s condition. With this approach, our model can restore structure-accurate and natural-looking faces even when the images have lost a lot of information due to degradation and extreme pose. Extensive experimental results on synthetic and real-world datasets validate the effectiveness of our methods.</p>
</div>

<div class="note primary"><h2 id="AAAI2024-Mean-Teacher-DETR-with-Masked-Feature-Alignment-A-Robust-Domain-Adaptive-Detection-Transformer-Framework"><a href="#AAAI2024-Mean-Teacher-DETR-with-Masked-Feature-Alignment-A-Robust-Domain-Adaptive-Detection-Transformer-Framework" class="headerlink" title="AAAI2024: Mean Teacher DETR with Masked Feature Alignment: A Robust Domain Adaptive Detection Transformer Framework"></a><a target="_blank" rel="noopener" href="https://ojs.aaai.org/index.php/AAAI/article/view/28405">AAAI2024: Mean Teacher DETR with Masked Feature Alignment: A Robust Domain Adaptive Detection Transformer Framework</a></h2><h3 id="Author-翁玮熙"><a href="#Author-翁玮熙" class="headerlink" title="Author: 翁玮熙"></a>Author: 翁玮熙</h3><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>Unsupervised domain adaptation object detection(UDAOD) research on Detection Transformer(DETR) mainly focuses on feature alignment and existing methods can be divided into two kinds, each of which has its unresolved issues. One-stage feature alignment methods can easily lead to performance fluctuation and training stagnation. Two-stage feature alignment method based on mean teacher comprises a pretraining stage followed by a self-training stage, each facing problems in obtaining reliable pretrained model and achieving consistent performance gains. Methods mentioned above have not yet explore how to utilize the third related domain such as target-like domain to assist adaptation. To address these issues, we propose a two-stage framework named MTM, i.e. Mean Teacher-DETR with Masked Feature Alignment. In the pretraining stage, we utilize labeled target-like images produced by image style transfer to avoid performance fluctuation. In the self-training stage, we leverage unlabeled target images by pseudo labels based on mean teacher and propose a module called Object Queries Knowledge Transfer(OQKT) to ensure consistent performance gains of the student model. Most importantly, we propose masked feature alignment methods including Masked Domain Query-based Feature Alignment(MDQFA) and Masked Token-wise Feature Alignment(MTWFA) to alleviate domain shift in a more robust way, which not only prevent training stagnation and lead to a robust pretrained model in the pretraining stage, but also enhance the model’s target performance in the self-training stage. Experiments on three challenging scenarios and a theoretical analysis verify the effectiveness of MTM.</p>
</div>

<div class="note primary"><h2 id="AAAI2024-Deep-Homography-Estimation-for-Visual-Place-Recognition"><a href="#AAAI2024-Deep-Homography-Estimation-for-Visual-Place-Recognition" class="headerlink" title="AAAI2024: Deep Homography Estimation for Visual Place Recognition"></a><a target="_blank" rel="noopener" href="https://ojs.aaai.org/index.php/AAAI/article/view/28901">AAAI2024: Deep Homography Estimation for Visual Place Recognition</a></h2><h3 id="Author-卢锋"><a href="#Author-卢锋" class="headerlink" title="Author: 卢锋"></a>Author: 卢锋</h3><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>Visual place recognition (VPR) is a fundamental task for many applications such as robot localization and augmented reality. Recently, the hierarchical VPR methods have received considerable attention due to the trade-off between accuracy and efficiency. They usually first use global features to retrieve the candidate images, then verify the spatial consistency of matched local features for re-ranking. However, the latter typically relies on the RANSAC algorithm for fitting homography, which is time-consuming and non-differentiable. This makes existing methods compromise to train the network only in global feature extraction. Here, we propose a transformer-based deep homography estimation (DHE) network that takes the dense feature map extracted by a backbone network as input and fits homography for fast and learnable geometric verification. Moreover, we design a re-projection error of inliers loss to train the DHE network without additional homography labels, which can also be jointly trained with the backbone network to help it extract the features that are more suitable for local matching. Extensive experiments on benchmark datasets show that our method can outperform several state-of-the-art methods. And it is more than one order of magnitude faster than the mainstream hierarchical VPR methods using RANSAC. The code is released at <a target="_blank" rel="noopener" href="https://github.com/Lu-Feng/DHE-VPR">https://github.com/Lu-Feng/DHE-VPR</a>.</p>
</div>

<div class="note primary"><h2 id="IEEE-T-MM-Towards-Effective-Collaborative-Learning-in-Long-Tailed-Recognition"><a href="#IEEE-T-MM-Towards-Effective-Collaborative-Learning-in-Long-Tailed-Recognition" class="headerlink" title="IEEE T-MM: Towards Effective Collaborative Learning in Long-Tailed Recognition"></a><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/10250973">IEEE T-MM: Towards Effective Collaborative Learning in Long-Tailed Recognition</a></h2><h3 id="Author-许正卓"><a href="#Author-许正卓" class="headerlink" title="Author: 许正卓"></a>Author: 许正卓</h3><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>Real-world data usually suffers from severe class imbalance and long-tailed distributions, where minority classes are significantly underrepresented compared to the majority ones. Recent research prefers to utilize multi-expert architectures to mitigate the model uncertainty on the minority, where collaborative learning is employed to aggregate the knowledge of experts, i.e., online distillation. In this article, we observe that the knowledge transfer between experts is imbalanced in terms of class distribution, which results in limited performance improvement of the minority classes. To address it, we propose a re-weighted distillation loss by comparing two classifiers’ predictions, which are supervised by online distillation and label annotations, respectively. We also emphasize that feature-level distillation will significantly improve model performance and increase feature robustness. Finally, we propose an Effective Collaborative Learning (ECL) framework that integrates a contrastive proxy task branch to further improve feature quality. Quantitative and qualitative experiments on four standard datasets demonstrate that ECL achieves state-of-the-art performance and the detailed ablation studies manifest the effectiveness of each component in ECL.</p>
</div>

<div class="note primary"><h2 id="ACL2023-LET-Leveraging-Error-Type-Information-for-Grammatical-Error-Correction"><a href="#ACL2023-LET-Leveraging-Error-Type-Information-for-Grammatical-Error-Correction" class="headerlink" title="ACL2023: LET: Leveraging Error Type Information for Grammatical Error Correction"></a><a target="_blank" rel="noopener" href="https://aclanthology.org/2023.findings-acl.371/">ACL2023: LET: Leveraging Error Type Information for Grammatical Error Correction</a></h2><h3 id="Author-李泓嘉"><a href="#Author-李泓嘉" class="headerlink" title="Author: 李泓嘉"></a>Author: 李泓嘉</h3><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>Grammatical error correction (GEC) aims to correct errors in given sentences and is significant to many downstream natural language understanding tasks. Recent work introduces the idea of grammatical error detection (GED) to improve the GEC task performance. In contrast, these explicit multi-stage works propagate and amplify the problem of misclassification of the GED module. To introduce more convincing error type information, we propose an end-to-end framework in this paper, which Leverages Error Type (LET) information in the generation process. First, the input text is fed into a classification module to obtain the error type corresponding to each token. Then, we introduce the category information into the decoder’s input and cross-attention module in two ways, respectively. Experiments on various datasets show that our proposed method outperforms existing methods by a clear margin.</p>
</div>

<div class="note primary"><h2 id="ACL2023-Tailoring-Instructions-to-Student’s-Learning-Levels-Boosts-Knowledge-Distillation"><a href="#ACL2023-Tailoring-Instructions-to-Student’s-Learning-Levels-Boosts-Knowledge-Distillation" class="headerlink" title="ACL2023: Tailoring Instructions to Student’s Learning Levels Boosts Knowledge Distillation"></a><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.09651">ACL2023: Tailoring Instructions to Student’s Learning Levels Boosts Knowledge Distillation</a></h2><h3 id="Author-任昱鑫"><a href="#Author-任昱鑫" class="headerlink" title="Author: 任昱鑫"></a>Author: 任昱鑫</h3><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>It has been commonly observed that a teacher model with superior performance does not necessarily result in a stronger student, highlighting a discrepancy between current teacher training practices and effective knowledge transfer. In order to enhance the guidance of the teacher training process, we introduce the concept of distillation influence to determine the impact of distillation from each training sample on the student’s generalization ability. In this paper, we propose Learning Good Teacher Matters (LGTM), an efficient training technique for incorporating distillation influence into the teacher’s learning process. By prioritizing samples that are likely to enhance the student’s generalization ability, our LGTM outperforms 10 common knowledge distillation baselines on 6 text classification tasks in the GLUE benchmark.</p>
</div>

<div class="note primary"><h2 id="IEEE-T-MM-HHF-Hashing-guided-Hinge-Function-for-Deep-Hashing-Retrieval"><a href="#IEEE-T-MM-HHF-Hashing-guided-Hinge-Function-for-Deep-Hashing-Retrieval" class="headerlink" title="IEEE T-MM: HHF: Hashing-guided Hinge Function for Deep Hashing Retrieval"></a><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9953581">IEEE T-MM: HHF: Hashing-guided Hinge Function for Deep Hashing Retrieval</a></h2><h3 id="Author-徐呈寅"><a href="#Author-徐呈寅" class="headerlink" title="Author: 徐呈寅"></a>Author: 徐呈寅</h3><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>Deep hashing has shown promising performance in large-scale image retrieval. The hashing process utilizes Deep Neural Networks (DNNs) to embed images into compact continuous latent codes, then map them into binary codes by hashing function for efficient retrieval. Recent approaches perform metric loss and quantization loss to supervise the two procedures that cluster samples with the same categories and alleviate semantic information loss after binarization in the end-to-end training framework. However, we observe the incompatible conflict that the optimal cluster positions are not identical to the ideal hash positions because of the different objectives of the two loss terms, which lead to severe ambiguity and error-hashing after the binarization process. To address the problem, we borrow the Theory of Minimum-Distance Bounds for Binary Linear Codes to design the inflection point that depends on the hash bit length and category numbers and thereby propose Hashing-guided Hinge Function (HHF) to explicitly enforce the termination of metric loss to prevent the negative pairs unlimited alienated. Such modification is proven effective and essential for training, which contributes to proper intra- and inter-distances for clusters and better hash positions for accurate image retrieval simultaneously. Extensive experiments in CIFAR-10, CIFAR-100, ImageNet, and MS-COCO justify that HHF consistently outperforms existing techniques and is robust and flexible to transplant into other methods. Code is available at <a target="_blank" rel="noopener" href="https://github.com/JerryXu0129/HHF">https://github.com/JerryXu0129/HHF</a> .</p>
</div>

<div class="note primary"><h2 id="ACM-MM2023-Enhanced-Image-Deblurring-An-Efficient-Frequency-Exploitation-and-Preservation-Network"><a href="#ACM-MM2023-Enhanced-Image-Deblurring-An-Efficient-Frequency-Exploitation-and-Preservation-Network" class="headerlink" title="ACM MM2023: Enhanced Image Deblurring: An Efficient Frequency Exploitation and Preservation Network"></a><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/abs/10.1145/3581783.3611976">ACM MM2023: Enhanced Image Deblurring: An Efficient Frequency Exploitation and Preservation Network</a></h2><h3 id="Author-董姝婷"><a href="#Author-董姝婷" class="headerlink" title="Author: 董姝婷"></a>Author: 董姝婷</h3><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>Most of these frequency-based deblurring methods mainly have two major limitations: (1) insufficient exploitation of frequency information, (2) inadequate preservation of frequency information. In this paper, we propose a novel Efficient Frequency Exploitation and Preservation Network (EFEP) to address these limitations. Firstly, we propose a novel Frequency-Balanced Exploitation Encoder (FBE-Encoder) to sufficiently exploit frequency information. We insert a novel Frequency-Balanced Navigator (FBN) module in the encoder, which establishes a dynamic balance that adaptively explores and integrates the correlations between frequency features and other features presented in the network. And it also can highlight the most important regions in frequency features. Secondly, considering the limitation that frequency information is inevitably lost in deep network architectures, we present an Enhanced Selective Frequency Decoder (ESF-Decoder) that not only effectively reduces spatial information redundancy, but also fully explores the different importance of various frequency information to ensure the supplement of valid spatial information and weaken the invalid information. Thirdly, each encoder&#x2F;decoder block of the EFEP consists of multiple Contrastive Residual Blocks (CRBs), which are designed to explicitly compute and incorporate feature distinctions. Powered by the above designs, our EFEP outperforms state-of-the-art models on both quantitative and qualitative evaluations.</p>
</div>

<div class="note primary"><h2 id="IJCAI2023-DFVSR-Directional-Frequency-Video-Super-Resolution-via-Asymmetric-and-Enhancement-Alignment-Network"><a href="#IJCAI2023-DFVSR-Directional-Frequency-Video-Super-Resolution-via-Asymmetric-and-Enhancement-Alignment-Network" class="headerlink" title="IJCAI2023: DFVSR: Directional Frequency Video Super-Resolution via Asymmetric and Enhancement Alignment Network"></a><a target="_blank" rel="noopener" href="https://www.ijcai.org/proceedings/2023/0076.pdf">IJCAI2023: DFVSR: Directional Frequency Video Super-Resolution via Asymmetric and Enhancement Alignment Network</a></h2><h3 id="Author-董姝婷"><a href="#Author-董姝婷" class="headerlink" title="Author: 董姝婷"></a>Author: 董姝婷</h3><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>Recently, techniques utilizing frequency-based methods have gained signifcant attention, as they exhibit exceptional restoration capabilities for detail and structure in video super-resolution tasks. However, most of these frequency-based methods mainly have three major limitations: 1) insuffcient exploration of object motion information, 2) inadequate enhancement for high-fdelity regions, and 3) loss of spatial information during convolution. In this paper, we propose a novel network, Directional Frequency Video Super-Resolution (DFVSR), to address these limitations. Specifcally, we reconsider object motion from a new perspective and propose Directional Frequency Representation (DFR), which not only borrows the property of frequency representation of detail and structure information but also contains the direction information of the object motion that is extremely significant in videos. Based on this representation, we propose a Directional Frequency-Enhanced Alignment (DFEA) to use double enhancements of task-related information for ensuring the retention of high-fdelity frequency regions to generate the high-quality alignment feature. Furthermore, we design a novel Asymmetrical U-shaped network architecture to progressively fuse these alignment features and output the fnal output. This architecture enables the intercommunication of the same level of resolution in the encoder and decoder to achieve the supplement of spatial information. Powered by the above designs, our method achieves superior performance over state-of-the-art models on both quantitative and qualitative evaluations.</p>
</div>

<div class="note primary"><h2 id="ACM-MM2023-Adaptive-Contrastive-Learning-for-Learning-Robust-Representations-under-Label-Noise"><a href="#ACM-MM2023-Adaptive-Contrastive-Learning-for-Learning-Robust-Representations-under-Label-Noise" class="headerlink" title="ACM MM2023: Adaptive Contrastive Learning for Learning Robust Representations under Label Noise"></a><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/abs/10.1145/3581783.3612491">ACM MM2023: Adaptive Contrastive Learning for Learning Robust Representations under Label Noise</a></h2><h3 id="Author-王子浩"><a href="#Author-王子浩" class="headerlink" title="Author: 王子浩"></a>Author: 王子浩</h3><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>Deep Neural Networks suffer significant performance degeneration when noisy labels corrupt latent data representations. Previous work has attempted to alleviate this problem by exploiting contrastive learning, the pair building of which is critical. However, existing methods either conduct sample-level processes and then use the resultant subset to construct pairs or directly perform pair-level selecting using a fixed threshold, both leading to sub-optimal pairing and subsequent representation learning. To address this issue, we propose a novel adaptive contrastive learning method (ACL) working at the pair level to select contrastive pairs adaptively. Specifically, we consider the model’s learning status to adjust the confidence threshold in a self-adaptive manner instead of fixing it. Then, towards the ineffectiveness of the thresholding method on unconfident pairs, we automatically apply instance-specific temperature to boost the confidence of accurately-predicted samples and their pairs. We further introduce temporal cross-ensembling to handle the impact of noisy labels on model predictions. As a result, diverse pairs are correctly selected for contrastive learning to induce discriminative representations robust to various types of label noise. Extensive experimental results on several standard benchmarks and real-world datasets indicate the superiority of ACL, especially in extremely noisy scenarios.</p>
</div>

<div class="note primary"><h2 id="ICCV2023-HiFace-High-Fidelity-3D-Face-Reconstruction-by-Learning-Static-and-Dynamic-Details"><a href="#ICCV2023-HiFace-High-Fidelity-3D-Face-Reconstruction-by-Learning-Static-and-Dynamic-Details" class="headerlink" title="ICCV2023: HiFace: High-Fidelity 3D Face Reconstruction by Learning Static and Dynamic Details"></a><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/ICCV2023/html/Chai_HiFace_High-Fidelity_3D_Face_Reconstruction_by_Learning_Static_and_Dynamic_ICCV_2023_paper.html">ICCV2023: HiFace: High-Fidelity 3D Face Reconstruction by Learning Static and Dynamic Details</a></h2><h3 id="Author-柴增豪"><a href="#Author-柴增豪" class="headerlink" title="Author: 柴增豪"></a>Author: 柴增豪</h3><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>3D Morphable Models (3DMMs) demonstrate great potential for reconstructing faithful and animatable 3D facial surfaces from a single image. The facial surface is influenced by the coarse shape, as well as the static detail (e,g., person-specific appearance) and dynamic detail (e.g., expression-driven wrinkles). Previous work struggles to decouple the static and dynamic details through image-level supervision, leading to reconstructions that are not realistic. In this paper, we aim at high-fidelity 3D face reconstruction and propose HiFace to explicitly model the static and dynamic details. Specifically, the static detail is modeled as the linear combination of a displacement basis, while the dynamic detail is modeled as the linear interpolation of two displacement maps with polarized expressions. We exploit several loss functions to jointly learn the coarse shape and fine details with both synthetic and real-world datasets, which enable HiFace to reconstruct high-fidelity 3D shapes with animatable details. Extensive quantitative and qualitative experiments demonstrate that HiFace presents state-of-the-art reconstruction quality and faithfully recovers both the static and dynamic details.</p>
</div>

<div class="note primary"><h2 id="ICCV2023-Accurate-3D-Face-Reconstruction-with-Facial-Component-Tokens"><a href="#ICCV2023-Accurate-3D-Face-Reconstruction-with-Facial-Component-Tokens" class="headerlink" title="ICCV2023: Accurate 3D Face Reconstruction with Facial Component Tokens"></a><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Accurate_3D_Face_Reconstruction_with_Facial_Component_Tokens_ICCV_2023_paper.html">ICCV2023: Accurate 3D Face Reconstruction with Facial Component Tokens</a></h2><h3 id="Author-章天珂"><a href="#Author-章天珂" class="headerlink" title="Author: 章天珂"></a>Author: 章天珂</h3><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>Accurately reconstructing 3D faces from monocular images and videos is crucial for various applications, such as digital avatar creation. However, the current deep learning-based methods face significant challenges in achieving accurate reconstruction with disentangled facial parameters and ensuring temporal stability in single-frame methods for 3D face tracking on video data. In this paper, we propose TokenFace, a transformer-based monocular 3D face reconstruction model. TokenFace uses separate tokens for different facial components to capture information about different facial parameters and employs temporal transformers to capture temporal information from video data. This design can naturally disentangle different facial components and is flexible to both 2D and 3D training data. Trained on hybrid 2D and 3D data, our model shows its power in accurately reconstructing faces from images and producing stable results for video data. Experimental results on popular benchmarks NoW and Stirling demonstrate that TokenFace achieves state-of-the-art performance, outperforming existing methods on all metrics by a large margin.</p>
</div>

<div class="note primary"><h2 id="ICCVW2023-Effective-Whole-body-Pose-Estimation-with-Two-stages-Distillation"><a href="#ICCVW2023-Effective-Whole-body-Pose-Estimation-with-Two-stages-Distillation" class="headerlink" title="ICCVW2023: Effective Whole-body Pose Estimation with Two-stages Distillation"></a><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/ICCV2023W/CV4Metaverse/html/Yang_Effective_Whole-Body_Pose_Estimation_with_Two-Stages_Distillation_ICCVW_2023_paper.html">ICCVW2023: Effective Whole-body Pose Estimation with Two-stages Distillation</a></h2><h3 id="Author-杨震东"><a href="#Author-杨震东" class="headerlink" title="Author: 杨震东"></a>Author: 杨震东</h3><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>Whole-body pose estimation localizes the human body, hand, face, and foot keypoints in an image. This task is challenging due to multi-scale body parts, fine-grained localization for low-resolution regions, and data scarcity. Meanwhile, applying a highly efficient and accurate pose estimator to widely human-centric understanding and generation tasks is urgent. In this work, we present a two-stage pose Distillation for Whole-body Pose estimators, named DWPose, to improve their effectiveness and efficiency. The first-stage distillation designs a weight-decay strategy while utilizing a teacher’s intermediate feature and final logits with both visible and invisible keypoints to supervise the student from scratch. The second stage distills the student model itself to further improve performance. Different from the previous self-knowledge distillation, this stage finetunes the student’s head with only 20% training time as a plug-and-play training strategy. For data limitations, we explore the UBody dataset that contains diverse facial expressions and hand gestures for real-life applications. Comprehensive experiments show the superiority of our proposed simple yet effective methods. We achieve new state-of-the-art performance on COCO-WholeBody, significantly boosting the whole-body AP of RTMPose-l from 64.8% to 66.5%, even surpassing RTMPose-x teacher with 65.3% AP. We release a series of models with different sizes, from tiny to large, for satisfying various downstream tasks. Our code and models are available at <a target="_blank" rel="noopener" href="https://github.com/IDEA-Research/DWPose">https://github.com/IDEA-Research/DWPose</a>.</p>
</div>

<div class="note primary"><h2 id="ICCV2023-From-Knowledge-Distillation-to-Self-Knowledge-Distillation-A-Unified-Approach-with-Normalized-Loss-and-Customized-Soft-Labels"><a href="#ICCV2023-From-Knowledge-Distillation-to-Self-Knowledge-Distillation-A-Unified-Approach-with-Normalized-Loss-and-Customized-Soft-Labels" class="headerlink" title="ICCV2023: From Knowledge Distillation to Self-Knowledge Distillation: A Unified Approach with Normalized Loss and Customized Soft Labels"></a><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/ICCV2023/html/Yang_From_Knowledge_Distillation_to_Self-Knowledge_Distillation_A_Unified_Approach_with_ICCV_2023_paper.html">ICCV2023: From Knowledge Distillation to Self-Knowledge Distillation: A Unified Approach with Normalized Loss and Customized Soft Labels</a></h2><h3 id="Author-杨震东"><a href="#Author-杨震东" class="headerlink" title="Author: 杨震东"></a>Author: 杨震东</h3><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>Knowledge Distillation (KD) uses the teacher’s prediction logits as soft labels to guide the student, while self-KD does not need a real teacher to require the soft labels. This work unifies the formulations of the two tasks by decomposing and reorganizing the generic KD loss into a Normalized KD (NKD) loss and customized soft labels for both target class (image’s category) and non-target classes named Universal Self-Knowledge Distillation (USKD). We decompose the KD loss and find the non-target loss from it forces the student’s non-target logits to match the teacher’s, but the sum of the two non-target logits is different, preventing them from being identical. NKD normalizes the non-target logits to equalize their sum. It can be generally used for KD and self-KD to better use the soft labels for distillation loss. USKD generates customized soft labels for both target and non-target classes without a teacher. It smooths the target logit of the student as the soft target label and uses the rank of the intermediate feature to generate the soft non-target labels with Zipf’s law. For KD with teachers, our NKD achieves state-of-the-art performance on CIFAR-100 and ImageNet datasets, boosting the ImageNet Top-1 accuracy of ResNet18 from 69.90% to 71.96% with a ResNet-34 teacher. For self-KD without teachers, USKD is the first self-KD method that can be effectively applied to both CNN and ViT models with negligible additional time and memory cost, resulting in new state-of-the-art results, such as 1.17% and 0.55% accuracy gains on ImageNet for MobileNet and DeiT-Tiny, respectively. Code is available at <a target="_blank" rel="noopener" href="https://github.com/yzd-v/cls_KD">https://github.com/yzd-v/cls_KD</a>.</p>
</div>

<div class="note primary"><h2 id="ICCV2023-Make-Encoder-Great-Again-in-3D-GAN-Inversion-through-Geometry-and-Occlusion-Aware-Encoding"><a href="#ICCV2023-Make-Encoder-Great-Again-in-3D-GAN-Inversion-through-Geometry-and-Occlusion-Aware-Encoding" class="headerlink" title="ICCV2023: Make Encoder Great Again in 3D GAN Inversion through Geometry and Occlusion-Aware Encoding"></a><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/ICCV2023/html/Yuan_Make_Encoder_Great_Again_in_3D_GAN_Inversion_through_Geometry_ICCV_2023_paper.html">ICCV2023: Make Encoder Great Again in 3D GAN Inversion through Geometry and Occlusion-Aware Encoding</a></h2><h3 id="Author-袁梓洋"><a href="#Author-袁梓洋" class="headerlink" title="Author: 袁梓洋"></a>Author: 袁梓洋</h3><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>3D GAN inversion aims to achieve high reconstruction fidelity and reasonable 3D geometry simultaneously from a single image input. However, existing 3D GAN inversion methods rely on time-consuming optimization for each individual case. In this work, we introduce a novel encoder-based inversion framework based on EG3D, one of the most widely-used 3D GAN models. We leverage the inherent properties of EG3D’s latent space to design a discriminator and a background depth regularization. This enables us to train a geometry-aware encoder capable of converting the input image into corresponding latent code. Additionally, we explore the feature space of EG3D and develop an adaptive refinement stage that improves the representation ability of features in EG3D to enhance the recovery of fine-grained textural details. Finally, we propose an occlusion-aware fusion operation to prevent distortion in unobserved regions. Our method achieves impressive results comparable to optimization-based methods while operating up to 500 times faster. Our framework is well-suited for applications such as semantic editing.</p>
</div>

<div class="note primary"><h2 id="ICML2023-UPop-Unified-and-Progressive-Pruning-for-Compressing-Vision-Language-Transformers"><a href="#ICML2023-UPop-Unified-and-Progressive-Pruning-for-Compressing-Vision-Language-Transformers" class="headerlink" title="ICML2023: UPop: Unified and Progressive Pruning for Compressing Vision-Language Transformers"></a><a target="_blank" rel="noopener" href="https://proceedings.mlr.press/v202/shi23e.html">ICML2023: UPop: Unified and Progressive Pruning for Compressing Vision-Language Transformers</a></h2><h3 id="Author-石大川"><a href="#Author-石大川" class="headerlink" title="Author: 石大川"></a>Author: 石大川</h3><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>Real-world data contains a vast amount of multimodal information, among which vision and language are the two most representative modalities. Moreover, increasingly heavier models, e.g., Transformers, have attracted the attention of researchers to model compression. However, how to compress multimodal models, especially vison-language Transformers, is still under-explored. This paper proposes the Unified and Progressive Pruning (UPop) as a universal vison-language Transformer compression framework, which incorporates 1) unifiedly searching multimodal subnets in a continuous optimization space from the original model, which enables automatic assignment of pruning ratios among compressible modalities and structures; 2) progressively searching and retraining the subnet, which maintains convergence between the search and retrain to attain higher compression ratios. Experiments on various tasks, datasets, and model architectures demonstrate the effectiveness and versatility of the proposed UPop framework. The code is available at <a target="_blank" rel="noopener" href="https://github.com/sdc17/UPop">https://github.com/sdc17/UPop</a>.</p>
</div>

<div class="note primary"><h2 id="ICML2023-Learning-to-Learn-from-APIs-Black-Box-Data-Free-Meta-Learning"><a href="#ICML2023-Learning-to-Learn-from-APIs-Black-Box-Data-Free-Meta-Learning" class="headerlink" title="ICML2023: Learning to Learn from APIs: Black-Box Data-Free Meta-Learning"></a><a target="_blank" rel="noopener" href="https://proceedings.mlr.press/v202/hu23g.html">ICML2023: Learning to Learn from APIs: Black-Box Data-Free Meta-Learning</a></h2><h3 id="Author-胡梓轩"><a href="#Author-胡梓轩" class="headerlink" title="Author: 胡梓轩"></a>Author: 胡梓轩</h3><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>Data-free meta-learning (DFML) aims to enable efficient learning of new tasks by meta-learning from a collection of pre-trained models without access to the training data. Existing DFML work can only meta-learn from (i) white-box and (ii) small-scale pre-trained models (iii) with the same architecture, neglecting the more practical setting where the users only have inference access to the APIs with arbitrary model architectures and model scale inside. To solve this issue, we propose a Bi-level Data-free Meta Knowledge Distillation (BiDf-MKD) framework to transfer more general meta knowledge from a collection of black-box APIs to one single meta model. Specifically, by just querying APIs, we inverse each API to recover its training data via a zero-order gradient estimator and then perform meta-learning via a novel bi-level meta knowledge distillation structure, in which we design a boundary query set recovery technique to recover a more informative query set near the decision boundary. In addition, to encourage better generalization within the setting of limited API budgets, we propose task memory replay to diversify the underlying task distribution by covering more interpolated tasks. Extensive experiments in various real-world scenarios show the superior performance of our BiDf-MKD framework.</p>
</div>

<div class="note primary"><h2 id="ICRA2023-AANet-Aggregation-and-Alignment-Network-with-Semi-hard-Positive-Sample-Mining-for-Hierarchical-Place-Recognition"><a href="#ICRA2023-AANet-Aggregation-and-Alignment-Network-with-Semi-hard-Positive-Sample-Mining-for-Hierarchical-Place-Recognition" class="headerlink" title="ICRA2023: AANet: Aggregation and Alignment Network with Semi-hard Positive Sample Mining for Hierarchical Place Recognition"></a><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/10160734">ICRA2023: AANet: Aggregation and Alignment Network with Semi-hard Positive Sample Mining for Hierarchical Place Recognition</a></h2><h3 id="Author-卢锋"><a href="#Author-卢锋" class="headerlink" title="Author: 卢锋"></a>Author: 卢锋</h3><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>Visual place recognition (VPR) is one of the research hotspots in robotics, which uses visual information to locate robots. Recently, the hierarchical two-stage VPR methods have become popular in this field due to the trade-off between accuracy and efficiency. These methods retrieve the top-k candidate images using the global features in the first stage, then re-rank the candidates by matching the local features in the second stage. However, they usually require additional al-gorithms (e.g. RANSAC) for geometric consistency verification in re-ranking, which is time-consuming. Here we propose a Dynamically Aligning Local Features (DALF) algorithm to align the local features under spatial constraints. It is significantly more efficient than the methods that need geometric consistency verification. We present a unified network capable of extracting global features for retrieving candidates via an aggregation module and aligning local features for re-ranking via the DALF alignment module. We call this network AANet. Meanwhile, many works use the simplest positive samples in triplet for weakly supervised training, which limits the ability of the network to recognize harder positive pairs. To address this issue, we propose a Semi-hard Positive Sample Mining (ShPSM) strategy to select appropriate hard positive images for training more robust VPR networks. Extensive experiments on four benchmark VPR datasets show that the proposed AANet can outperform several state-of-the-art methods with less time consumption. The code is released at <a target="_blank" rel="noopener" href="https://github.com/Lu-Feng/AANet">https://github.com/Lu-Feng/AANet</a>.</p>
</div>

<div class="note primary"><h2 id="CVPR2023-High-fidelity-Facial-Avatar-Reconstruction-from-Monocular-Video-with-Generative-Priors"><a href="#CVPR2023-High-fidelity-Facial-Avatar-Reconstruction-from-Monocular-Video-with-Generative-Priors" class="headerlink" title="CVPR2023: High-fidelity Facial Avatar Reconstruction from Monocular Video with Generative Priors"></a><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2023/html/Bai_High-Fidelity_Facial_Avatar_Reconstruction_From_Monocular_Video_With_Generative_Priors_CVPR_2023_paper.html">CVPR2023: High-fidelity Facial Avatar Reconstruction from Monocular Video with Generative Priors</a></h2><h3 id="Author-白云鹏"><a href="#Author-白云鹏" class="headerlink" title="Author: 白云鹏"></a>Author: 白云鹏</h3><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>High-fidelity facial avatar reconstruction from a monocular video is a significant research problem in computer graphics and computer vision. Recently, Neural Radiance Field (NeRF) has shown impressive novel view rendering results and has been considered for facial avatar reconstruction. However, the complex facial dynamics and missing 3D information in monocular videos raise significant challenges for faithful facial reconstruction. In this work, we propose a new method for NeRF-based facial avatar reconstruction that utilizes 3D-aware generative prior. Different from existing works that depend on a conditional deformation field for dynamic modeling, we propose to learn a personalized generative prior, which is formulated as a local and low dimensional subspace in the latent space of 3D-GAN. We propose an efficient method to construct the personalized generative prior based on a small set of facial images of a given individual. After learning, it allows for photo-realistic rendering with novel views, and the face reenactment can be realized by performing navigation in the latent space. Our proposed method is applicable for different driven signals, including RGB images, 3DMM coefficients, and audio. Compared with existing works, we obtain superior novel view synthesis results and faithfully face reenactment performance. The code is available here <a target="_blank" rel="noopener" href="https://github.com/bbaaii/HFA-GP">https://github.com/bbaaii/HFA-GP</a>.</p>
</div>

<div class="note primary"><h2 id="CVPR2023-Learning-Imbalanced-Data-with-Vision-Transformers"><a href="#CVPR2023-Learning-Imbalanced-Data-with-Vision-Transformers" class="headerlink" title="CVPR2023: Learning Imbalanced Data with Vision Transformers"></a><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2023/html/Xu_Learning_Imbalanced_Data_With_Vision_Transformers_CVPR_2023_paper.html">CVPR2023: Learning Imbalanced Data with Vision Transformers</a></h2><h3 id="Author-许正卓"><a href="#Author-许正卓" class="headerlink" title="Author: 许正卓"></a>Author: 许正卓</h3><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>The real-world data tends to be heavily imbalanced and severely skew the data-driven deep neural networks, which makes Long-Tailed Recognition (LTR) a massive challenging task. Existing LTR methods seldom train Vision Transformers (ViTs) with Long-Tailed (LT) data, while the off-the-shelf pretrain weight of ViTs always leads to unfair comparisons. In this paper, we systematically investigate the ViTs’ performance in LTR and propose LiVT to train ViTs from scratch only with LT data. With the observation that ViTs suffer more severe LTR problems, we conduct Masked Generative Pretraining (MGP) to learn generalized features. With ample and solid evidence, we show that MGP is more robust than supervised manners. Although Binary Cross Entropy (BCE) loss performs well with ViTs, it struggles on the LTR tasks. We further propose the balanced BCE to ameliorate it with strong theoretical groundings. Specially, we derive the unbiased extension of Sigmoid and compensate extra logit margins for deploying it. Our Bal-BCE contributes to the quick convergence of ViTs in just a few epochs. Extensive experiments demonstrate that with MGP and Bal-BCE, LiVT successfully trains ViTs well without any additional data and outperforms comparable state-of-the-art methods significantly, e.g., our ViT-B achieves 81.0% Top-1 accuracy in iNaturalist 2018 without bells and whistles. Code is available at <a target="_blank" rel="noopener" href="https://github.com/XuZhengzhuo/LiVT">https://github.com/XuZhengzhuo/LiVT</a>.</p>
</div>

<div class="note primary"><h2 id="IEEE-TCSVT-Task-adaptive-Feature-Disentanglement-and-Hallucination-for-Few-shot-Classification"><a href="#IEEE-TCSVT-Task-adaptive-Feature-Disentanglement-and-Hallucination-for-Few-shot-Classification" class="headerlink" title="IEEE TCSVT: Task-adaptive Feature Disentanglement and Hallucination for Few-shot Classification"></a><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/10024322">IEEE TCSVT: Task-adaptive Feature Disentanglement and Hallucination for Few-shot Classification</a></h2><h3 id="Author-胡梓轩"><a href="#Author-胡梓轩" class="headerlink" title="Author: 胡梓轩"></a>Author: 胡梓轩</h3><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>Few-shot classification is a challenging task of computer vision and is critical to the data-sparse scenario like rare disease diagnosis. Feature augmentation is a straightforward way to alleviate the data-sparse issue in few-shot classification. However, mimicking the original feature distribution from a small amount of data is challenging. Existing augmentation-based methods are task-agnostic: the augmented feature is not with optimal intra-class diversity and inter-class discriminability concerning a certain task. To address this drawback, we propose a novel Task-adaptive Feature Disentanglement and Hallucination framework, dubbed TaFDH. Concretely, we first perceive the task information to disentangle the original feature into two components: class-irrelevant and class-specific features. Then more class-irrelevant features are decoded from a learned variational distribution, fused with the class-specific feature to get the augmented features. Finally, a generalized prior distribution over a quadratic classifier is meta-learned, which can be fast adapted to the class-specific posterior, thus further alleviating the inadequacy and uncertainty of feature hallucination via the nature of Bayesian inference. In this way, we construct a more discriminable embedding space with reasonable intra-class diversity instead of simply restoring the original embedding space, which can lead to a more precise decision boundary. We obtain the augmented features equipped with enhanced inter-class discriminability by highlighting the most discriminable part while boosting the intra-class diversity by fusing with the diverse generated class-irrelevant parts. Experiments on five multi-grained few-shot classification datasets demonstrate the superiority of our method.</p>
</div>

<div class="note primary"><h2 id="CVPR2023-Architecture-Dataset-and-Model-Scale-Agnostic-Data-free-Meta-Learning"><a href="#CVPR2023-Architecture-Dataset-and-Model-Scale-Agnostic-Data-free-Meta-Learning" class="headerlink" title="CVPR2023: Architecture, Dataset and Model-Scale Agnostic Data-free Meta-Learning"></a><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2023/html/Hu_Architecture_Dataset_and_Model-Scale_Agnostic_Data-Free_Meta-Learning_CVPR_2023_paper.html">CVPR2023: Architecture, Dataset and Model-Scale Agnostic Data-free Meta-Learning</a></h2><h3 id="Author-胡梓轩"><a href="#Author-胡梓轩" class="headerlink" title="Author: 胡梓轩"></a>Author: 胡梓轩</h3><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>The goal of data-free meta-learning is to learn useful prior knowledge from a collection of pre-trained models without accessing their training data. However, existing works only solve the problem in parameter space, which (i) ignore the fruitful data knowledge contained in the pre-trained models; (ii) can not scale to large-scale pre-trained models; (iii) can only meta-learn pre-trained models with the same network architecture. To address those issues, we propose a unified framework, dubbed PURER, which contains: (1) ePisode cUrriculum inveRsion (ECI) during data-free meta training; and (2) invErsion calibRation following inner loop (ICFIL) during meta testing. During meta training, we propose ECI to perform pseudo episode training for learning to adapt fast to new unseen tasks. Specifically, we progressively synthesize a sequence of pseudo episodes by distilling the training data from each pre-trained model. The ECI adaptively increases the difficulty level of pseudo episodes according to the real-time feedback of the meta model. We formulate the optimization process of meta training with ECI as an adversarial form in an end-to-end manner. During meta testing, we further propose a simple plug-and-play supplement–ICFIL–only used during meta testing to narrow the gap between meta training and meta testing task distribution. Extensive experiments in various real-world scenarios show the superior performance of ours.</p>
</div>

<div class="note primary"><h2 id="CVPR2023-Uni-Perceiver-v2-A-Generalist-Model-for-Large-Scale-Vision-and-Vision-Language-Tasks"><a href="#CVPR2023-Uni-Perceiver-v2-A-Generalist-Model-for-Large-Scale-Vision-and-Vision-Language-Tasks" class="headerlink" title="CVPR2023: Uni-Perceiver v2: A Generalist Model for Large-Scale Vision and Vision-Language Tasks"></a><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2023/html/Li_Uni-Perceiver_v2_A_Generalist_Model_for_Large-Scale_Vision_and_Vision-Language_CVPR_2023_paper.html">CVPR2023: Uni-Perceiver v2: A Generalist Model for Large-Scale Vision and Vision-Language Tasks</a></h2><h3 id="Author-江晓湖"><a href="#Author-江晓湖" class="headerlink" title="Author: 江晓湖"></a>Author: 江晓湖</h3><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>Despite the remarkable success of foundation models, their task-specific fine-tuning paradigm makes them inconsistent with the goal of general perception modeling. The key to eliminating this inconsistency is to use generalist models for general task modeling. However, existing attempts at generalist models are inadequate in both versatility and performance. In this paper, we propose Uni-Perceiver v2, which is the first generalist model capable of handling major large-scale vision and vision-language tasks with competitive performance. Specifically, images are encoded as general region proposals, while texts are encoded via a Transformer-based language model. The encoded representations are transformed by a task-agnostic decoder. Different tasks are formulated as a unified maximum likelihood estimation problem. We further propose an effective optimization technique named Task-Balanced Gradient Normalization to ensure stable multi-task learning with an unmixed sampling strategy, which is helpful for tasks requiring large batch-size training. After being jointly trained on various tasks, Uni-Perceiver v2 is capable of directly handling downstream tasks without any task-specific adaptation. Results show that Uni-Perceiver v2 outperforms all existing generalist models in both versatility and performance. Meanwhile, compared with the commonly-recognized strong baselines that require tasks-specific fine-tuning, Uni-Perceiver v2 achieves competitive performance on a broad range of vision and vision-language tasks.</p>
</div>

<div class="note primary"><h2 id="ICASSP2023-FREQUENCY-RECIPROCAL-ACTION-AND-FUSION-FOR-SINGLE-IMAGE-SUPER-RESOLUTION"><a href="#ICASSP2023-FREQUENCY-RECIPROCAL-ACTION-AND-FUSION-FOR-SINGLE-IMAGE-SUPER-RESOLUTION" class="headerlink" title="ICASSP2023: FREQUENCY RECIPROCAL ACTION AND FUSION FOR SINGLE IMAGE SUPER-RESOLUTION"></a><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/10096598">ICASSP2023: FREQUENCY RECIPROCAL ACTION AND FUSION FOR SINGLE IMAGE SUPER-RESOLUTION</a></h2><h3 id="Author-董姝婷"><a href="#Author-董姝婷" class="headerlink" title="Author: 董姝婷"></a>Author: 董姝婷</h3><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>Frequency-based methods have recently received much attention due to their impressive restoration of detail and structure in single image super-resolution (SISR). However, most of these methods mainly use frequency information as auxiliary means but ignore exploring the correlations and pixel distribution differences among various frequencies. To address the limitations, we propose a novel Frequency Reciprocal Action and Fusion Network (FRAF) that explores various frequency correlations and differences. Specifically, we design a Frequency Reciprocal Action (FRA) module, which safely enhances valid spatial information and decreases un-necessary repetition by reciprocal action among various spatial frequencies, to generate refined high- and low-frequency features. These refined frequency features are then progressively to guide the details and structure recovery, respectively. Furthermore, we develop a Detail and Structure Fusion (DSF) module to adaptively select, enhance and fuse the features to output the final HR image. This way ensures the final image is a high-quality product with rich details and a clear structure. Experimental results demonstrate that our method achieves superior performance over state-of-the-art (SOTA) approaches on both quantitative and qualitative evaluations.</p>
</div>

<div class="note primary"><h2 id="ICASSP2023-Rethink-Long-Tailed-Recognition-With-Vision-Transforms"><a href="#ICASSP2023-Rethink-Long-Tailed-Recognition-With-Vision-Transforms" class="headerlink" title="ICASSP2023: Rethink Long-Tailed Recognition With Vision Transforms"></a><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/10097154">ICASSP2023: Rethink Long-Tailed Recognition With Vision Transforms</a></h2><h3 id="Author-许正卓"><a href="#Author-许正卓" class="headerlink" title="Author: 许正卓"></a>Author: 许正卓</h3><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>In the real world, data tends to follow long-tailed distributions w.r.t. class or attribution, motivating the challenging Long-Tailed Recognition (LTR) problem. In this paper, we revisit recent LTR methods with promising Vision Transformers (ViT). We figure out that 1) ViT is hard to train with longtailed data. 2) ViT learns generalized features in an unsupervised manner, like mask generative training, either on longtailed or balanced datasets. Hence, we propose to adopt unsupervised learning to utilize long-tailed data. Furthermore, we propose the Predictive Distribution Calibration (PDC) as a novel metric for LTR, where the model tends to simply classify inputs into common classes. Our PDC can measure the model calibration of predictive preferences quantitatively. On this basis, we find many LTR approaches alleviate it slightly, despite the accuracy improvement. Extensive experiments on benchmark datasets validate that PDC reflects the model’s predictive preference precisely, which is consistent with the visualization.</p>
</div>

<div class="note primary"><h2 id="IEEE-TNNLS-PatchNet-Maximize-the-Exploration-of-Congeneric-Semantics-for-Weakly-Supervised-Semantic-Segmentation"><a href="#IEEE-TNNLS-PatchNet-Maximize-the-Exploration-of-Congeneric-Semantics-for-Weakly-Supervised-Semantic-Segmentation" class="headerlink" title="IEEE TNNLS: PatchNet: Maximize the Exploration of Congeneric Semantics for Weakly Supervised Semantic Segmentation"></a><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/10153071">IEEE TNNLS: PatchNet: Maximize the Exploration of Congeneric Semantics for Weakly Supervised Semantic Segmentation</a></h2><h3 id="Author-张可"><a href="#Author-张可" class="headerlink" title="Author: 张可"></a>Author: 张可</h3><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>With the increase in the number of image data and the lack of corresponding labels, weakly supervised learning has drawn a lot of attention recently in computer vision tasks, especially in the fine-grained semantic segmentation problem. To alleviate human efforts from expensive pixel-by-pixel annotations, our method focuses on weakly supervised semantic segmentation (WSSS) with image-level labels, which are much easier to obtain. As a considerable gap exists between pixel-level segmentation and image-level labels, how to reflect the image-level semantic information on each pixel is an important question. To explore the congeneric semantic regions from the same class to the maximum, we construct the patch-level semantic augmentation network (PatchNet) based on the self-detected patches from different images that contain the same class labels. Patches can frame the objects as much as possible and include as little background as possible. The patch-level semantic augmentation network that is established with patches as the nodes can maximize the mutual learning of similar objects. We regard the embedding vectors of patches as nodes and use a transformer-based complementary learning module to construct weighted edges according to the embedding similarity between different nodes. Moreover, to better supplement semantic information, we propose softcomplementary loss functions matched with the whole network structure. We conduct experiments on the popular PASCAL VOC 2012 and MS COCO 2014 benchmarks, and our model yields the state-of-the-art performance.</p>
</div>

<div class="note primary"><h2 id="AAAI2023-Truncate-Split-Contrast-A-Framework-for-Learning-from-Mislabeled-Videos"><a href="#AAAI2023-Truncate-Split-Contrast-A-Framework-for-Learning-from-Mislabeled-Videos" class="headerlink" title="AAAI2023: Truncate-Split-Contrast: A Framework for Learning from Mislabeled Videos"></a><a target="_blank" rel="noopener" href="https://ojs.aaai.org/index.php/AAAI/article/view/25375">AAAI2023: Truncate-Split-Contrast: A Framework for Learning from Mislabeled Videos</a></h2><h3 id="Author-王子啸"><a href="#Author-王子啸" class="headerlink" title="Author: 王子啸"></a>Author: 王子啸</h3><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>Learning with noisy label is a classic problem that has been extensively studied for image tasks, but much less for video in the literature. A straightforward migration from images to videos without considering temporal semantics and computational cost is not a sound choice. In this paper, we propose two new strategies for video analysis with noisy labels: 1) a lightweight channel selection method dubbed as Channel Truncation for feature-based label noise detection. This method selects the most discriminative channels to split clean and noisy instances in each category. 2) A novel contrastive strategy dubbed as Noise Contrastive Learning, which constructs the relationship between clean and noisy instances to regularize model training. Experiments on three well-known benchmark datasets for video classification show that our proposed truNcatE-split-contrAsT (NEAT) significantly outperforms the existing baselines. By reducing the dimension to 10% of it, our method achieves over 0.4 noise detection F1-score and 5% classification accuracy improvement on Mini-Kinetics dataset under severe noise (symmetric-80%). Thanks to Noise Contrastive Learning, the average classification accuracy improvement on Mini-Kinetics and Sth-Sth-V1 is over 1.6%.</p>
</div>

<div class="note primary"><h2 id="AAAI2023-Darwinian-Model-Upgrades-Model-Evolving-with-Selective-Compatibility"><a href="#AAAI2023-Darwinian-Model-Upgrades-Model-Evolving-with-Selective-Compatibility" class="headerlink" title="AAAI2023: Darwinian Model Upgrades: Model Evolving with Selective Compatibility"></a><a target="_blank" rel="noopener" href="https://ojs.aaai.org/index.php/AAAI/article/view/25447">AAAI2023: Darwinian Model Upgrades: Model Evolving with Selective Compatibility</a></h2><h3 id="Author-张斌杰"><a href="#Author-张斌杰" class="headerlink" title="Author: 张斌杰"></a>Author: 张斌杰</h3><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>The traditional model upgrading paradigm for retrieval requires recomputing all gallery embeddings before deploying the new model (dubbed as “backfilling”), which is quite expensive and time-consuming considering billions of instances in industrial applications. BCT presents the first step towards backward-compatible model upgrades to get rid of backfilling. It is workable but leaves the new model in a dilemma between new feature discriminativeness and new-to-old compatibility due to the undifferentiated compatibility constraints. In this work, we propose Darwinian Model Upgrades (DMU), which disentangle the inheritance and variation in the model evolving with selective backward compatibility and forward adaptation, respectively. The old-to-new heritable knowledge is measured by old feature discriminativeness, and the gallery features, especially those of poor quality, are evolved in a lightweight manner to become more adaptive in the new latent space. We demonstrate the superiority of DMU through comprehensive experiments on large-scale landmark retrieval and face recognition benchmarks. DMU effectively alleviates the new-to-new degradation at the same time improving new-to-old compatibility, rendering a more proper model upgrading paradigm in large-scale retrieval systems.Code: <a target="_blank" rel="noopener" href="https://github.com/TencentARC/OpenCompatible">https://github.com/TencentARC/OpenCompatible</a>.</p>
</div>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>





</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">CVML</span>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>







  





</body>
</html>
