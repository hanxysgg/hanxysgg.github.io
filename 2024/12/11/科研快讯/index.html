<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#660874"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#660874">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha256-5eIC48iZUHmSlSUz9XtjRyK2mzQkHScZY1WdMaoz74E=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"thu-cvml.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.21.1","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":-1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="ICCV2023: When Noisy Labels Meet Long Tail Dilemmas: A Representation Calibration MethodAuthor: 张曼怡AbstractReal-world large-scale datasets are both noisily labeled and class-imbalanced. The issues ser">
<meta property="og:type" content="article">
<meta property="og:title" content="2023-2025年科研成果展示">
<meta property="og:url" content="https://thu-cvml.github.io/2024/12/11/%E7%A7%91%E7%A0%94%E5%BF%AB%E8%AE%AF/index.html">
<meta property="og:site_name" content="清华大学&lt;br&gt;深圳国际研究生院&lt;br&gt;袁春教授课题组">
<meta property="og:description" content="ICCV2023: When Noisy Labels Meet Long Tail Dilemmas: A Representation Calibration MethodAuthor: 张曼怡AbstractReal-world large-scale datasets are both noisily labeled and class-imbalanced. The issues ser">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://raw.githubusercontent.com/THU-CVML/thu-cvml.github.io/refs/heads/main/images/RCAL.png">
<meta property="article:published_time" content="2024-12-11T08:34:09.000Z">
<meta property="article:modified_time" content="2024-12-17T15:14:16.405Z">
<meta property="article:author" content="袁春">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/THU-CVML/thu-cvml.github.io/refs/heads/main/images/RCAL.png">


<link rel="canonical" href="https://thu-cvml.github.io/2024/12/11/%E7%A7%91%E7%A0%94%E5%BF%AB%E8%AE%AF/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://thu-cvml.github.io/2024/12/11/%E7%A7%91%E7%A0%94%E5%BF%AB%E8%AE%AF/","path":"2024/12/11/科研快讯/","title":"2023-2025年科研成果展示"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>2023-2025年科研成果展示 | 清华大学<br>深圳国际研究生院<br>袁春教授课题组</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">清华大学<br>深圳国际研究生院<br>袁春教授课题组</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">计算机视觉与机器学习<br>Computer Vision and Machine Learning<br>CVML</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-overview"><a href="/Professor/" rel="section"><i class="fa fa-book fa-fw"></i>Overview</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="Searching..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#ICCV2023-When-Noisy-Labels-Meet-Long-Tail-Dilemmas-A-Representation-Calibration-Method"><span class="nav-number">1.</span> <span class="nav-text">ICCV2023: When Noisy Labels Meet Long Tail Dilemmas: A Representation Calibration Method</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Author-%E5%BC%A0%E6%9B%BC%E6%80%A1"><span class="nav-number">1.1.</span> <span class="nav-text">Author: 张曼怡</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract"><span class="nav-number">1.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#AAAI2025-Aligning-Composed-Query-with-Image-via-Discriminative-Perception-from-Negative-Correspondences"><span class="nav-number">2.</span> <span class="nav-text">AAAI2025: Aligning Composed Query with Image via Discriminative Perception from Negative Correspondences</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Author-%E7%8E%8B%E4%BE%9D%E5%87%A1"><span class="nav-number">2.1.</span> <span class="nav-text">Author: 王依凡</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract-1"><span class="nav-number">2.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#AAAI2025-Rethinking-Pseudo-Label-Guided-Learning-for-Weakly-Supervised-Temporal-Action-Localization-from-the-Perspective-of-Noise-Correction"><span class="nav-number">3.</span> <span class="nav-text">AAAI2025: Rethinking Pseudo-Label Guided Learning for Weakly-Supervised Temporal Action Localization from the Perspective of Noise Correction</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Author-%E5%BC%A0%E6%9D%83"><span class="nav-number">3.1.</span> <span class="nav-text">Author: 张权</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract-2"><span class="nav-number">3.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#NeurIPS2025-SuperVLAD-Compact-and-Robust-Image-Descriptors-for-Visual-Place-Recognition"><span class="nav-number">4.</span> <span class="nav-text">NeurIPS2025: SuperVLAD: Compact and Robust Image Descriptors for Visual Place Recognition</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Author-%E5%8D%A2%E9%94%8B"><span class="nav-number">4.1.</span> <span class="nav-text">Author: 卢锋</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract-3"><span class="nav-number">4.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ICPR2024-Benchmarking-AI-in-Mental-Health-A-Critical-Examination-of-LLMs-Across-Key-Performance-and-Ethical-Metrics"><span class="nav-number">5.</span> <span class="nav-text">ICPR2024: Benchmarking AI in Mental Health: A Critical Examination of LLMs Across Key Performance and Ethical Metrics</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Author-%E8%A2%81%E7%9D%BF"><span class="nav-number">5.1.</span> <span class="nav-text">Author: 袁睿</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract-4"><span class="nav-number">5.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ICPR2024-AMC-OA-Adaptive-Multi-Scale-Convolutional-Networks-with-Optimized-Attention-for-Temporal-Action-Localization"><span class="nav-number">6.</span> <span class="nav-text">ICPR2024: AMC-OA: Adaptive Multi-Scale Convolutional Networks with Optimized Attention for Temporal Action Localization</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Author-%E8%A2%81%E7%9D%BF-1"><span class="nav-number">6.1.</span> <span class="nav-text">Author: 袁睿</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract-5"><span class="nav-number">6.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ACMMM2024-Semantic-Distillation-from-Neighborhood-for-Composed-Image-Retrieval"><span class="nav-number">7.</span> <span class="nav-text">ACMMM2024: Semantic Distillation from Neighborhood for Composed Image Retrieval</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Author-%E7%8E%8B%E4%BE%9D%E5%87%A1-1"><span class="nav-number">7.1.</span> <span class="nav-text">Author: 王依凡</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract-6"><span class="nav-number">7.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ACM-MM2024-CustomNet-Object-Customization-with-Variable-Viewpoints-in-Text-to-Image-Diffusion-Models"><span class="nav-number">8.</span> <span class="nav-text">ACM MM2024: CustomNet: Object Customization with Variable-Viewpoints in Text-to-Image Diffusion Models</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Author-%E8%A2%81%E6%A2%93%E6%B4%8B"><span class="nav-number">8.1.</span> <span class="nav-text">Author: 袁梓洋</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract-7"><span class="nav-number">8.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ECCV2024-GVGEN-Text-to-3D-Generation-with-Volumetric-Representation"><span class="nav-number">9.</span> <span class="nav-text">ECCV2024: GVGEN: Text-to-3D Generation with Volumetric Representation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Author-%E4%BD%95%E7%9B%B8%E9%BE%99"><span class="nav-number">9.1.</span> <span class="nav-text">Author: 何相龙</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract-8"><span class="nav-number">9.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ECCV2024-A-Task-is-Worth-One-Word-Learning-with-Task-Prompts-for-High-Quality-Versatile-Image-Inpainting"><span class="nav-number">10.</span> <span class="nav-text">ECCV2024: A Task is Worth One Word: Learning with Task Prompts for High-Quality Versatile Image Inpainting</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Author-%E5%BA%84%E4%BF%8A%E8%B1%AA"><span class="nav-number">10.1.</span> <span class="nav-text">Author: 庄俊豪</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract-9"><span class="nav-number">10.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ECCV2024-DreamDiffusion-High-Quality-EEG-to-Image-Generation-with-Temporal-Masked-Signal-Modeling-and-CLIP-Alignment"><span class="nav-number">11.</span> <span class="nav-text">ECCV2024: DreamDiffusion: High-Quality EEG-to-Image Generation with Temporal Masked Signal Modeling and CLIP Alignment</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Author-%E7%99%BD%E4%BA%91%E9%B9%8F"><span class="nav-number">11.1.</span> <span class="nav-text">Author: 白云鹏</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract-10"><span class="nav-number">11.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ECCV2024-MirrorGaussian-Reflecting-3D-Gaussians-for-Reconstructing-Mirror-Reflections"><span class="nav-number">12.</span> <span class="nav-text">ECCV2024: MirrorGaussian: Reflecting 3D Gaussians for Reconstructing Mirror Reflections</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Author-%E5%88%98%E4%BD%B3%E6%9C%88"><span class="nav-number">12.1.</span> <span class="nav-text">Author: 刘佳月</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract-11"><span class="nav-number">12.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#IEEE-TCSVT-Meta-Learning-without-Data-via-Unconditional-Diffusion-Models"><span class="nav-number">13.</span> <span class="nav-text">IEEE TCSVT: Meta-Learning without Data via Unconditional Diffusion Models</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Author-%E9%9F%A6%E6%B0%B8%E8%B4%A4"><span class="nav-number">13.1.</span> <span class="nav-text">Author: 韦永贤</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract-12"><span class="nav-number">13.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ICML2024-CrossGET-Cross-Guided-Ensemble-of-Tokens-for-Accelerating-Vision-Language-Transformers"><span class="nav-number">14.</span> <span class="nav-text">ICML2024: CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Author-%E7%9F%B3%E5%A4%A7%E5%B7%9D"><span class="nav-number">14.1.</span> <span class="nav-text">Author: 石大川</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract-13"><span class="nav-number">14.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ICML2024-Sparse-Model-Inversion-Efficient-Inversion-of-Vision-Transformers-with-Less-Hallucination"><span class="nav-number">15.</span> <span class="nav-text">ICML2024: Sparse Model Inversion: Efficient Inversion of Vision Transformers with Less Hallucination</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Author-%E8%83%A1%E6%A2%93%E8%BD%A9"><span class="nav-number">15.1.</span> <span class="nav-text">Author: 胡梓轩</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract-14"><span class="nav-number">15.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ICML2024-Task-Groupings-Regularization-Data-Free-Meta-Learning-with-Heterogeneous-Pre-trained-Models"><span class="nav-number">16.</span> <span class="nav-text">ICML2024: Task Groupings Regularization: Data-Free Meta-Learning with Heterogeneous Pre-trained Models</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Author-%E9%9F%A6%E6%B0%B8%E8%B4%A4-1"><span class="nav-number">16.1.</span> <span class="nav-text">Author: 韦永贤</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract-15"><span class="nav-number">16.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ICML2024-DFD-Distillng-the-Feature-Disparity-Differently-for-Detectors"><span class="nav-number">17.</span> <span class="nav-text">ICML2024: DFD: Distillng the Feature Disparity Differently for Detectors</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Author-%E5%88%98%E5%BA%B7"><span class="nav-number">17.1.</span> <span class="nav-text">Author: 刘康</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract-16"><span class="nav-number">17.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#IJCNN2024-Integrating-Local-Global-Features-for-Estimating-Shortest-Path-Distance-in-Large-Scale-Graphs"><span class="nav-number">18.</span> <span class="nav-text">IJCNN2024: Integrating Local &amp; Global Features for Estimating Shortest-Path Distance in Large-Scale Graphs</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Author-%E7%8E%8B%E6%B5%A9%E5%AE%87"><span class="nav-number">18.1.</span> <span class="nav-text">Author: 王浩宇</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract-17"><span class="nav-number">18.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#IJCNN2024-MMR-Multi-Scale-Motion-Retargeting-Between-Skeleton-Agnostic-Characters"><span class="nav-number">19.</span> <span class="nav-text">IJCNN2024: MMR: Multi-Scale Motion Retargeting Between Skeleton-Agnostic Characters</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Author-%E7%8E%8B%E6%B5%A9%E5%AE%87-1"><span class="nav-number">19.1.</span> <span class="nav-text">Author: 王浩宇</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract-18"><span class="nav-number">19.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#IJCNN2024-Error-Bound-Based-Noise-Schedule-Design-in-Diffusion-Models"><span class="nav-number">20.</span> <span class="nav-text">IJCNN2024: Error Bound Based Noise Schedule Design in Diffusion Models</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Author-%E5%88%98%E5%8A%9B%E6%BA%90"><span class="nav-number">20.1.</span> <span class="nav-text">Author: 刘力源</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract-19"><span class="nav-number">20.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#IJCNN2024-Noise-Weighting-Phased-Prompt-Image-Editing"><span class="nav-number">21.</span> <span class="nav-text">IJCNN2024: Noise Weighting Phased Prompt Image Editing</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Author-%E5%BE%90%E5%9B%BD%E7%82%9C"><span class="nav-number">21.1.</span> <span class="nav-text">Author: 徐国炜</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract-20"><span class="nav-number">21.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CVPR2024-Distilling-Semantic-Priors-from-SAM-to-Efficient-Image-Restoration-Models"><span class="nav-number">22.</span> <span class="nav-text">CVPR2024: Distilling Semantic Priors from SAM to Efficient Image Restoration Models</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Author-%E5%BC%A0%E6%9D%83-1"><span class="nav-number">22.1.</span> <span class="nav-text">Author: 张权</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract-21"><span class="nav-number">22.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CVPR2024-CricaVPR-Cross-image-Correlation-aware-Representation-Learning-for-Visual-Place-Recognition"><span class="nav-number">23.</span> <span class="nav-text">CVPR2024: CricaVPR: Cross-image Correlation-aware Representation Learning for Visual Place Recognition</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Author-%E5%8D%A2%E9%94%8B-1"><span class="nav-number">23.1.</span> <span class="nav-text">Author: 卢锋</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract-22"><span class="nav-number">23.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CVPR2024-A-Free-Lunch-for-Faster-and-Better-Data-Free-Meta-Learning"><span class="nav-number">24.</span> <span class="nav-text">CVPR2024: A Free Lunch for Faster and Better Data-Free Meta-Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Author-%E9%9F%A6%E6%B0%B8%E8%B4%A4-2"><span class="nav-number">24.1.</span> <span class="nav-text">Author: 韦永贤</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract-23"><span class="nav-number">24.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#IEEE-T-MM-Negative-Sensitive-Framework-with-Semantic-Enhancement-for-Composed-Image-Retrieval"><span class="nav-number">25.</span> <span class="nav-text">IEEE T-MM: Negative-Sensitive Framework with Semantic Enhancement for Composed Image Retrieval</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Author-%E7%8E%8B%E4%BE%9D%E5%87%A1-2"><span class="nav-number">25.1.</span> <span class="nav-text">Author: 王依凡</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract-24"><span class="nav-number">25.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ICLR2024-Convolution-Meets-LoRA-Parameter-Efficient-Finetuning-for-Segment-Anything-Model"><span class="nav-number">26.</span> <span class="nav-text">ICLR2024: Convolution Meets LoRA: Parameter Efficient Finetuning for Segment Anything Model</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Author-%E9%92%9F%E5%AD%90%E6%B6%B5"><span class="nav-number">26.1.</span> <span class="nav-text">Author: 钟子涵</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract-25"><span class="nav-number">26.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ICLR2024-Towards-Seamless-Adaptation-of-Pre-trained-Models-for-Visual-Place-Recognition"><span class="nav-number">27.</span> <span class="nav-text">ICLR2024: Towards Seamless Adaptation of Pre-trained Models for Visual Place Recognition</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Author-%E5%8D%A2%E9%94%8B-2"><span class="nav-number">27.1.</span> <span class="nav-text">Author: 卢锋</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract-26"><span class="nav-number">27.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#AAAI2024-Efficient-Conditional-Diffusion-Model-with-Probability-Flow-Sampling-for-Image-Super-resolution"><span class="nav-number">28.</span> <span class="nav-text">AAAI2024: Efficient Conditional Diffusion Model with Probability Flow Sampling for Image Super-resolution</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Author-%E8%A2%81%E5%AE%87%E9%9F%AC"><span class="nav-number">28.1.</span> <span class="nav-text">Author: 袁宇韬</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract-27"><span class="nav-number">28.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#AAAI2024-Blind-Face-Restoration-under-Extreme-Conditions-Leveraging-3D-2D-Prior-Fusion-for-Superior-Structural-and-Texture-Recovery"><span class="nav-number">29.</span> <span class="nav-text">AAAI2024: Blind Face Restoration under Extreme Conditions: Leveraging 3D-2D Prior Fusion for Superior Structural and Texture Recovery</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Author-%E8%A2%81%E6%A2%93%E6%B4%8B-1"><span class="nav-number">29.1.</span> <span class="nav-text">Author: 袁梓洋</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract-28"><span class="nav-number">29.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#AAAI2024-Mean-Teacher-DETR-with-Masked-Feature-Alignment-A-Robust-Domain-Adaptive-Detection-Transformer-Framework"><span class="nav-number">30.</span> <span class="nav-text">AAAI2024: Mean Teacher DETR with Masked Feature Alignment: A Robust Domain Adaptive Detection Transformer Framework</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Author-%E7%BF%81%E7%8E%AE%E7%86%99"><span class="nav-number">30.1.</span> <span class="nav-text">Author: 翁玮熙</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract-29"><span class="nav-number">30.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#AAAI2024-Deep-Homography-Estimation-for-Visual-Place-Recognition"><span class="nav-number">31.</span> <span class="nav-text">AAAI2024: Deep Homography Estimation for Visual Place Recognition</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Author-%E5%8D%A2%E9%94%8B-3"><span class="nav-number">31.1.</span> <span class="nav-text">Author: 卢锋</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract-30"><span class="nav-number">31.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#IEEE-T-MM-Towards-Effective-Collaborative-Learning-in-Long-Tailed-Recognition"><span class="nav-number">32.</span> <span class="nav-text">IEEE T-MM: Towards Effective Collaborative Learning in Long-Tailed Recognition</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Author-%E8%AE%B8%E6%AD%A3%E5%8D%93"><span class="nav-number">32.1.</span> <span class="nav-text">Author: 许正卓</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract-31"><span class="nav-number">32.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ACL2023-LET-Leveraging-Error-Type-Information-for-Grammatical-Error-Correction"><span class="nav-number">33.</span> <span class="nav-text">ACL2023: LET: Leveraging Error Type Information for Grammatical Error Correction</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Author-%E6%9D%8E%E6%B3%93%E5%98%89"><span class="nav-number">33.1.</span> <span class="nav-text">Author: 李泓嘉</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract-32"><span class="nav-number">33.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ACL2023-Tailoring-Instructions-to-Student%E2%80%99s-Learning-Levels-Boosts-Knowledge-Distillation"><span class="nav-number">34.</span> <span class="nav-text">ACL2023: Tailoring Instructions to Student’s Learning Levels Boosts Knowledge Distillation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Author-%E4%BB%BB%E6%98%B1%E9%91%AB"><span class="nav-number">34.1.</span> <span class="nav-text">Author: 任昱鑫</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract-33"><span class="nav-number">34.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#IEEE-T-MM-HHF-Hashing-guided-Hinge-Function-for-Deep-Hashing-Retrieval"><span class="nav-number">35.</span> <span class="nav-text">IEEE T-MM: HHF: Hashing-guided Hinge Function for Deep Hashing Retrieval</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Author-%E5%BE%90%E5%91%88%E5%AF%85"><span class="nav-number">35.1.</span> <span class="nav-text">Author: 徐呈寅</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract-34"><span class="nav-number">35.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ACM-MM2023-Enhanced-Image-Deblurring-An-Efficient-Frequency-Exploitation-and-Preservation-Network"><span class="nav-number">36.</span> <span class="nav-text">ACM MM2023: Enhanced Image Deblurring: An Efficient Frequency Exploitation and Preservation Network</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Author-%E8%91%A3%E5%A7%9D%E5%A9%B7"><span class="nav-number">36.1.</span> <span class="nav-text">Author: 董姝婷</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract-35"><span class="nav-number">36.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#IJCAI2023-DFVSR-Directional-Frequency-Video-Super-Resolution-via-Asymmetric-and-Enhancement-Alignment-Network"><span class="nav-number">37.</span> <span class="nav-text">IJCAI2023: DFVSR: Directional Frequency Video Super-Resolution via Asymmetric and Enhancement Alignment Network</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Author-%E8%91%A3%E5%A7%9D%E5%A9%B7-1"><span class="nav-number">37.1.</span> <span class="nav-text">Author: 董姝婷</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract-36"><span class="nav-number">37.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ACM-MM2023-Adaptive-Contrastive-Learning-for-Learning-Robust-Representations-under-Label-Noise"><span class="nav-number">38.</span> <span class="nav-text">ACM MM2023: Adaptive Contrastive Learning for Learning Robust Representations under Label Noise</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Author-%E7%8E%8B%E5%AD%90%E6%B5%A9"><span class="nav-number">38.1.</span> <span class="nav-text">Author: 王子浩</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract-37"><span class="nav-number">38.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ICCV2023-HiFace-High-Fidelity-3D-Face-Reconstruction-by-Learning-Static-and-Dynamic-Details"><span class="nav-number">39.</span> <span class="nav-text">ICCV2023: HiFace: High-Fidelity 3D Face Reconstruction by Learning Static and Dynamic Details</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Author-%E6%9F%B4%E5%A2%9E%E8%B1%AA"><span class="nav-number">39.1.</span> <span class="nav-text">Author: 柴增豪</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract-38"><span class="nav-number">39.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ICCV2023-Accurate-3D-Face-Reconstruction-with-Facial-Component-Tokens"><span class="nav-number">40.</span> <span class="nav-text">ICCV2023: Accurate 3D Face Reconstruction with Facial Component Tokens</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Author-%E7%AB%A0%E5%A4%A9%E7%8F%82"><span class="nav-number">40.1.</span> <span class="nav-text">Author: 章天珂</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract-39"><span class="nav-number">40.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ICCVW2023-Effective-Whole-body-Pose-Estimation-with-Two-stages-Distillation"><span class="nav-number">41.</span> <span class="nav-text">ICCVW2023: Effective Whole-body Pose Estimation with Two-stages Distillation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Author-%E6%9D%A8%E9%9C%87%E4%B8%9C"><span class="nav-number">41.1.</span> <span class="nav-text">Author: 杨震东</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract-40"><span class="nav-number">41.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ICCV2023-From-Knowledge-Distillation-to-Self-Knowledge-Distillation-A-Unified-Approach-with-Normalized-Loss-and-Customized-Soft-Labels"><span class="nav-number">42.</span> <span class="nav-text">ICCV2023: From Knowledge Distillation to Self-Knowledge Distillation: A Unified Approach with Normalized Loss and Customized Soft Labels</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Author-%E6%9D%A8%E9%9C%87%E4%B8%9C-1"><span class="nav-number">42.1.</span> <span class="nav-text">Author: 杨震东</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract-41"><span class="nav-number">42.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ICCV2023-Make-Encoder-Great-Again-in-3D-GAN-Inversion-through-Geometry-and-Occlusion-Aware-Encoding"><span class="nav-number">43.</span> <span class="nav-text">ICCV2023: Make Encoder Great Again in 3D GAN Inversion through Geometry and Occlusion-Aware Encoding</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Author-%E8%A2%81%E6%A2%93%E6%B4%8B-2"><span class="nav-number">43.1.</span> <span class="nav-text">Author: 袁梓洋</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract-42"><span class="nav-number">43.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ICML2023-UPop-Unified-and-Progressive-Pruning-for-Compressing-Vision-Language-Transformers"><span class="nav-number">44.</span> <span class="nav-text">ICML2023: UPop: Unified and Progressive Pruning for Compressing Vision-Language Transformers</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Author-%E7%9F%B3%E5%A4%A7%E5%B7%9D-1"><span class="nav-number">44.1.</span> <span class="nav-text">Author: 石大川</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract-43"><span class="nav-number">44.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ICML2023-Learning-to-Learn-from-APIs-Black-Box-Data-Free-Meta-Learning"><span class="nav-number">45.</span> <span class="nav-text">ICML2023: Learning to Learn from APIs: Black-Box Data-Free Meta-Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Author-%E8%83%A1%E6%A2%93%E8%BD%A9-1"><span class="nav-number">45.1.</span> <span class="nav-text">Author: 胡梓轩</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract-44"><span class="nav-number">45.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ICRA2023-AANet-Aggregation-and-Alignment-Network-with-Semi-hard-Positive-Sample-Mining-for-Hierarchical-Place-Recognition"><span class="nav-number">46.</span> <span class="nav-text">ICRA2023: AANet: Aggregation and Alignment Network with Semi-hard Positive Sample Mining for Hierarchical Place Recognition</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Author-%E5%8D%A2%E9%94%8B-4"><span class="nav-number">46.1.</span> <span class="nav-text">Author: 卢锋</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract-45"><span class="nav-number">46.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CVPR2023-High-fidelity-Facial-Avatar-Reconstruction-from-Monocular-Video-with-Generative-Priors"><span class="nav-number">47.</span> <span class="nav-text">CVPR2023: High-fidelity Facial Avatar Reconstruction from Monocular Video with Generative Priors</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Author-%E7%99%BD%E4%BA%91%E9%B9%8F-1"><span class="nav-number">47.1.</span> <span class="nav-text">Author: 白云鹏</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract-46"><span class="nav-number">47.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CVPR2023-Learning-Imbalanced-Data-with-Vision-Transformers"><span class="nav-number">48.</span> <span class="nav-text">CVPR2023: Learning Imbalanced Data with Vision Transformers</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Author-%E8%AE%B8%E6%AD%A3%E5%8D%93-1"><span class="nav-number">48.1.</span> <span class="nav-text">Author: 许正卓</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract-47"><span class="nav-number">48.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#IEEE-TCSVT-Task-adaptive-Feature-Disentanglement-and-Hallucination-for-Few-shot-Classification"><span class="nav-number">49.</span> <span class="nav-text">IEEE TCSVT: Task-adaptive Feature Disentanglement and Hallucination for Few-shot Classification</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Author-%E8%83%A1%E6%A2%93%E8%BD%A9-2"><span class="nav-number">49.1.</span> <span class="nav-text">Author: 胡梓轩</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract-48"><span class="nav-number">49.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CVPR2023-Architecture-Dataset-and-Model-Scale-Agnostic-Data-free-Meta-Learning"><span class="nav-number">50.</span> <span class="nav-text">CVPR2023: Architecture, Dataset and Model-Scale Agnostic Data-free Meta-Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Author-%E8%83%A1%E6%A2%93%E8%BD%A9-3"><span class="nav-number">50.1.</span> <span class="nav-text">Author: 胡梓轩</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract-49"><span class="nav-number">50.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CVPR2023-Uni-Perceiver-v2-A-Generalist-Model-for-Large-Scale-Vision-and-Vision-Language-Tasks"><span class="nav-number">51.</span> <span class="nav-text">CVPR2023: Uni-Perceiver v2: A Generalist Model for Large-Scale Vision and Vision-Language Tasks</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Author-%E6%B1%9F%E6%99%93%E6%B9%96"><span class="nav-number">51.1.</span> <span class="nav-text">Author: 江晓湖</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract-50"><span class="nav-number">51.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ICASSP2023-FREQUENCY-RECIPROCAL-ACTION-AND-FUSION-FOR-SINGLE-IMAGE-SUPER-RESOLUTION"><span class="nav-number">52.</span> <span class="nav-text">ICASSP2023: FREQUENCY RECIPROCAL ACTION AND FUSION FOR SINGLE IMAGE SUPER-RESOLUTION</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Author-%E8%91%A3%E5%A7%9D%E5%A9%B7-2"><span class="nav-number">52.1.</span> <span class="nav-text">Author: 董姝婷</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract-51"><span class="nav-number">52.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ICASSP2023-Rethink-Long-Tailed-Recognition-With-Vision-Transforms"><span class="nav-number">53.</span> <span class="nav-text">ICASSP2023: Rethink Long-Tailed Recognition With Vision Transforms</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Author-%E8%AE%B8%E6%AD%A3%E5%8D%93-2"><span class="nav-number">53.1.</span> <span class="nav-text">Author: 许正卓</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract-52"><span class="nav-number">53.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#IEEE-TNNLS-PatchNet-Maximize-the-Exploration-of-Congeneric-Semantics-for-Weakly-Supervised-Semantic-Segmentation"><span class="nav-number">54.</span> <span class="nav-text">IEEE TNNLS: PatchNet: Maximize the Exploration of Congeneric Semantics for Weakly Supervised Semantic Segmentation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Author-%E5%BC%A0%E5%8F%AF"><span class="nav-number">54.1.</span> <span class="nav-text">Author: 张可</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract-53"><span class="nav-number">54.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#AAAI2023-Truncate-Split-Contrast-A-Framework-for-Learning-from-Mislabeled-Videos"><span class="nav-number">55.</span> <span class="nav-text">AAAI2023: Truncate-Split-Contrast: A Framework for Learning from Mislabeled Videos</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Author-%E7%8E%8B%E5%AD%90%E5%95%B8"><span class="nav-number">55.1.</span> <span class="nav-text">Author: 王子啸</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract-54"><span class="nav-number">55.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#AAAI2023-Darwinian-Model-Upgrades-Model-Evolving-with-Selective-Compatibility"><span class="nav-number">56.</span> <span class="nav-text">AAAI2023: Darwinian Model Upgrades: Model Evolving with Selective Compatibility</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Author-%E5%BC%A0%E6%96%8C%E6%9D%B0"><span class="nav-number">56.1.</span> <span class="nav-text">Author: 张斌杰</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract-55"><span class="nav-number">56.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="袁春"
      src="/images/Chunyuan.jpg">
  <p class="site-author-name" itemprop="name">袁春</p>
  <div class="site-description" itemprop="description">教授，博士生导师</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">1</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://scholar.google.com/citations?hl=en&user=fYdxi2sAAAAJ" title="Google Scholar → https:&#x2F;&#x2F;scholar.google.com&#x2F;citations?hl&#x3D;en&amp;user&#x3D;fYdxi2sAAAAJ" rel="noopener me" target="_blank">Google Scholar</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://thu-cvml.github.io/2024/12/11/%E7%A7%91%E7%A0%94%E5%BF%AB%E8%AE%AF/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/Chunyuan.jpg">
      <meta itemprop="name" content="袁春">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="清华大学<br>深圳国际研究生院<br>袁春教授课题组">
      <meta itemprop="description" content="教授，博士生导师">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="2023-2025年科研成果展示 | 清华大学<br>深圳国际研究生院<br>袁春教授课题组">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          2023-2025年科研成果展示
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-12-11 16:34:09" itemprop="dateCreated datePublished" datetime="2024-12-11T16:34:09+08:00">2024-12-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-12-17 23:14:16" itemprop="dateModified" datetime="2024-12-17T23:14:16+08:00">2024-12-17</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h2 id="ICCV2023-When-Noisy-Labels-Meet-Long-Tail-Dilemmas-A-Representation-Calibration-Method"><a href="#ICCV2023-When-Noisy-Labels-Meet-Long-Tail-Dilemmas-A-Representation-Calibration-Method" class="headerlink" title="ICCV2023: When Noisy Labels Meet Long Tail Dilemmas: A Representation Calibration Method"></a><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_When_Noisy_Labels_Meet_Long_Tail_Dilemmas_A_Representation_Calibration_ICCV_2023_paper.html?trk=public_post_comment-text">ICCV2023: When Noisy Labels Meet Long Tail Dilemmas: A Representation Calibration Method</a></h2><h3 id="Author-张曼怡"><a href="#Author-张曼怡" class="headerlink" title="Author: 张曼怡"></a>Author: 张曼怡</h3><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>Real-world large-scale datasets are both noisily labeled and class-imbalanced. The issues seriously hurt the generalization of trained models. It is hence significant to address the simultaneous incorrect labeling and class-imbalance, i.e., the problem of learning with noisy labels on long-tailed data. Previous works develop several methods for the problem. However, they always rely on strong assumptions that are invalid or hard to be checked in practice. In this paper, to handle the problem and address the limitations of prior works, we propose a representation calibration method RCAL. Specifically, RCAL works with the representations extracted by unsupervised contrastive learning. We assume that without incorrect labeling and class imbalance, the representations of instances in each class conform to a multivariate Gaussian distribution, which is much milder and easier to be checked. Based on the assumption, we recover underlying representation distributions from polluted ones resulting from mislabeled and class-imbalanced data. Additional data points are then sampled from the recovered distributions to help generalization. Moreover, during classifier training, representation learning takes advantage of representation robustness brought by contrastive learning, which further improves the classifier performance. We derive theoretical results to discuss the effectiveness of our representation calibration. Experiments on multiple benchmarks justify our claims and confirm the superiority of the proposed method.</p>
<p><img src="https://raw.githubusercontent.com/THU-CVML/thu-cvml.github.io/refs/heads/main/images/RCAL.png"></p>
<h2 id="AAAI2025-Aligning-Composed-Query-with-Image-via-Discriminative-Perception-from-Negative-Correspondences"><a href="#AAAI2025-Aligning-Composed-Query-with-Image-via-Discriminative-Perception-from-Negative-Correspondences" class="headerlink" title="AAAI2025: Aligning Composed Query with Image via Discriminative Perception from Negative Correspondences"></a><a href="">AAAI2025: Aligning Composed Query with Image via Discriminative Perception from Negative Correspondences</a></h2><h3 id="Author-王依凡"><a href="#Author-王依凡" class="headerlink" title="Author: 王依凡"></a>Author: 王依凡</h3><h3 id="Abstract-1"><a href="#Abstract-1" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="AAAI2025-Rethinking-Pseudo-Label-Guided-Learning-for-Weakly-Supervised-Temporal-Action-Localization-from-the-Perspective-of-Noise-Correction"><a href="#AAAI2025-Rethinking-Pseudo-Label-Guided-Learning-for-Weakly-Supervised-Temporal-Action-Localization-from-the-Perspective-of-Noise-Correction" class="headerlink" title="AAAI2025: Rethinking Pseudo-Label Guided Learning for Weakly-Supervised Temporal Action Localization from the Perspective of Noise Correction"></a><a href="">AAAI2025: Rethinking Pseudo-Label Guided Learning for Weakly-Supervised Temporal Action Localization from the Perspective of Noise Correction</a></h2><h3 id="Author-张权"><a href="#Author-张权" class="headerlink" title="Author: 张权"></a>Author: 张权</h3><h3 id="Abstract-2"><a href="#Abstract-2" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="NeurIPS2025-SuperVLAD-Compact-and-Robust-Image-Descriptors-for-Visual-Place-Recognition"><a href="#NeurIPS2025-SuperVLAD-Compact-and-Robust-Image-Descriptors-for-Visual-Place-Recognition" class="headerlink" title="NeurIPS2025: SuperVLAD: Compact and Robust Image Descriptors for Visual Place Recognition"></a><a href="">NeurIPS2025: SuperVLAD: Compact and Robust Image Descriptors for Visual Place Recognition</a></h2><h3 id="Author-卢锋"><a href="#Author-卢锋" class="headerlink" title="Author: 卢锋"></a>Author: 卢锋</h3><h3 id="Abstract-3"><a href="#Abstract-3" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="ICPR2024-Benchmarking-AI-in-Mental-Health-A-Critical-Examination-of-LLMs-Across-Key-Performance-and-Ethical-Metrics"><a href="#ICPR2024-Benchmarking-AI-in-Mental-Health-A-Critical-Examination-of-LLMs-Across-Key-Performance-and-Ethical-Metrics" class="headerlink" title="ICPR2024: Benchmarking AI in Mental Health: A Critical Examination of LLMs Across Key Performance and Ethical Metrics"></a><a href="">ICPR2024: Benchmarking AI in Mental Health: A Critical Examination of LLMs Across Key Performance and Ethical Metrics</a></h2><h3 id="Author-袁睿"><a href="#Author-袁睿" class="headerlink" title="Author: 袁睿"></a>Author: 袁睿</h3><h3 id="Abstract-4"><a href="#Abstract-4" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="ICPR2024-AMC-OA-Adaptive-Multi-Scale-Convolutional-Networks-with-Optimized-Attention-for-Temporal-Action-Localization"><a href="#ICPR2024-AMC-OA-Adaptive-Multi-Scale-Convolutional-Networks-with-Optimized-Attention-for-Temporal-Action-Localization" class="headerlink" title="ICPR2024: AMC-OA: Adaptive Multi-Scale Convolutional Networks with Optimized Attention for Temporal Action Localization"></a><a href="">ICPR2024: AMC-OA: Adaptive Multi-Scale Convolutional Networks with Optimized Attention for Temporal Action Localization</a></h2><h3 id="Author-袁睿-1"><a href="#Author-袁睿-1" class="headerlink" title="Author: 袁睿"></a>Author: 袁睿</h3><h3 id="Abstract-5"><a href="#Abstract-5" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="ACMMM2024-Semantic-Distillation-from-Neighborhood-for-Composed-Image-Retrieval"><a href="#ACMMM2024-Semantic-Distillation-from-Neighborhood-for-Composed-Image-Retrieval" class="headerlink" title="ACMMM2024: Semantic Distillation from Neighborhood for Composed Image Retrieval"></a><a href="">ACMMM2024: Semantic Distillation from Neighborhood for Composed Image Retrieval</a></h2><h3 id="Author-王依凡-1"><a href="#Author-王依凡-1" class="headerlink" title="Author: 王依凡"></a>Author: 王依凡</h3><h3 id="Abstract-6"><a href="#Abstract-6" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="ACM-MM2024-CustomNet-Object-Customization-with-Variable-Viewpoints-in-Text-to-Image-Diffusion-Models"><a href="#ACM-MM2024-CustomNet-Object-Customization-with-Variable-Viewpoints-in-Text-to-Image-Diffusion-Models" class="headerlink" title="ACM MM2024: CustomNet: Object Customization with Variable-Viewpoints in Text-to-Image Diffusion Models"></a><a href="">ACM MM2024: CustomNet: Object Customization with Variable-Viewpoints in Text-to-Image Diffusion Models</a></h2><h3 id="Author-袁梓洋"><a href="#Author-袁梓洋" class="headerlink" title="Author: 袁梓洋"></a>Author: 袁梓洋</h3><h3 id="Abstract-7"><a href="#Abstract-7" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="ECCV2024-GVGEN-Text-to-3D-Generation-with-Volumetric-Representation"><a href="#ECCV2024-GVGEN-Text-to-3D-Generation-with-Volumetric-Representation" class="headerlink" title="ECCV2024: GVGEN: Text-to-3D Generation with Volumetric Representation"></a><a href="">ECCV2024: GVGEN: Text-to-3D Generation with Volumetric Representation</a></h2><h3 id="Author-何相龙"><a href="#Author-何相龙" class="headerlink" title="Author: 何相龙"></a>Author: 何相龙</h3><h3 id="Abstract-8"><a href="#Abstract-8" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="ECCV2024-A-Task-is-Worth-One-Word-Learning-with-Task-Prompts-for-High-Quality-Versatile-Image-Inpainting"><a href="#ECCV2024-A-Task-is-Worth-One-Word-Learning-with-Task-Prompts-for-High-Quality-Versatile-Image-Inpainting" class="headerlink" title="ECCV2024: A Task is Worth One Word: Learning with Task Prompts for High-Quality Versatile Image Inpainting"></a><a href="">ECCV2024: A Task is Worth One Word: Learning with Task Prompts for High-Quality Versatile Image Inpainting</a></h2><h3 id="Author-庄俊豪"><a href="#Author-庄俊豪" class="headerlink" title="Author: 庄俊豪"></a>Author: 庄俊豪</h3><h3 id="Abstract-9"><a href="#Abstract-9" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="ECCV2024-DreamDiffusion-High-Quality-EEG-to-Image-Generation-with-Temporal-Masked-Signal-Modeling-and-CLIP-Alignment"><a href="#ECCV2024-DreamDiffusion-High-Quality-EEG-to-Image-Generation-with-Temporal-Masked-Signal-Modeling-and-CLIP-Alignment" class="headerlink" title="ECCV2024: DreamDiffusion: High-Quality EEG-to-Image Generation with Temporal Masked Signal Modeling and CLIP Alignment"></a><a href="">ECCV2024: DreamDiffusion: High-Quality EEG-to-Image Generation with Temporal Masked Signal Modeling and CLIP Alignment</a></h2><h3 id="Author-白云鹏"><a href="#Author-白云鹏" class="headerlink" title="Author: 白云鹏"></a>Author: 白云鹏</h3><h3 id="Abstract-10"><a href="#Abstract-10" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="ECCV2024-MirrorGaussian-Reflecting-3D-Gaussians-for-Reconstructing-Mirror-Reflections"><a href="#ECCV2024-MirrorGaussian-Reflecting-3D-Gaussians-for-Reconstructing-Mirror-Reflections" class="headerlink" title="ECCV2024: MirrorGaussian: Reflecting 3D Gaussians for Reconstructing Mirror Reflections"></a><a href="">ECCV2024: MirrorGaussian: Reflecting 3D Gaussians for Reconstructing Mirror Reflections</a></h2><h3 id="Author-刘佳月"><a href="#Author-刘佳月" class="headerlink" title="Author: 刘佳月"></a>Author: 刘佳月</h3><h3 id="Abstract-11"><a href="#Abstract-11" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="IEEE-TCSVT-Meta-Learning-without-Data-via-Unconditional-Diffusion-Models"><a href="#IEEE-TCSVT-Meta-Learning-without-Data-via-Unconditional-Diffusion-Models" class="headerlink" title="IEEE TCSVT: Meta-Learning without Data via Unconditional Diffusion Models"></a><a href="">IEEE TCSVT: Meta-Learning without Data via Unconditional Diffusion Models</a></h2><h3 id="Author-韦永贤"><a href="#Author-韦永贤" class="headerlink" title="Author: 韦永贤"></a>Author: 韦永贤</h3><h3 id="Abstract-12"><a href="#Abstract-12" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="ICML2024-CrossGET-Cross-Guided-Ensemble-of-Tokens-for-Accelerating-Vision-Language-Transformers"><a href="#ICML2024-CrossGET-Cross-Guided-Ensemble-of-Tokens-for-Accelerating-Vision-Language-Transformers" class="headerlink" title="ICML2024: CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers"></a><a href="">ICML2024: CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers</a></h2><h3 id="Author-石大川"><a href="#Author-石大川" class="headerlink" title="Author: 石大川"></a>Author: 石大川</h3><h3 id="Abstract-13"><a href="#Abstract-13" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="ICML2024-Sparse-Model-Inversion-Efficient-Inversion-of-Vision-Transformers-with-Less-Hallucination"><a href="#ICML2024-Sparse-Model-Inversion-Efficient-Inversion-of-Vision-Transformers-with-Less-Hallucination" class="headerlink" title="ICML2024: Sparse Model Inversion: Efficient Inversion of Vision Transformers with Less Hallucination"></a><a href="">ICML2024: Sparse Model Inversion: Efficient Inversion of Vision Transformers with Less Hallucination</a></h2><h3 id="Author-胡梓轩"><a href="#Author-胡梓轩" class="headerlink" title="Author: 胡梓轩"></a>Author: 胡梓轩</h3><h3 id="Abstract-14"><a href="#Abstract-14" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="ICML2024-Task-Groupings-Regularization-Data-Free-Meta-Learning-with-Heterogeneous-Pre-trained-Models"><a href="#ICML2024-Task-Groupings-Regularization-Data-Free-Meta-Learning-with-Heterogeneous-Pre-trained-Models" class="headerlink" title="ICML2024: Task Groupings Regularization: Data-Free Meta-Learning with Heterogeneous Pre-trained Models"></a><a href="">ICML2024: Task Groupings Regularization: Data-Free Meta-Learning with Heterogeneous Pre-trained Models</a></h2><h3 id="Author-韦永贤-1"><a href="#Author-韦永贤-1" class="headerlink" title="Author: 韦永贤"></a>Author: 韦永贤</h3><h3 id="Abstract-15"><a href="#Abstract-15" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="ICML2024-DFD-Distillng-the-Feature-Disparity-Differently-for-Detectors"><a href="#ICML2024-DFD-Distillng-the-Feature-Disparity-Differently-for-Detectors" class="headerlink" title="ICML2024: DFD: Distillng the Feature Disparity Differently for Detectors"></a><a href="">ICML2024: DFD: Distillng the Feature Disparity Differently for Detectors</a></h2><h3 id="Author-刘康"><a href="#Author-刘康" class="headerlink" title="Author: 刘康"></a>Author: 刘康</h3><h3 id="Abstract-16"><a href="#Abstract-16" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="IJCNN2024-Integrating-Local-Global-Features-for-Estimating-Shortest-Path-Distance-in-Large-Scale-Graphs"><a href="#IJCNN2024-Integrating-Local-Global-Features-for-Estimating-Shortest-Path-Distance-in-Large-Scale-Graphs" class="headerlink" title="IJCNN2024: Integrating Local &amp; Global Features for Estimating Shortest-Path Distance in Large-Scale Graphs"></a><a href="">IJCNN2024: Integrating Local &amp; Global Features for Estimating Shortest-Path Distance in Large-Scale Graphs</a></h2><h3 id="Author-王浩宇"><a href="#Author-王浩宇" class="headerlink" title="Author: 王浩宇"></a>Author: 王浩宇</h3><h3 id="Abstract-17"><a href="#Abstract-17" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="IJCNN2024-MMR-Multi-Scale-Motion-Retargeting-Between-Skeleton-Agnostic-Characters"><a href="#IJCNN2024-MMR-Multi-Scale-Motion-Retargeting-Between-Skeleton-Agnostic-Characters" class="headerlink" title="IJCNN2024: MMR: Multi-Scale Motion Retargeting Between Skeleton-Agnostic Characters"></a><a href="">IJCNN2024: MMR: Multi-Scale Motion Retargeting Between Skeleton-Agnostic Characters</a></h2><h3 id="Author-王浩宇-1"><a href="#Author-王浩宇-1" class="headerlink" title="Author: 王浩宇"></a>Author: 王浩宇</h3><h3 id="Abstract-18"><a href="#Abstract-18" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="IJCNN2024-Error-Bound-Based-Noise-Schedule-Design-in-Diffusion-Models"><a href="#IJCNN2024-Error-Bound-Based-Noise-Schedule-Design-in-Diffusion-Models" class="headerlink" title="IJCNN2024: Error Bound Based Noise Schedule Design in Diffusion Models"></a><a href="">IJCNN2024: Error Bound Based Noise Schedule Design in Diffusion Models</a></h2><h3 id="Author-刘力源"><a href="#Author-刘力源" class="headerlink" title="Author: 刘力源"></a>Author: 刘力源</h3><h3 id="Abstract-19"><a href="#Abstract-19" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="IJCNN2024-Noise-Weighting-Phased-Prompt-Image-Editing"><a href="#IJCNN2024-Noise-Weighting-Phased-Prompt-Image-Editing" class="headerlink" title="IJCNN2024: Noise Weighting Phased Prompt Image Editing"></a><a href="">IJCNN2024: Noise Weighting Phased Prompt Image Editing</a></h2><h3 id="Author-徐国炜"><a href="#Author-徐国炜" class="headerlink" title="Author: 徐国炜"></a>Author: 徐国炜</h3><h3 id="Abstract-20"><a href="#Abstract-20" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="CVPR2024-Distilling-Semantic-Priors-from-SAM-to-Efficient-Image-Restoration-Models"><a href="#CVPR2024-Distilling-Semantic-Priors-from-SAM-to-Efficient-Image-Restoration-Models" class="headerlink" title="CVPR2024: Distilling Semantic Priors from SAM to Efficient Image Restoration Models"></a><a href="">CVPR2024: Distilling Semantic Priors from SAM to Efficient Image Restoration Models</a></h2><h3 id="Author-张权-1"><a href="#Author-张权-1" class="headerlink" title="Author: 张权"></a>Author: 张权</h3><h3 id="Abstract-21"><a href="#Abstract-21" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="CVPR2024-CricaVPR-Cross-image-Correlation-aware-Representation-Learning-for-Visual-Place-Recognition"><a href="#CVPR2024-CricaVPR-Cross-image-Correlation-aware-Representation-Learning-for-Visual-Place-Recognition" class="headerlink" title="CVPR2024: CricaVPR: Cross-image Correlation-aware Representation Learning for Visual Place Recognition"></a><a href="">CVPR2024: CricaVPR: Cross-image Correlation-aware Representation Learning for Visual Place Recognition</a></h2><h3 id="Author-卢锋-1"><a href="#Author-卢锋-1" class="headerlink" title="Author: 卢锋"></a>Author: 卢锋</h3><h3 id="Abstract-22"><a href="#Abstract-22" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="CVPR2024-A-Free-Lunch-for-Faster-and-Better-Data-Free-Meta-Learning"><a href="#CVPR2024-A-Free-Lunch-for-Faster-and-Better-Data-Free-Meta-Learning" class="headerlink" title="CVPR2024: A Free Lunch for Faster and Better Data-Free Meta-Learning"></a><a href="">CVPR2024: A Free Lunch for Faster and Better Data-Free Meta-Learning</a></h2><h3 id="Author-韦永贤-2"><a href="#Author-韦永贤-2" class="headerlink" title="Author: 韦永贤"></a>Author: 韦永贤</h3><h3 id="Abstract-23"><a href="#Abstract-23" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="IEEE-T-MM-Negative-Sensitive-Framework-with-Semantic-Enhancement-for-Composed-Image-Retrieval"><a href="#IEEE-T-MM-Negative-Sensitive-Framework-with-Semantic-Enhancement-for-Composed-Image-Retrieval" class="headerlink" title="IEEE T-MM: Negative-Sensitive Framework with Semantic Enhancement for Composed Image Retrieval"></a><a href="">IEEE T-MM: Negative-Sensitive Framework with Semantic Enhancement for Composed Image Retrieval</a></h2><h3 id="Author-王依凡-2"><a href="#Author-王依凡-2" class="headerlink" title="Author: 王依凡"></a>Author: 王依凡</h3><h3 id="Abstract-24"><a href="#Abstract-24" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="ICLR2024-Convolution-Meets-LoRA-Parameter-Efficient-Finetuning-for-Segment-Anything-Model"><a href="#ICLR2024-Convolution-Meets-LoRA-Parameter-Efficient-Finetuning-for-Segment-Anything-Model" class="headerlink" title="ICLR2024: Convolution Meets LoRA: Parameter Efficient Finetuning for Segment Anything Model"></a><a href="">ICLR2024: Convolution Meets LoRA: Parameter Efficient Finetuning for Segment Anything Model</a></h2><h3 id="Author-钟子涵"><a href="#Author-钟子涵" class="headerlink" title="Author: 钟子涵"></a>Author: 钟子涵</h3><h3 id="Abstract-25"><a href="#Abstract-25" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="ICLR2024-Towards-Seamless-Adaptation-of-Pre-trained-Models-for-Visual-Place-Recognition"><a href="#ICLR2024-Towards-Seamless-Adaptation-of-Pre-trained-Models-for-Visual-Place-Recognition" class="headerlink" title="ICLR2024: Towards Seamless Adaptation of Pre-trained Models for Visual Place Recognition"></a><a href="">ICLR2024: Towards Seamless Adaptation of Pre-trained Models for Visual Place Recognition</a></h2><h3 id="Author-卢锋-2"><a href="#Author-卢锋-2" class="headerlink" title="Author: 卢锋"></a>Author: 卢锋</h3><h3 id="Abstract-26"><a href="#Abstract-26" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="AAAI2024-Efficient-Conditional-Diffusion-Model-with-Probability-Flow-Sampling-for-Image-Super-resolution"><a href="#AAAI2024-Efficient-Conditional-Diffusion-Model-with-Probability-Flow-Sampling-for-Image-Super-resolution" class="headerlink" title="AAAI2024: Efficient Conditional Diffusion Model with Probability Flow Sampling for Image Super-resolution"></a><a href="">AAAI2024: Efficient Conditional Diffusion Model with Probability Flow Sampling for Image Super-resolution</a></h2><h3 id="Author-袁宇韬"><a href="#Author-袁宇韬" class="headerlink" title="Author: 袁宇韬"></a>Author: 袁宇韬</h3><h3 id="Abstract-27"><a href="#Abstract-27" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="AAAI2024-Blind-Face-Restoration-under-Extreme-Conditions-Leveraging-3D-2D-Prior-Fusion-for-Superior-Structural-and-Texture-Recovery"><a href="#AAAI2024-Blind-Face-Restoration-under-Extreme-Conditions-Leveraging-3D-2D-Prior-Fusion-for-Superior-Structural-and-Texture-Recovery" class="headerlink" title="AAAI2024: Blind Face Restoration under Extreme Conditions: Leveraging 3D-2D Prior Fusion for Superior Structural and Texture Recovery"></a><a href="">AAAI2024: Blind Face Restoration under Extreme Conditions: Leveraging 3D-2D Prior Fusion for Superior Structural and Texture Recovery</a></h2><h3 id="Author-袁梓洋-1"><a href="#Author-袁梓洋-1" class="headerlink" title="Author: 袁梓洋"></a>Author: 袁梓洋</h3><h3 id="Abstract-28"><a href="#Abstract-28" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="AAAI2024-Mean-Teacher-DETR-with-Masked-Feature-Alignment-A-Robust-Domain-Adaptive-Detection-Transformer-Framework"><a href="#AAAI2024-Mean-Teacher-DETR-with-Masked-Feature-Alignment-A-Robust-Domain-Adaptive-Detection-Transformer-Framework" class="headerlink" title="AAAI2024: Mean Teacher DETR with Masked Feature Alignment: A Robust Domain Adaptive Detection Transformer Framework"></a><a href="">AAAI2024: Mean Teacher DETR with Masked Feature Alignment: A Robust Domain Adaptive Detection Transformer Framework</a></h2><h3 id="Author-翁玮熙"><a href="#Author-翁玮熙" class="headerlink" title="Author: 翁玮熙"></a>Author: 翁玮熙</h3><h3 id="Abstract-29"><a href="#Abstract-29" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="AAAI2024-Deep-Homography-Estimation-for-Visual-Place-Recognition"><a href="#AAAI2024-Deep-Homography-Estimation-for-Visual-Place-Recognition" class="headerlink" title="AAAI2024: Deep Homography Estimation for Visual Place Recognition"></a><a href="">AAAI2024: Deep Homography Estimation for Visual Place Recognition</a></h2><h3 id="Author-卢锋-3"><a href="#Author-卢锋-3" class="headerlink" title="Author: 卢锋"></a>Author: 卢锋</h3><h3 id="Abstract-30"><a href="#Abstract-30" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="IEEE-T-MM-Towards-Effective-Collaborative-Learning-in-Long-Tailed-Recognition"><a href="#IEEE-T-MM-Towards-Effective-Collaborative-Learning-in-Long-Tailed-Recognition" class="headerlink" title="IEEE T-MM: Towards Effective Collaborative Learning in Long-Tailed Recognition"></a><a href="">IEEE T-MM: Towards Effective Collaborative Learning in Long-Tailed Recognition</a></h2><h3 id="Author-许正卓"><a href="#Author-许正卓" class="headerlink" title="Author: 许正卓"></a>Author: 许正卓</h3><h3 id="Abstract-31"><a href="#Abstract-31" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="ACL2023-LET-Leveraging-Error-Type-Information-for-Grammatical-Error-Correction"><a href="#ACL2023-LET-Leveraging-Error-Type-Information-for-Grammatical-Error-Correction" class="headerlink" title="ACL2023: LET: Leveraging Error Type Information for Grammatical Error Correction"></a><a href="">ACL2023: LET: Leveraging Error Type Information for Grammatical Error Correction</a></h2><h3 id="Author-李泓嘉"><a href="#Author-李泓嘉" class="headerlink" title="Author: 李泓嘉"></a>Author: 李泓嘉</h3><h3 id="Abstract-32"><a href="#Abstract-32" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="ACL2023-Tailoring-Instructions-to-Student’s-Learning-Levels-Boosts-Knowledge-Distillation"><a href="#ACL2023-Tailoring-Instructions-to-Student’s-Learning-Levels-Boosts-Knowledge-Distillation" class="headerlink" title="ACL2023: Tailoring Instructions to Student’s Learning Levels Boosts Knowledge Distillation"></a><a href="">ACL2023: Tailoring Instructions to Student’s Learning Levels Boosts Knowledge Distillation</a></h2><h3 id="Author-任昱鑫"><a href="#Author-任昱鑫" class="headerlink" title="Author: 任昱鑫"></a>Author: 任昱鑫</h3><h3 id="Abstract-33"><a href="#Abstract-33" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="IEEE-T-MM-HHF-Hashing-guided-Hinge-Function-for-Deep-Hashing-Retrieval"><a href="#IEEE-T-MM-HHF-Hashing-guided-Hinge-Function-for-Deep-Hashing-Retrieval" class="headerlink" title="IEEE T-MM: HHF: Hashing-guided Hinge Function for Deep Hashing Retrieval"></a><a href="">IEEE T-MM: HHF: Hashing-guided Hinge Function for Deep Hashing Retrieval</a></h2><h3 id="Author-徐呈寅"><a href="#Author-徐呈寅" class="headerlink" title="Author: 徐呈寅"></a>Author: 徐呈寅</h3><h3 id="Abstract-34"><a href="#Abstract-34" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="ACM-MM2023-Enhanced-Image-Deblurring-An-Efficient-Frequency-Exploitation-and-Preservation-Network"><a href="#ACM-MM2023-Enhanced-Image-Deblurring-An-Efficient-Frequency-Exploitation-and-Preservation-Network" class="headerlink" title="ACM MM2023: Enhanced Image Deblurring: An Efficient Frequency Exploitation and Preservation Network"></a><a href="">ACM MM2023: Enhanced Image Deblurring: An Efficient Frequency Exploitation and Preservation Network</a></h2><h3 id="Author-董姝婷"><a href="#Author-董姝婷" class="headerlink" title="Author: 董姝婷"></a>Author: 董姝婷</h3><h3 id="Abstract-35"><a href="#Abstract-35" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="IJCAI2023-DFVSR-Directional-Frequency-Video-Super-Resolution-via-Asymmetric-and-Enhancement-Alignment-Network"><a href="#IJCAI2023-DFVSR-Directional-Frequency-Video-Super-Resolution-via-Asymmetric-and-Enhancement-Alignment-Network" class="headerlink" title="IJCAI2023: DFVSR: Directional Frequency Video Super-Resolution via Asymmetric and Enhancement Alignment Network"></a><a href="">IJCAI2023: DFVSR: Directional Frequency Video Super-Resolution via Asymmetric and Enhancement Alignment Network</a></h2><h3 id="Author-董姝婷-1"><a href="#Author-董姝婷-1" class="headerlink" title="Author: 董姝婷"></a>Author: 董姝婷</h3><h3 id="Abstract-36"><a href="#Abstract-36" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="ACM-MM2023-Adaptive-Contrastive-Learning-for-Learning-Robust-Representations-under-Label-Noise"><a href="#ACM-MM2023-Adaptive-Contrastive-Learning-for-Learning-Robust-Representations-under-Label-Noise" class="headerlink" title="ACM MM2023: Adaptive Contrastive Learning for Learning Robust Representations under Label Noise"></a><a href="">ACM MM2023: Adaptive Contrastive Learning for Learning Robust Representations under Label Noise</a></h2><h3 id="Author-王子浩"><a href="#Author-王子浩" class="headerlink" title="Author: 王子浩"></a>Author: 王子浩</h3><h3 id="Abstract-37"><a href="#Abstract-37" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="ICCV2023-HiFace-High-Fidelity-3D-Face-Reconstruction-by-Learning-Static-and-Dynamic-Details"><a href="#ICCV2023-HiFace-High-Fidelity-3D-Face-Reconstruction-by-Learning-Static-and-Dynamic-Details" class="headerlink" title="ICCV2023: HiFace: High-Fidelity 3D Face Reconstruction by Learning Static and Dynamic Details"></a><a href="">ICCV2023: HiFace: High-Fidelity 3D Face Reconstruction by Learning Static and Dynamic Details</a></h2><h3 id="Author-柴增豪"><a href="#Author-柴增豪" class="headerlink" title="Author: 柴增豪"></a>Author: 柴增豪</h3><h3 id="Abstract-38"><a href="#Abstract-38" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="ICCV2023-Accurate-3D-Face-Reconstruction-with-Facial-Component-Tokens"><a href="#ICCV2023-Accurate-3D-Face-Reconstruction-with-Facial-Component-Tokens" class="headerlink" title="ICCV2023: Accurate 3D Face Reconstruction with Facial Component Tokens"></a><a href="">ICCV2023: Accurate 3D Face Reconstruction with Facial Component Tokens</a></h2><h3 id="Author-章天珂"><a href="#Author-章天珂" class="headerlink" title="Author: 章天珂"></a>Author: 章天珂</h3><h3 id="Abstract-39"><a href="#Abstract-39" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="ICCVW2023-Effective-Whole-body-Pose-Estimation-with-Two-stages-Distillation"><a href="#ICCVW2023-Effective-Whole-body-Pose-Estimation-with-Two-stages-Distillation" class="headerlink" title="ICCVW2023: Effective Whole-body Pose Estimation with Two-stages Distillation"></a><a href="">ICCVW2023: Effective Whole-body Pose Estimation with Two-stages Distillation</a></h2><h3 id="Author-杨震东"><a href="#Author-杨震东" class="headerlink" title="Author: 杨震东"></a>Author: 杨震东</h3><h3 id="Abstract-40"><a href="#Abstract-40" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="ICCV2023-From-Knowledge-Distillation-to-Self-Knowledge-Distillation-A-Unified-Approach-with-Normalized-Loss-and-Customized-Soft-Labels"><a href="#ICCV2023-From-Knowledge-Distillation-to-Self-Knowledge-Distillation-A-Unified-Approach-with-Normalized-Loss-and-Customized-Soft-Labels" class="headerlink" title="ICCV2023: From Knowledge Distillation to Self-Knowledge Distillation: A Unified Approach with Normalized Loss and Customized Soft Labels"></a><a href="">ICCV2023: From Knowledge Distillation to Self-Knowledge Distillation: A Unified Approach with Normalized Loss and Customized Soft Labels</a></h2><h3 id="Author-杨震东-1"><a href="#Author-杨震东-1" class="headerlink" title="Author: 杨震东"></a>Author: 杨震东</h3><h3 id="Abstract-41"><a href="#Abstract-41" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="ICCV2023-Make-Encoder-Great-Again-in-3D-GAN-Inversion-through-Geometry-and-Occlusion-Aware-Encoding"><a href="#ICCV2023-Make-Encoder-Great-Again-in-3D-GAN-Inversion-through-Geometry-and-Occlusion-Aware-Encoding" class="headerlink" title="ICCV2023: Make Encoder Great Again in 3D GAN Inversion through Geometry and Occlusion-Aware Encoding"></a><a href="">ICCV2023: Make Encoder Great Again in 3D GAN Inversion through Geometry and Occlusion-Aware Encoding</a></h2><h3 id="Author-袁梓洋-2"><a href="#Author-袁梓洋-2" class="headerlink" title="Author: 袁梓洋"></a>Author: 袁梓洋</h3><h3 id="Abstract-42"><a href="#Abstract-42" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="ICML2023-UPop-Unified-and-Progressive-Pruning-for-Compressing-Vision-Language-Transformers"><a href="#ICML2023-UPop-Unified-and-Progressive-Pruning-for-Compressing-Vision-Language-Transformers" class="headerlink" title="ICML2023: UPop: Unified and Progressive Pruning for Compressing Vision-Language Transformers"></a><a href="">ICML2023: UPop: Unified and Progressive Pruning for Compressing Vision-Language Transformers</a></h2><h3 id="Author-石大川-1"><a href="#Author-石大川-1" class="headerlink" title="Author: 石大川"></a>Author: 石大川</h3><h3 id="Abstract-43"><a href="#Abstract-43" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="ICML2023-Learning-to-Learn-from-APIs-Black-Box-Data-Free-Meta-Learning"><a href="#ICML2023-Learning-to-Learn-from-APIs-Black-Box-Data-Free-Meta-Learning" class="headerlink" title="ICML2023: Learning to Learn from APIs: Black-Box Data-Free Meta-Learning"></a><a href="">ICML2023: Learning to Learn from APIs: Black-Box Data-Free Meta-Learning</a></h2><h3 id="Author-胡梓轩-1"><a href="#Author-胡梓轩-1" class="headerlink" title="Author: 胡梓轩"></a>Author: 胡梓轩</h3><h3 id="Abstract-44"><a href="#Abstract-44" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="ICRA2023-AANet-Aggregation-and-Alignment-Network-with-Semi-hard-Positive-Sample-Mining-for-Hierarchical-Place-Recognition"><a href="#ICRA2023-AANet-Aggregation-and-Alignment-Network-with-Semi-hard-Positive-Sample-Mining-for-Hierarchical-Place-Recognition" class="headerlink" title="ICRA2023: AANet: Aggregation and Alignment Network with Semi-hard Positive Sample Mining for Hierarchical Place Recognition"></a><a href="">ICRA2023: AANet: Aggregation and Alignment Network with Semi-hard Positive Sample Mining for Hierarchical Place Recognition</a></h2><h3 id="Author-卢锋-4"><a href="#Author-卢锋-4" class="headerlink" title="Author: 卢锋"></a>Author: 卢锋</h3><h3 id="Abstract-45"><a href="#Abstract-45" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="CVPR2023-High-fidelity-Facial-Avatar-Reconstruction-from-Monocular-Video-with-Generative-Priors"><a href="#CVPR2023-High-fidelity-Facial-Avatar-Reconstruction-from-Monocular-Video-with-Generative-Priors" class="headerlink" title="CVPR2023: High-fidelity Facial Avatar Reconstruction from Monocular Video with Generative Priors"></a><a href="">CVPR2023: High-fidelity Facial Avatar Reconstruction from Monocular Video with Generative Priors</a></h2><h3 id="Author-白云鹏-1"><a href="#Author-白云鹏-1" class="headerlink" title="Author: 白云鹏"></a>Author: 白云鹏</h3><h3 id="Abstract-46"><a href="#Abstract-46" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="CVPR2023-Learning-Imbalanced-Data-with-Vision-Transformers"><a href="#CVPR2023-Learning-Imbalanced-Data-with-Vision-Transformers" class="headerlink" title="CVPR2023: Learning Imbalanced Data with Vision Transformers"></a><a href="">CVPR2023: Learning Imbalanced Data with Vision Transformers</a></h2><h3 id="Author-许正卓-1"><a href="#Author-许正卓-1" class="headerlink" title="Author: 许正卓"></a>Author: 许正卓</h3><h3 id="Abstract-47"><a href="#Abstract-47" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="IEEE-TCSVT-Task-adaptive-Feature-Disentanglement-and-Hallucination-for-Few-shot-Classification"><a href="#IEEE-TCSVT-Task-adaptive-Feature-Disentanglement-and-Hallucination-for-Few-shot-Classification" class="headerlink" title="IEEE TCSVT: Task-adaptive Feature Disentanglement and Hallucination for Few-shot Classification"></a><a href="">IEEE TCSVT: Task-adaptive Feature Disentanglement and Hallucination for Few-shot Classification</a></h2><h3 id="Author-胡梓轩-2"><a href="#Author-胡梓轩-2" class="headerlink" title="Author: 胡梓轩"></a>Author: 胡梓轩</h3><h3 id="Abstract-48"><a href="#Abstract-48" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="CVPR2023-Architecture-Dataset-and-Model-Scale-Agnostic-Data-free-Meta-Learning"><a href="#CVPR2023-Architecture-Dataset-and-Model-Scale-Agnostic-Data-free-Meta-Learning" class="headerlink" title="CVPR2023: Architecture, Dataset and Model-Scale Agnostic Data-free Meta-Learning"></a><a href="">CVPR2023: Architecture, Dataset and Model-Scale Agnostic Data-free Meta-Learning</a></h2><h3 id="Author-胡梓轩-3"><a href="#Author-胡梓轩-3" class="headerlink" title="Author: 胡梓轩"></a>Author: 胡梓轩</h3><h3 id="Abstract-49"><a href="#Abstract-49" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="CVPR2023-Uni-Perceiver-v2-A-Generalist-Model-for-Large-Scale-Vision-and-Vision-Language-Tasks"><a href="#CVPR2023-Uni-Perceiver-v2-A-Generalist-Model-for-Large-Scale-Vision-and-Vision-Language-Tasks" class="headerlink" title="CVPR2023: Uni-Perceiver v2: A Generalist Model for Large-Scale Vision and Vision-Language Tasks"></a><a href="">CVPR2023: Uni-Perceiver v2: A Generalist Model for Large-Scale Vision and Vision-Language Tasks</a></h2><h3 id="Author-江晓湖"><a href="#Author-江晓湖" class="headerlink" title="Author: 江晓湖"></a>Author: 江晓湖</h3><h3 id="Abstract-50"><a href="#Abstract-50" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="ICASSP2023-FREQUENCY-RECIPROCAL-ACTION-AND-FUSION-FOR-SINGLE-IMAGE-SUPER-RESOLUTION"><a href="#ICASSP2023-FREQUENCY-RECIPROCAL-ACTION-AND-FUSION-FOR-SINGLE-IMAGE-SUPER-RESOLUTION" class="headerlink" title="ICASSP2023: FREQUENCY RECIPROCAL ACTION AND FUSION FOR SINGLE IMAGE SUPER-RESOLUTION"></a><a href="">ICASSP2023: FREQUENCY RECIPROCAL ACTION AND FUSION FOR SINGLE IMAGE SUPER-RESOLUTION</a></h2><h3 id="Author-董姝婷-2"><a href="#Author-董姝婷-2" class="headerlink" title="Author: 董姝婷"></a>Author: 董姝婷</h3><h3 id="Abstract-51"><a href="#Abstract-51" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="ICASSP2023-Rethink-Long-Tailed-Recognition-With-Vision-Transforms"><a href="#ICASSP2023-Rethink-Long-Tailed-Recognition-With-Vision-Transforms" class="headerlink" title="ICASSP2023: Rethink Long-Tailed Recognition With Vision Transforms"></a><a href="">ICASSP2023: Rethink Long-Tailed Recognition With Vision Transforms</a></h2><h3 id="Author-许正卓-2"><a href="#Author-许正卓-2" class="headerlink" title="Author: 许正卓"></a>Author: 许正卓</h3><h3 id="Abstract-52"><a href="#Abstract-52" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="IEEE-TNNLS-PatchNet-Maximize-the-Exploration-of-Congeneric-Semantics-for-Weakly-Supervised-Semantic-Segmentation"><a href="#IEEE-TNNLS-PatchNet-Maximize-the-Exploration-of-Congeneric-Semantics-for-Weakly-Supervised-Semantic-Segmentation" class="headerlink" title="IEEE TNNLS: PatchNet: Maximize the Exploration of Congeneric Semantics for Weakly Supervised Semantic Segmentation"></a><a href="">IEEE TNNLS: PatchNet: Maximize the Exploration of Congeneric Semantics for Weakly Supervised Semantic Segmentation</a></h2><h3 id="Author-张可"><a href="#Author-张可" class="headerlink" title="Author: 张可"></a>Author: 张可</h3><h3 id="Abstract-53"><a href="#Abstract-53" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="AAAI2023-Truncate-Split-Contrast-A-Framework-for-Learning-from-Mislabeled-Videos"><a href="#AAAI2023-Truncate-Split-Contrast-A-Framework-for-Learning-from-Mislabeled-Videos" class="headerlink" title="AAAI2023: Truncate-Split-Contrast: A Framework for Learning from Mislabeled Videos"></a><a href="">AAAI2023: Truncate-Split-Contrast: A Framework for Learning from Mislabeled Videos</a></h2><h3 id="Author-王子啸"><a href="#Author-王子啸" class="headerlink" title="Author: 王子啸"></a>Author: 王子啸</h3><h3 id="Abstract-54"><a href="#Abstract-54" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="AAAI2023-Darwinian-Model-Upgrades-Model-Evolving-with-Selective-Compatibility"><a href="#AAAI2023-Darwinian-Model-Upgrades-Model-Evolving-with-Selective-Compatibility" class="headerlink" title="AAAI2023: Darwinian Model Upgrades: Model Evolving with Selective Compatibility"></a><a target="_blank" rel="noopener" href="https://ojs.aaai.org/index.php/AAAI/article/view/25447">AAAI2023: Darwinian Model Upgrades: Model Evolving with Selective Compatibility</a></h2><h3 id="Author-张斌杰"><a href="#Author-张斌杰" class="headerlink" title="Author: 张斌杰"></a>Author: 张斌杰</h3><h3 id="Abstract-55"><a href="#Abstract-55" class="headerlink" title="Abstract"></a>Abstract</h3><p>The traditional model upgrading paradigm for retrieval requires recomputing all gallery embeddings before deploying the new model (dubbed as “backfilling”), which is quite expensive and time-consuming considering billions of instances in industrial applications. BCT presents the first step towards backward-compatible model upgrades to get rid of backfilling. It is workable but leaves the new model in a dilemma between new feature discriminativeness and new-to-old compatibility due to the undifferentiated compatibility constraints. In this work, we propose Darwinian Model Upgrades (DMU), which disentangle the inheritance and variation in the model evolving with selective backward compatibility and forward adaptation, respectively. The old-to-new heritable knowledge is measured by old feature discriminativeness, and the gallery features, especially those of poor quality, are evolved in a lightweight manner to become more adaptive in the new latent space. We demonstrate the superiority of DMU through comprehensive experiments on large-scale landmark retrieval and face recognition benchmarks. DMU effectively alleviates the new-to-new degradation at the same time improving new-to-old compatibility, rendering a more proper model upgrading paradigm in large-scale retrieval systems.Code: <a target="_blank" rel="noopener" href="https://github.com/TencentARC/OpenCompatible">https://github.com/TencentARC/OpenCompatible</a>.</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="followme">
  <span>Welcome to my other publishing channels</span>

  <div class="social-list">

      <div class="social-item">
          <a target="_blank" class="social-link" href="https://scholar.google.com/citations?hl=en&user=fYdxi2sAAAAJ">
            <span class="icon">
              <i class=""></i>
            </span>

            <span class="label">Google Scholar</span>
          </a>
      </div>
  </div>
</div>


        

    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">袁春</span>
  </div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>







  





</body>
</html>
