<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#660874"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#660874">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha256-5eIC48iZUHmSlSUz9XtjRyK2mzQkHScZY1WdMaoz74E=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"thu-cvml.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.21.1","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":-1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="ICCV2023: When Noisy Labels Meet Long Tail Dilemmas: A Representation Calibration Method Author: 张曼怡 Abstract Real-world large-scale datasets are both noisily labeled and class-imbalanced. The i">
<meta property="og:type" content="article">
<meta property="og:title" content="2023-2025年CVML科研成果展示">
<meta property="og:url" content="https://thu-cvml.github.io/2024/12/11/%E7%A7%91%E7%A0%94%E5%BF%AB%E8%AE%AF/index.html">
<meta property="og:site_name" content="清华大学&lt;br&gt;深圳国际研究生院&lt;br&gt;袁春教授课题组">
<meta property="og:description" content="ICCV2023: When Noisy Labels Meet Long Tail Dilemmas: A Representation Calibration Method Author: 张曼怡 Abstract Real-world large-scale datasets are both noisily labeled and class-imbalanced. The i">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2024-12-11T08:34:09.000Z">
<meta property="article:modified_time" content="2024-12-18T05:16:13.901Z">
<meta property="article:author" content="袁春">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://thu-cvml.github.io/2024/12/11/%E7%A7%91%E7%A0%94%E5%BF%AB%E8%AE%AF/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://thu-cvml.github.io/2024/12/11/%E7%A7%91%E7%A0%94%E5%BF%AB%E8%AE%AF/","path":"2024/12/11/科研快讯/","title":"2023-2025年CVML科研成果展示"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>2023-2025年CVML科研成果展示 | 清华大学<br>深圳国际研究生院<br>袁春教授课题组</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">清华大学<br>深圳国际研究生院<br>袁春教授课题组</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">计算机视觉与机器学习<br>Computer Vision and Machine Learning<br>CVML</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-overview"><a href="/Professor/" rel="section"><i class="fa fa-book fa-fw"></i>Overview</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="Searching..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#iccv2023-when-noisy-labels-meet-long-tail-dilemmas-a-representation-calibration-method"><span class="nav-number">1.</span> <span class="nav-text">ICCV2023:
When Noisy Labels Meet Long Tail Dilemmas: A Representation Calibration
Method</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#author-%E5%BC%A0%E6%9B%BC%E6%80%A1"><span class="nav-number">1.1.</span> <span class="nav-text">Author: 张曼怡</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#abstract"><span class="nav-number">1.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#aaai2025-aligning-composed-query-with-image-via-discriminative-perception-from-negative-correspondences"><span class="nav-number">2.</span> <span class="nav-text">AAAI2025: Aligning Composed Query with Image via Discriminative
Perception from Negative Correspondences</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#author-%E7%8E%8B%E4%BE%9D%E5%87%A1"><span class="nav-number">2.1.</span> <span class="nav-text">Author: 王依凡</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#abstract"><span class="nav-number">2.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#aaai2025-rethinking-pseudo-label-guided-learning-for-weakly-supervised-temporal-action-localization-from-the-perspective-of-noise-correction"><span class="nav-number">3.</span> <span class="nav-text">AAAI2025: Rethinking Pseudo-Label Guided Learning for
Weakly-Supervised Temporal Action Localization from the Perspective of
Noise Correction</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#author-%E5%BC%A0%E6%9D%83"><span class="nav-number">3.1.</span> <span class="nav-text">Author: 张权</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#abstract"><span class="nav-number">3.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#neurips2025-supervlad-compact-and-robust-image-descriptors-for-visual-place-recognition"><span class="nav-number">4.</span> <span class="nav-text">NeurIPS2025: SuperVLAD: Compact and Robust Image Descriptors for
Visual Place Recognition</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#author-%E5%8D%A2%E9%94%8B"><span class="nav-number">4.1.</span> <span class="nav-text">Author: 卢锋</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#abstract"><span class="nav-number">4.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#icpr2024-benchmarking-ai-in-mental-health-a-critical-examination-of-llms-across-key-performance-and-ethical-metrics"><span class="nav-number">5.</span> <span class="nav-text">ICPR2024:
Benchmarking AI in Mental Health: A Critical Examination of LLMs Across
Key Performance and Ethical Metrics</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#author-%E8%A2%81%E7%9D%BF"><span class="nav-number">5.1.</span> <span class="nav-text">Author: 袁睿</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#abstract"><span class="nav-number">5.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#icpr2024-amc-oa-adaptive-multi-scale-convolutional-networks-with-optimized-attention-for-temporal-action-localization"><span class="nav-number">6.</span> <span class="nav-text">ICPR2024:
AMC-OA: Adaptive Multi-Scale Convolutional Networks with Optimized
Attention for Temporal Action Localization</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#author-%E8%A2%81%E7%9D%BF"><span class="nav-number">6.1.</span> <span class="nav-text">Author: 袁睿</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#abstract"><span class="nav-number">6.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#acm-mm2024-semantic-distillation-from-neighborhood-for-composed-image-retrieval"><span class="nav-number">7.</span> <span class="nav-text">ACM MM2024:
Semantic Distillation from Neighborhood for Composed Image
Retrieval</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#author-%E7%8E%8B%E4%BE%9D%E5%87%A1"><span class="nav-number">7.1.</span> <span class="nav-text">Author: 王依凡</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#abstract"><span class="nav-number">7.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#acm-mm2024-customnet-object-customization-with-variable-viewpoints-in-text-to-image-diffusion-models"><span class="nav-number">8.</span> <span class="nav-text">ACM MM2024:
CustomNet: Object Customization with Variable-Viewpoints in
Text-to-Image Diffusion Models</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#author-%E8%A2%81%E6%A2%93%E6%B4%8B"><span class="nav-number">8.1.</span> <span class="nav-text">Author: 袁梓洋</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#abstract"><span class="nav-number">8.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#eccv2024-gvgen-text-to-3d-generation-with-volumetric-representation"><span class="nav-number">9.</span> <span class="nav-text">ECCV2024:
GVGEN: Text-to-3D Generation with Volumetric Representation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#author-%E4%BD%95%E7%9B%B8%E9%BE%99"><span class="nav-number">9.1.</span> <span class="nav-text">Author: 何相龙</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#abstract"><span class="nav-number">9.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#eccv2024-a-task-is-worth-one-word-learning-with-task-prompts-for-high-quality-versatile-image-inpainting"><span class="nav-number">10.</span> <span class="nav-text">ECCV2024:
A Task is Worth One Word: Learning with Task Prompts for High-Quality
Versatile Image Inpainting</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#author-%E5%BA%84%E4%BF%8A%E8%B1%AA"><span class="nav-number">10.1.</span> <span class="nav-text">Author: 庄俊豪</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#abstract"><span class="nav-number">10.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#eccv2024-dreamdiffusion-high-quality-eeg-to-image-generation-with-temporal-masked-signal-modeling-and-clip-alignment"><span class="nav-number">11.</span> <span class="nav-text">ECCV2024:
DreamDiffusion: High-Quality EEG-to-Image Generation with Temporal
Masked Signal Modeling and CLIP Alignment</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#author-%E7%99%BD%E4%BA%91%E9%B9%8F"><span class="nav-number">11.1.</span> <span class="nav-text">Author: 白云鹏</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#abstract"><span class="nav-number">11.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#eccv2024-mirrorgaussian-reflecting-3d-gaussians-for-reconstructing-mirror-reflections"><span class="nav-number">12.</span> <span class="nav-text">ECCV2024:
MirrorGaussian: Reflecting 3D Gaussians for Reconstructing Mirror
Reflections</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#author-%E5%88%98%E4%BD%B3%E6%9C%88"><span class="nav-number">12.1.</span> <span class="nav-text">Author: 刘佳月</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#abstract"><span class="nav-number">12.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ieee-tcsvt-meta-learning-without-data-via-unconditional-diffusion-models"><span class="nav-number">13.</span> <span class="nav-text">IEEE
TCSVT: Meta-Learning without Data via Unconditional Diffusion
Models</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#author-%E9%9F%A6%E6%B0%B8%E8%B4%A4"><span class="nav-number">13.1.</span> <span class="nav-text">Author: 韦永贤</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#abstract"><span class="nav-number">13.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#icml2024-crossget-cross-guided-ensemble-of-tokens-for-accelerating-vision-language-transformers"><span class="nav-number">14.</span> <span class="nav-text">ICML2024: CrossGET: Cross-Guided
Ensemble of Tokens for Accelerating Vision-Language
Transformers</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#author-%E7%9F%B3%E5%A4%A7%E5%B7%9D"><span class="nav-number">14.1.</span> <span class="nav-text">Author: 石大川</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#abstract"><span class="nav-number">14.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#icml2024-sparse-model-inversion-efficient-inversion-of-vision-transformers-with-less-hallucination"><span class="nav-number">15.</span> <span class="nav-text">ICML2024: Sparse Model
Inversion: Efficient Inversion of Vision Transformers with Less
Hallucination</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#author-%E8%83%A1%E6%A2%93%E8%BD%A9"><span class="nav-number">15.1.</span> <span class="nav-text">Author: 胡梓轩</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#abstract"><span class="nav-number">15.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#icml2024-task-groupings-regularization-data-free-meta-learning-with-heterogeneous-pre-trained-models"><span class="nav-number">16.</span> <span class="nav-text">ICML2024: Task Groupings
Regularization: Data-Free Meta-Learning with Heterogeneous Pre-trained
Models</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#author-%E9%9F%A6%E6%B0%B8%E8%B4%A4"><span class="nav-number">16.1.</span> <span class="nav-text">Author: 韦永贤</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#abstract"><span class="nav-number">16.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#icml2024-dfd-distillng-the-feature-disparity-differently-for-detectors"><span class="nav-number">17.</span> <span class="nav-text">ICML2024: DFD:
Distillng the Feature Disparity Differently for Detectors</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#author-%E5%88%98%E5%BA%B7"><span class="nav-number">17.1.</span> <span class="nav-text">Author: 刘康</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#abstract"><span class="nav-number">17.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ijcnn2024-integrating-local-global-features-for-estimating-shortest-path-distance-in-large-scale-graphs"><span class="nav-number">18.</span> <span class="nav-text">IJCNN2024:
Integrating Local &amp; Global Features for Estimating Shortest-Path
Distance in Large-Scale Graphs</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#author-%E7%8E%8B%E6%B5%A9%E5%AE%87"><span class="nav-number">18.1.</span> <span class="nav-text">Author: 王浩宇</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#abstract"><span class="nav-number">18.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ijcnn2024-mmr-multi-scale-motion-retargeting-between-skeleton-agnostic-characters"><span class="nav-number">19.</span> <span class="nav-text">IJCNN2024:
MMR: Multi-Scale Motion Retargeting Between Skeleton-Agnostic
Characters</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#author-%E7%8E%8B%E6%B5%A9%E5%AE%87"><span class="nav-number">19.1.</span> <span class="nav-text">Author: 王浩宇</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#abstract"><span class="nav-number">19.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ijcnn2024-error-bound-based-noise-schedule-design-in-diffusion-models"><span class="nav-number">20.</span> <span class="nav-text">IJCNN2024:
Error Bound Based Noise Schedule Design in Diffusion Models</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#author-%E5%88%98%E5%8A%9B%E6%BA%90"><span class="nav-number">20.1.</span> <span class="nav-text">Author: 刘力源</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#abstract"><span class="nav-number">20.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ijcnn2024-noise-weighting-phased-prompt-image-editing"><span class="nav-number">21.</span> <span class="nav-text">IJCNN2024:
Noise Weighting Phased Prompt Image Editing</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#author-%E5%BE%90%E5%9B%BD%E7%82%9C"><span class="nav-number">21.1.</span> <span class="nav-text">Author: 徐国炜</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#abstract"><span class="nav-number">21.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#cvpr2024-distilling-semantic-priors-from-sam-to-efficient-image-restoration-models"><span class="nav-number">22.</span> <span class="nav-text">CVPR2024:
Distilling Semantic Priors from SAM to Efficient Image Restoration
Models</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#author-%E5%BC%A0%E6%9D%83"><span class="nav-number">22.1.</span> <span class="nav-text">Author: 张权</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#abstract"><span class="nav-number">22.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#cvpr2024-cricavpr-cross-image-correlation-aware-representation-learning-for-visual-place-recognition"><span class="nav-number">23.</span> <span class="nav-text">CVPR2024:
CricaVPR: Cross-image Correlation-aware Representation Learning for
Visual Place Recognition</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#author-%E5%8D%A2%E9%94%8B"><span class="nav-number">23.1.</span> <span class="nav-text">Author: 卢锋</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#abstract"><span class="nav-number">23.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#cvpr2024-a-free-lunch-for-faster-and-better-data-free-meta-learning"><span class="nav-number">24.</span> <span class="nav-text">CVPR2024:
A Free Lunch for Faster and Better Data-Free Meta-Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#author-%E9%9F%A6%E6%B0%B8%E8%B4%A4"><span class="nav-number">24.1.</span> <span class="nav-text">Author: 韦永贤</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#abstract"><span class="nav-number">24.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ieee-t-mm-negative-sensitive-framework-with-semantic-enhancement-for-composed-image-retrieval"><span class="nav-number">25.</span> <span class="nav-text">IEEE T-MM:
Negative-Sensitive Framework with Semantic Enhancement for Composed
Image Retrieval</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#author-%E7%8E%8B%E4%BE%9D%E5%87%A1"><span class="nav-number">25.1.</span> <span class="nav-text">Author: 王依凡</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#abstract"><span class="nav-number">25.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#iclr2024-convolution-meets-lora-parameter-efficient-finetuning-for-segment-anything-model"><span class="nav-number">26.</span> <span class="nav-text">ICLR2024: Convolution Meets
LoRA: Parameter Efficient Finetuning for Segment Anything Model</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#author-%E9%92%9F%E5%AD%90%E6%B6%B5"><span class="nav-number">26.1.</span> <span class="nav-text">Author: 钟子涵</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#abstract"><span class="nav-number">26.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#iclr2024-towards-seamless-adaptation-of-pre-trained-models-for-visual-place-recognition"><span class="nav-number">27.</span> <span class="nav-text">ICLR2024: Towards Seamless
Adaptation of Pre-trained Models for Visual Place Recognition</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#author-%E5%8D%A2%E9%94%8B"><span class="nav-number">27.1.</span> <span class="nav-text">Author: 卢锋</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#abstract"><span class="nav-number">27.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#aaai2024-efficient-conditional-diffusion-model-with-probability-flow-sampling-for-image-super-resolution"><span class="nav-number">28.</span> <span class="nav-text">AAAI2024:
Efficient Conditional Diffusion Model with Probability Flow Sampling for
Image Super-resolution</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#author-%E8%A2%81%E5%AE%87%E9%9F%AC"><span class="nav-number">28.1.</span> <span class="nav-text">Author: 袁宇韬</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#abstract"><span class="nav-number">28.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#aaai2024-blind-face-restoration-under-extreme-conditions-leveraging-3d-2d-prior-fusion-for-superior-structural-and-texture-recovery"><span class="nav-number">29.</span> <span class="nav-text">AAAI2024:
Blind Face Restoration under Extreme Conditions: Leveraging 3D-2D Prior
Fusion for Superior Structural and Texture Recovery</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#author-%E8%A2%81%E6%A2%93%E6%B4%8B"><span class="nav-number">29.1.</span> <span class="nav-text">Author: 袁梓洋</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#abstract"><span class="nav-number">29.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#aaai2024-mean-teacher-detr-with-masked-feature-alignment-a-robust-domain-adaptive-detection-transformer-framework"><span class="nav-number">30.</span> <span class="nav-text">AAAI2024:
Mean Teacher DETR with Masked Feature Alignment: A Robust Domain
Adaptive Detection Transformer Framework</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#author-%E7%BF%81%E7%8E%AE%E7%86%99"><span class="nav-number">30.1.</span> <span class="nav-text">Author: 翁玮熙</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#abstract"><span class="nav-number">30.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#aaai2024-deep-homography-estimation-for-visual-place-recognition"><span class="nav-number">31.</span> <span class="nav-text">AAAI2024:
Deep Homography Estimation for Visual Place Recognition</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#author-%E5%8D%A2%E9%94%8B"><span class="nav-number">31.1.</span> <span class="nav-text">Author: 卢锋</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#abstract"><span class="nav-number">31.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ieee-t-mm-towards-effective-collaborative-learning-in-long-tailed-recognition"><span class="nav-number">32.</span> <span class="nav-text">IEEE T-MM:
Towards Effective Collaborative Learning in Long-Tailed
Recognition</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#author-%E8%AE%B8%E6%AD%A3%E5%8D%93"><span class="nav-number">32.1.</span> <span class="nav-text">Author: 许正卓</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#abstract"><span class="nav-number">32.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#acl2023-let-leveraging-error-type-information-for-grammatical-error-correction"><span class="nav-number">33.</span> <span class="nav-text">ACL2023: LET:
Leveraging Error Type Information for Grammatical Error
Correction</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#author-%E6%9D%8E%E6%B3%93%E5%98%89"><span class="nav-number">33.1.</span> <span class="nav-text">Author: 李泓嘉</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#abstract"><span class="nav-number">33.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#acl2023-tailoring-instructions-to-students-learning-levels-boosts-knowledge-distillation"><span class="nav-number">34.</span> <span class="nav-text">ACL2023: Tailoring Instructions
to Student&#39;s Learning Levels Boosts Knowledge Distillation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#author-%E4%BB%BB%E6%98%B1%E9%91%AB"><span class="nav-number">34.1.</span> <span class="nav-text">Author: 任昱鑫</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#abstract"><span class="nav-number">34.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ieee-t-mm-hhf-hashing-guided-hinge-function-for-deep-hashing-retrieval"><span class="nav-number">35.</span> <span class="nav-text">IEEE T-MM:
HHF: Hashing-guided Hinge Function for Deep Hashing Retrieval</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#author-%E5%BE%90%E5%91%88%E5%AF%85"><span class="nav-number">35.1.</span> <span class="nav-text">Author: 徐呈寅</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#abstract"><span class="nav-number">35.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#acm-mm2023-enhanced-image-deblurring-an-efficient-frequency-exploitation-and-preservation-network"><span class="nav-number">36.</span> <span class="nav-text">ACM MM2023:
Enhanced Image Deblurring: An Efficient Frequency Exploitation and
Preservation Network</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#author-%E8%91%A3%E5%A7%9D%E5%A9%B7"><span class="nav-number">36.1.</span> <span class="nav-text">Author: 董姝婷</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#abstract"><span class="nav-number">36.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ijcai2023-dfvsr-directional-frequency-video-super-resolution-via-asymmetric-and-enhancement-alignment-network"><span class="nav-number">37.</span> <span class="nav-text">IJCAI2023: DFVSR:
Directional Frequency Video Super-Resolution via Asymmetric and
Enhancement Alignment Network</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#author-%E8%91%A3%E5%A7%9D%E5%A9%B7"><span class="nav-number">37.1.</span> <span class="nav-text">Author: 董姝婷</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#abstract"><span class="nav-number">37.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#acm-mm2023-adaptive-contrastive-learning-for-learning-robust-representations-under-label-noise"><span class="nav-number">38.</span> <span class="nav-text">ACM MM2023:
Adaptive Contrastive Learning for Learning Robust Representations under
Label Noise</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#author-%E7%8E%8B%E5%AD%90%E6%B5%A9"><span class="nav-number">38.1.</span> <span class="nav-text">Author: 王子浩</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#abstract"><span class="nav-number">38.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#iccv2023-hiface-high-fidelity-3d-face-reconstruction-by-learning-static-and-dynamic-details"><span class="nav-number">39.</span> <span class="nav-text">ICCV2023:
HiFace: High-Fidelity 3D Face Reconstruction by Learning Static and
Dynamic Details</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#author-%E6%9F%B4%E5%A2%9E%E8%B1%AA"><span class="nav-number">39.1.</span> <span class="nav-text">Author: 柴增豪</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#abstract"><span class="nav-number">39.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#iccv2023-accurate-3d-face-reconstruction-with-facial-component-tokens"><span class="nav-number">40.</span> <span class="nav-text">ICCV2023:
Accurate 3D Face Reconstruction with Facial Component Tokens</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#author-%E7%AB%A0%E5%A4%A9%E7%8F%82"><span class="nav-number">40.1.</span> <span class="nav-text">Author: 章天珂</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#abstract"><span class="nav-number">40.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#iccvw2023-effective-whole-body-pose-estimation-with-two-stages-distillation"><span class="nav-number">41.</span> <span class="nav-text">ICCVW2023:
Effective Whole-body Pose Estimation with Two-stages
Distillation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#author-%E6%9D%A8%E9%9C%87%E4%B8%9C"><span class="nav-number">41.1.</span> <span class="nav-text">Author: 杨震东</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#abstract"><span class="nav-number">41.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#iccv2023-from-knowledge-distillation-to-self-knowledge-distillation-a-unified-approach-with-normalized-loss-and-customized-soft-labels"><span class="nav-number">42.</span> <span class="nav-text">ICCV2023:
From Knowledge Distillation to Self-Knowledge Distillation: A Unified
Approach with Normalized Loss and Customized Soft Labels</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#author-%E6%9D%A8%E9%9C%87%E4%B8%9C"><span class="nav-number">42.1.</span> <span class="nav-text">Author: 杨震东</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#abstract"><span class="nav-number">42.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#iccv2023-make-encoder-great-again-in-3d-gan-inversion-through-geometry-and-occlusion-aware-encoding"><span class="nav-number">43.</span> <span class="nav-text">ICCV2023:
Make Encoder Great Again in 3D GAN Inversion through Geometry and
Occlusion-Aware Encoding</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#author-%E8%A2%81%E6%A2%93%E6%B4%8B"><span class="nav-number">43.1.</span> <span class="nav-text">Author: 袁梓洋</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#abstract"><span class="nav-number">43.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#icml2023-upop-unified-and-progressive-pruning-for-compressing-vision-language-transformers"><span class="nav-number">44.</span> <span class="nav-text">ICML2023: UPop:
Unified and Progressive Pruning for Compressing Vision-Language
Transformers</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#author-%E7%9F%B3%E5%A4%A7%E5%B7%9D"><span class="nav-number">44.1.</span> <span class="nav-text">Author: 石大川</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#abstract"><span class="nav-number">44.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#icml2023-learning-to-learn-from-apis-black-box-data-free-meta-learning"><span class="nav-number">45.</span> <span class="nav-text">ICML2023: Learning
to Learn from APIs: Black-Box Data-Free Meta-Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#author-%E8%83%A1%E6%A2%93%E8%BD%A9"><span class="nav-number">45.1.</span> <span class="nav-text">Author: 胡梓轩</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#abstract"><span class="nav-number">45.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#icra2023-aanet-aggregation-and-alignment-network-with-semi-hard-positive-sample-mining-for-hierarchical-place-recognition"><span class="nav-number">46.</span> <span class="nav-text">ICRA2023:
AANet: Aggregation and Alignment Network with Semi-hard Positive Sample
Mining for Hierarchical Place Recognition</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#author-%E5%8D%A2%E9%94%8B"><span class="nav-number">46.1.</span> <span class="nav-text">Author: 卢锋</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#abstract"><span class="nav-number">46.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#cvpr2023-high-fidelity-facial-avatar-reconstruction-from-monocular-video-with-generative-priors"><span class="nav-number">47.</span> <span class="nav-text">CVPR2023:
High-fidelity Facial Avatar Reconstruction from Monocular Video with
Generative Priors</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#author-%E7%99%BD%E4%BA%91%E9%B9%8F"><span class="nav-number">47.1.</span> <span class="nav-text">Author: 白云鹏</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#abstract"><span class="nav-number">47.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#cvpr2023-learning-imbalanced-data-with-vision-transformers"><span class="nav-number">48.</span> <span class="nav-text">CVPR2023:
Learning Imbalanced Data with Vision Transformers</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#author-%E8%AE%B8%E6%AD%A3%E5%8D%93"><span class="nav-number">48.1.</span> <span class="nav-text">Author: 许正卓</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#abstract"><span class="nav-number">48.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ieee-tcsvt-task-adaptive-feature-disentanglement-and-hallucination-for-few-shot-classification"><span class="nav-number">49.</span> <span class="nav-text">IEEE
TCSVT: Task-adaptive Feature Disentanglement and Hallucination for
Few-shot Classification</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#author-%E8%83%A1%E6%A2%93%E8%BD%A9"><span class="nav-number">49.1.</span> <span class="nav-text">Author: 胡梓轩</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#abstract"><span class="nav-number">49.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#cvpr2023-architecture-dataset-and-model-scale-agnostic-data-free-meta-learning"><span class="nav-number">50.</span> <span class="nav-text">CVPR2023:
Architecture, Dataset and Model-Scale Agnostic Data-free
Meta-Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#author-%E8%83%A1%E6%A2%93%E8%BD%A9"><span class="nav-number">50.1.</span> <span class="nav-text">Author: 胡梓轩</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#abstract"><span class="nav-number">50.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#cvpr2023-uni-perceiver-v2-a-generalist-model-for-large-scale-vision-and-vision-language-tasks"><span class="nav-number">51.</span> <span class="nav-text">CVPR2023:
Uni-Perceiver v2: A Generalist Model for Large-Scale Vision and
Vision-Language Tasks</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#author-%E6%B1%9F%E6%99%93%E6%B9%96"><span class="nav-number">51.1.</span> <span class="nav-text">Author: 江晓湖</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#abstract"><span class="nav-number">51.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#icassp2023-frequency-reciprocal-action-and-fusion-for-single-image-super-resolution"><span class="nav-number">52.</span> <span class="nav-text">ICASSP2023:
FREQUENCY RECIPROCAL ACTION AND FUSION FOR SINGLE IMAGE
SUPER-RESOLUTION</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#author-%E8%91%A3%E5%A7%9D%E5%A9%B7"><span class="nav-number">52.1.</span> <span class="nav-text">Author: 董姝婷</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#abstract"><span class="nav-number">52.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#icassp2023-rethink-long-tailed-recognition-with-vision-transforms"><span class="nav-number">53.</span> <span class="nav-text">ICASSP2023:
Rethink Long-Tailed Recognition With Vision Transforms</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#author-%E8%AE%B8%E6%AD%A3%E5%8D%93"><span class="nav-number">53.1.</span> <span class="nav-text">Author: 许正卓</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#abstract"><span class="nav-number">53.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ieee-tnnls-patchnet-maximize-the-exploration-of-congeneric-semantics-for-weakly-supervised-semantic-segmentation"><span class="nav-number">54.</span> <span class="nav-text">IEEE
TNNLS: PatchNet: Maximize the Exploration of Congeneric Semantics for
Weakly Supervised Semantic Segmentation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#author-%E5%BC%A0%E5%8F%AF"><span class="nav-number">54.1.</span> <span class="nav-text">Author: 张可</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#abstract"><span class="nav-number">54.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#aaai2023-truncate-split-contrast-a-framework-for-learning-from-mislabeled-videos"><span class="nav-number">55.</span> <span class="nav-text">AAAI2023:
Truncate-Split-Contrast: A Framework for Learning from Mislabeled
Videos</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#author-%E7%8E%8B%E5%AD%90%E5%95%B8"><span class="nav-number">55.1.</span> <span class="nav-text">Author: 王子啸</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#abstract"><span class="nav-number">55.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#aaai2023-darwinian-model-upgrades-model-evolving-with-selective-compatibility"><span class="nav-number">56.</span> <span class="nav-text">AAAI2023:
Darwinian Model Upgrades: Model Evolving with Selective
Compatibility</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#author-%E5%BC%A0%E6%96%8C%E6%9D%B0"><span class="nav-number">56.1.</span> <span class="nav-text">Author: 张斌杰</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#abstract"><span class="nav-number">56.2.</span> <span class="nav-text">Abstract</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="袁春"
      src="/images/Chunyuan.jpg">
  <p class="site-author-name" itemprop="name">袁春</p>
  <div class="site-description" itemprop="description">教授，博士生导师</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">1</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://scholar.google.com/citations?hl=en&user=fYdxi2sAAAAJ" title="Google Scholar → https:&#x2F;&#x2F;scholar.google.com&#x2F;citations?hl&#x3D;en&amp;user&#x3D;fYdxi2sAAAAJ" rel="noopener me" target="_blank">Google Scholar</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://thu-cvml.github.io/2024/12/11/%E7%A7%91%E7%A0%94%E5%BF%AB%E8%AE%AF/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/Chunyuan.jpg">
      <meta itemprop="name" content="袁春">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="清华大学<br>深圳国际研究生院<br>袁春教授课题组">
      <meta itemprop="description" content="教授，博士生导师">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="2023-2025年CVML科研成果展示 | 清华大学<br>深圳国际研究生院<br>袁春教授课题组">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          2023-2025年CVML科研成果展示
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-12-11 16:34:09" itemprop="dateCreated datePublished" datetime="2024-12-11T16:34:09+08:00">2024-12-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-12-18 13:16:13" itemprop="dateModified" datetime="2024-12-18T13:16:13+08:00">2024-12-18</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><div class="note primary"><h2
id="iccv2023-when-noisy-labels-meet-long-tail-dilemmas-a-representation-calibration-method"><a
target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_When_Noisy_Labels_Meet_Long_Tail_Dilemmas_A_Representation_Calibration_ICCV_2023_paper.html?trk=public_post_comment-text">ICCV2023:
When Noisy Labels Meet Long Tail Dilemmas: A Representation Calibration
Method</a></h2>
<h3 id="author-张曼怡">Author: 张曼怡</h3>
<h3 id="abstract">Abstract</h3>
<p>Real-world large-scale datasets are both noisily labeled and
class-imbalanced. The issues seriously hurt the generalization of
trained models. It is hence significant to address the simultaneous
incorrect labeling and class-imbalance, i.e., the problem of learning
with noisy labels on long-tailed data. Previous works develop several
methods for the problem. However, they always rely on strong assumptions
that are invalid or hard to be checked in practice. In this paper, to
handle the problem and address the limitations of prior works, we
propose a representation calibration method RCAL. Specifically, RCAL
works with the representations extracted by unsupervised contrastive
learning. We assume that without incorrect labeling and class imbalance,
the representations of instances in each class conform to a multivariate
Gaussian distribution, which is much milder and easier to be checked.
Based on the assumption, we recover underlying representation
distributions from polluted ones resulting from mislabeled and
class-imbalanced data. Additional data points are then sampled from the
recovered distributions to help generalization. Moreover, during
classifier training, representation learning takes advantage of
representation robustness brought by contrastive learning, which further
improves the classifier performance. We derive theoretical results to
discuss the effectiveness of our representation calibration. Experiments
on multiple benchmarks justify our claims and confirm the superiority of
the proposed method.</p>
<p><img
src="https://raw.githubusercontent.com/THU-CVML/thu-cvml.github.io/refs/heads/main/images/RCAL.png" /></p>
</div>
<div class="note primary"><h2
id="aaai2025-aligning-composed-query-with-image-via-discriminative-perception-from-negative-correspondences"><a
href="">AAAI2025: Aligning Composed Query with Image via Discriminative
Perception from Negative Correspondences</a></h2>
<h3 id="author-王依凡">Author: 王依凡</h3>
<h3 id="abstract">Abstract</h3>
</div>
<div class="note primary"><h2
id="aaai2025-rethinking-pseudo-label-guided-learning-for-weakly-supervised-temporal-action-localization-from-the-perspective-of-noise-correction"><a
href="">AAAI2025: Rethinking Pseudo-Label Guided Learning for
Weakly-Supervised Temporal Action Localization from the Perspective of
Noise Correction</a></h2>
<h3 id="author-张权">Author: 张权</h3>
<h3 id="abstract">Abstract</h3>
</div>
<div class="note primary"><h2
id="neurips2025-supervlad-compact-and-robust-image-descriptors-for-visual-place-recognition"><a
href="">NeurIPS2025: SuperVLAD: Compact and Robust Image Descriptors for
Visual Place Recognition</a></h2>
<h3 id="author-卢锋">Author: 卢锋</h3>
<h3 id="abstract">Abstract</h3>
</div>
<div class="note primary"><h2
id="icpr2024-benchmarking-ai-in-mental-health-a-critical-examination-of-llms-across-key-performance-and-ethical-metrics"><a
target="_blank" rel="noopener" href="https://link.springer.com/chapter/10.1007/978-3-031-78104-9_24">ICPR2024:
Benchmarking AI in Mental Health: A Critical Examination of LLMs Across
Key Performance and Ethical Metrics</a></h2>
<h3 id="author-袁睿">Author: 袁睿</h3>
<h3 id="abstract">Abstract</h3>
<p>The rapid advancement of artificial intelligence (AI) has led to an
increasing application of Large Language Models (LLMs) in psychological
counseling. This study focuses on a comprehensive evaluation of LLMs in
this domain, moving beyond traditional case-based reasoning. We
introduce a novel multi-agent LLM framework that enhances the analysis
of psychological case interactions. Our approach involves expanding the
Emotional First Aid dataset with diverse client backgrounds, enhancing
its applicability and generalizability. A sophisticated user profile
model, incorporating eight critical dimensions, is developed and applied
within a multi-agent system to examine counseling scenarios. The
system’s performance is extensively evaluated based on accuracy,
robustness, consistency, and fairness. The findings reveal significant
differences among LLMs in these areas, highlighting their strengths and
limitations in psychological interventions. This research underscores
the need for ongoing refinement in LLM applications to ensure equitable
and reliable support in psychological counseling. The detailed results
and methodologies are available on the GitHub platform for further
academic scrutiny and development.</p>
</div>
<div class="note primary"><h2
id="icpr2024-amc-oa-adaptive-multi-scale-convolutional-networks-with-optimized-attention-for-temporal-action-localization"><a
target="_blank" rel="noopener" href="https://link.springer.com/chapter/10.1007/978-3-031-78107-0_9">ICPR2024:
AMC-OA: Adaptive Multi-Scale Convolutional Networks with Optimized
Attention for Temporal Action Localization</a></h2>
<h3 id="author-袁睿">Author: 袁睿</h3>
<h3 id="abstract">Abstract</h3>
<p>Temporal Action Localization (TAL) is crucial in video understanding,
focusing on identifying and timestamping actions within raw video
footage. A critical challenge in TAL is processing the rich
spatiotemporal details inherent in videos, traditionally addressed
through methods adapted from image processing. The Vision Transformer
(VIT) model marked a significant evolution, using a self-attention
mechanism for enhanced temporal information blending. Despite these
advancements, two key issues remain: insufficient extraction of spatial
semantic information at lower levels of feature pyramids and inadequate
capture of temporal semantic information at higher levels. To address
these challenges, we introduce Adaptive Multi-Scale Convolutional
Networks with Optimized Attention (AMC-OA). AMC-OA enhances lower-level
features within the pyramid using multi-scale convolutional kernels,
enriching spatial contextual semantics. Simultaneously, upper-level
features are refined with a temporally-focused contextual enhancement
network utilizing residual structures for better temporal understanding.
To further improve the model’s capability in handling extensive temporal
spans, we integrate an advanced multi-head attention mechanism.
Empirical results on benchmarks like THUMOS14 and ActivityNet1.3
demonstrate AMC-OA’s superiority in TAL tasks, significantly improving
both spatial and temporal information extraction compared to
state-of-the-art models.</p>
</div>
<div class="note primary"><h2
id="acm-mm2024-semantic-distillation-from-neighborhood-for-composed-image-retrieval"><a
target="_blank" rel="noopener" href="https://dl.acm.org/doi/abs/10.1145/3664647.3681493">ACM MM2024:
Semantic Distillation from Neighborhood for Composed Image
Retrieval</a></h2>
<h3 id="author-王依凡">Author: 王依凡</h3>
<h3 id="abstract">Abstract</h3>
<p>The challenging task composed image retrieval targets at identifying
the matched image from the multi-modal query with a reference image and
a textual modifier. Most existing methods are devoted to composing the
unified query representations from the query images and texts, yet the
distribution gaps between the hybrid-modal query representations and
visual target representations are neglected. However, directly
incorporating target features on the query may cause ambiguous rankings
and poor robustness due to the insufficient exploration of the
distinguishments and overfitting issues. To address the above concerns,
we propose a novel framework termed <em>SemAntic Distillation from
Neighborhood (SADN)</em> for composed image retrieval. For mitigating
the distribution divergences, we construct neighborhood sampling from
the target domain for each query and aggregate neighborhood features
with adaptive weights to restructure the query representations.
Specifically, the adaptive weights are determined by the collaboration
of two individual modules, as correspondence-induced adaption and
divergence-based correction. Correspondence-induced adaption accounts
for capturing the correlation alignments from neighbor features under
the guidance of the positive representations, and the divergence-based
correction regulates the weights based on the embedding distances
between hard negatives and the query in the latent space. Extensive
results and ablation studies on CIRR and FashionIQ validate that the
proposed semantic distillation from neighborhood significantly
outperforms baseline methods.</p>
</div>
<div class="note primary"><h2
id="acm-mm2024-customnet-object-customization-with-variable-viewpoints-in-text-to-image-diffusion-models"><a
target="_blank" rel="noopener" href="https://dl.acm.org/doi/abs/10.1145/3664647.3681396">ACM MM2024:
CustomNet: Object Customization with Variable-Viewpoints in
Text-to-Image Diffusion Models</a></h2>
<h3 id="author-袁梓洋">Author: 袁梓洋</h3>
<h3 id="abstract">Abstract</h3>
<p>Incorporating a customized object into image generation presents an
attractive feature in text-to-image (T2I) generation. Some methods
finetune T2I models for each object individually at test-time, which
tend to be overfitted and time-consuming. Others train an extra encoder
to extract object visual information for customization efficiently but
struggle to preserve the object's identity. To address these
limitations, we present CustomNet, a unified encoder-based object
customization framework that explicitly incorporates 3D novel view
synthesis capabilities into the customization process. This integration
facilitates the adjustment of spatial positions and viewpoints,
producing diverse outputs while effectively preserving the object's
identity. To train our model effectively, we propose a dataset
construction pipeline to better handle real-world objects and complex
backgrounds. Additionally, we introduce delicate designs that enable
location control and flexible background control through textual
descriptions or user-defined backgrounds. Our method allows for object
customization without the need of test-time optimization, providing
simultaneous control over viewpoints, location, and text. Experimental
results show that our method outperforms other customization methods
regarding identity preservation, diversity, and harmony. Codes are
available at https://github.com/TencentARC/CustomNet.</p>
</div>
<div class="note primary"><h2
id="eccv2024-gvgen-text-to-3d-generation-with-volumetric-representation"><a
target="_blank" rel="noopener" href="https://link.springer.com/chapter/10.1007/978-3-031-73242-3_26">ECCV2024:
GVGEN: Text-to-3D Generation with Volumetric Representation</a></h2>
<h3 id="author-何相龙">Author: 何相龙</h3>
<h3 id="abstract">Abstract</h3>
<p>In recent years, 3D Gaussian splatting has emerged as a powerful
technique for 3D reconstruction and generation, known for its fast and
high-quality rendering capabilities. Nevertheless, these methods often
come with limitations, either lacking the ability to produce diverse
samples or requiring prolonged inference times. To address these
shortcomings, this paper introduces a novel diffusion-based framework,
GVGEN, designed to efficiently generate 3D Gaussian representations from
text input. We propose two innovative techniques: (1) <em>Structured
Volumetric Representation</em>. We first arrange disorganized 3D
Gaussian points as a structured form GaussianVolume. This transformation
allows the capture of intricate texture details within a volume composed
of a fixed number of Gaussians. To better optimize the representation of
these details, we propose a unique pruning and densifying method named
the Candidate Pool Strategy, enhancing detail fidelity through selective
optimization. (2) <em>Coarse-to-fine Generation Pipeline</em>. To
simplify the generation of GaussianVolume and empower the model to
generate instances with detailed 3D geometry, we propose a
coarse-to-fine pipeline. It initially constructs a basic geometric
structure, followed by the prediction of complete Gaussian attributes.
Our framework, GVGEN, demonstrates superior performance in qualitative
and quantitative assessments compared to existing 3D generation methods.
Simultaneously, it maintains a fast generation speed (∼7 s), effectively
striking a balance between quality and efficiency. Our project page is
https://gvgen.github.io/.</p>
</div>
<div class="note primary"><h2
id="eccv2024-a-task-is-worth-one-word-learning-with-task-prompts-for-high-quality-versatile-image-inpainting"><a
target="_blank" rel="noopener" href="https://link.springer.com/chapter/10.1007/978-3-031-73636-0_12">ECCV2024:
A Task is Worth One Word: Learning with Task Prompts for High-Quality
Versatile Image Inpainting</a></h2>
<h3 id="author-庄俊豪">Author: 庄俊豪</h3>
<h3 id="abstract">Abstract</h3>
<p>Advancing image inpainting is challenging as it requires filling
user-specified regions for various intents, such as background filling
and object synthesis. Existing approaches focus on either context-aware
filling or object synthesis using text descriptions. However, achieving
both tasks simultaneously is challenging due to differing training
strategies. To overcome this challenge, we introduce
<strong>PowerPaint</strong>, the first high-quality and versatile
inpainting model that excels in multiple inpainting tasks. First, we
introduce learnable task prompts along with tailored fine-tuning
strategies to guide the model’s focus on different inpainting targets
explicitly. This enables PowerPaint to accomplish various inpainting
tasks by utilizing different task prompts, resulting in state-of-the-art
performance. Second, we demonstrate the versatility of the task prompt
in PowerPaint by showcasing its effectiveness as a negative prompt for
object removal. Moreover, we leverage prompt interpolation techniques to
enable controllable shape-guided object inpainting, enhancing the
model’s applicability in shape-guided applications. Finally, we conduct
extensive experiments and applications to verify the effectiveness of
PowerPaint. We release our codes and models on our project page:
https://powerpaint.github.io/.</p>
</div>
<div class="note primary"><h2
id="eccv2024-dreamdiffusion-high-quality-eeg-to-image-generation-with-temporal-masked-signal-modeling-and-clip-alignment"><a
target="_blank" rel="noopener" href="https://link.springer.com/chapter/10.1007/978-3-031-72751-1_27">ECCV2024:
DreamDiffusion: High-Quality EEG-to-Image Generation with Temporal
Masked Signal Modeling and CLIP Alignment</a></h2>
<h3 id="author-白云鹏">Author: 白云鹏</h3>
<h3 id="abstract">Abstract</h3>
<p>This paper introduces DreamDiffusion, a novel method for generating
high-quality images directly from brain electroencephalogram (EEG)
signals, without the need to translate thoughts into text.
DreamDiffusion leverages pre-trained text-to-image models and employs
temporal masked signal modeling to pre-train the EEG encoder for
effective and robust EEG representations. Additionally, the method
further leverages the CLIP image encoder to provide extra supervision to
better align EEG, text, and image embeddings with limited EEG-image
pairs. Overall, the proposed method overcomes the challenges of using
EEG signals for image generation, such as noise, limited information,
and individual differences, and achieves promising results. Quantitative
and qualitative results demonstrate the effectiveness of the proposed
method as a significant step towards portable and low-cost
“thoughts-to-image”, with potential applications in neuroscience and
computer vision.</p>
</div>
<div class="note primary"><h2
id="eccv2024-mirrorgaussian-reflecting-3d-gaussians-for-reconstructing-mirror-reflections"><a
target="_blank" rel="noopener" href="https://link.springer.com/chapter/10.1007/978-3-031-73220-1_22">ECCV2024:
MirrorGaussian: Reflecting 3D Gaussians for Reconstructing Mirror
Reflections</a></h2>
<h3 id="author-刘佳月">Author: 刘佳月</h3>
<h3 id="abstract">Abstract</h3>
<p>3D Gaussian Splatting showcases notable advancements in
photo-realistic and real-time novel view synthesis. However, it faces
challenges in modeling mirror reflections, which exhibit substantial
appearance variations from different viewpoints. To tackle this problem,
we present MirrorGaussian, the first method for mirror scene
reconstruction with real-time rendering based on 3D Gaussian Splatting.
The key insight is grounded on the mirror symmetry between the
real-world space and the virtual mirror space. We introduce an intuitive
dual-rendering strategy that enables differentiable rasterization of
both the real-world 3D Gaussians and the mirrored counterpart obtained
by reflecting the former about the mirror plane. All 3D Gaussians are
jointly optimized with the mirror plane in an end-to-end framework.
MirrorGaussian achieves high-quality and real-time rendering in scenes
with mirrors, empowering scene editing like adding new mirrors and
objects. Comprehensive experiments on multiple datasets demonstrate that
our approach significantly outperforms existing methods, achieving
state-of-the-art results. Project page:
https://mirror-gaussian.github.io/.</p>
</div>
<div class="note primary"><h2
id="ieee-tcsvt-meta-learning-without-data-via-unconditional-diffusion-models"><a
target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/10587268">IEEE
TCSVT: Meta-Learning without Data via Unconditional Diffusion
Models</a></h2>
<h3 id="author-韦永贤">Author: 韦永贤</h3>
<h3 id="abstract">Abstract</h3>
<p>Although few-shot learning aims to address data scarcity, it still
requires large, annotated datasets for training, which are often
unavailable due to cost and privacy concerns. Previous studies have
utilized pre-trained diffusion models, either to synthesize auxiliary
data besides limited labeled samples, or to employ diffusion models as
zero-shot classifiers. However, they are limited to conditional
diffusion models needing class prior information (e.g., carefully
crafted text prompts) about unseen tasks. To overcome this, we leverage
unconditional diffusion models without needs for class information to
train a meta-model capable of generalizing to unseen tasks. The
framework contains (1) a meta-learning without data approach that uses
synthetic data during training; and (2) a diffusion model-based data
augmentation to calibrate the distribution shift during testing. During
meta-training, we implement a self-taught class-learner to gradually
capture class concepts, guiding unconditional diffusion models to
generate a labeled pseudo dataset. This pseudo dataset is then used to
jointly train the class-learner and the meta-model, allowing for
iterative refinement and clear differentiation between classes. During
meta-testing, we introduce a data augmentation that employs the
diffusion models used in meta-training, to narrow the gap between
meta-training and meta-testing task distribution. This enables the
meta-model trained on synthetic images to effectively classify real
images in unseen tasks. Comprehensive experiments showcase the
superiority and adaptability of our approach in four real-world
scenarios. Code available at https://github.com/WalkerWorldPeace/MLWDUDM
.</p>
</div>
<div class="note primary"><h2
id="icml2024-crossget-cross-guided-ensemble-of-tokens-for-accelerating-vision-language-transformers"><a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.17455">ICML2024: CrossGET: Cross-Guided
Ensemble of Tokens for Accelerating Vision-Language
Transformers</a></h2>
<h3 id="author-石大川">Author: 石大川</h3>
<h3 id="abstract">Abstract</h3>
<p>Recent vision-language models have achieved tremendous advances.
However, their computational costs are also escalating dramatically,
making model acceleration exceedingly critical. To pursue more efficient
vision-language Transformers, this paper introduces Cross-Guided
Ensemble of Tokens (CrossGET), a general acceleration framework for
vision-language Transformers. This framework adaptively combines tokens
in real-time during inference, significantly reducing computational
costs while maintaining high performance. CrossGET features two primary
innovations: 1) Cross-Guided Matching and Ensemble. CrossGET leverages
cross-modal guided token matching and ensemble to effectively utilize
cross-modal information, achieving wider applicability across both
modality-independent models, e.g., CLIP, and modality-dependent ones,
e.g., BLIP2. 2) Complete-Graph Soft Matching. CrossGET introduces an
algorithm for the token-matching mechanism, ensuring reliable matching
results while facilitating parallelizability and high efficiency.
Extensive experiments have been conducted on various vision-language
tasks, such as image-text retrieval, visual reasoning, image captioning,
and visual question answering. The performance on both classic
multimodal architectures and emerging multimodal LLMs demonstrates the
framework's effectiveness and versatility. The code is available at
https://github.com/sdc17/CrossGET .</p>
</div>
<div class="note primary"><h2
id="icml2024-sparse-model-inversion-efficient-inversion-of-vision-transformers-with-less-hallucination"><a
target="_blank" rel="noopener" href="https://openreview.net/forum?id=T0lFfO8HaK">ICML2024: Sparse Model
Inversion: Efficient Inversion of Vision Transformers with Less
Hallucination</a></h2>
<h3 id="author-胡梓轩">Author: 胡梓轩</h3>
<h3 id="abstract">Abstract</h3>
<p>Model inversion, which aims to reconstruct the original training data
from pre-trained discriminative models, is especially useful when the
original training data is unavailable due to privacy, usage rights, or
size constraints. However, existing dense inversion methods attempt to
reconstruct the entire image area, making them extremely inefficient
when inverting high-resolution images from large-scale Vision
Transformers (ViTs). We further identify two underlying causes of this
inefficiency: the redundant inversion of noisy backgrounds and the
unintended inversion of spurious correlations—a phenomenon we term
``hallucination'' in model inversion. To address these limitations, we
propose a novel sparse model inversion strategy, as a plug-and-play
extension to speed up existing dense inversion methods with no need for
modifying their original loss functions. Specifically, we selectively
invert semantic foregrounds while stopping the inversion of noisy
backgrounds and potential spurious correlations. Through both
theoretical and empirical studies, we validate the efficacy of our
approach in achieving significant inversion acceleration (up to ×3.79)
while maintaining comparable or even enhanced downstream performance in
data-free model quantization and data-free knowledge transfer. Code is
available at https://github.com/Egg-Hu/SMI.</p>
</div>
<div class="note primary"><h2
id="icml2024-task-groupings-regularization-data-free-meta-learning-with-heterogeneous-pre-trained-models"><a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.16560">ICML2024: Task Groupings
Regularization: Data-Free Meta-Learning with Heterogeneous Pre-trained
Models</a></h2>
<h3 id="author-韦永贤">Author: 韦永贤</h3>
<h3 id="abstract">Abstract</h3>
<p>Data-Free Meta-Learning (DFML) aims to derive knowledge from a
collection of pre-trained models without accessing their original data,
enabling the rapid adaptation to new unseen tasks. Current methods often
overlook the heterogeneity among pre-trained models, which leads to
performance degradation due to task conflicts. In this paper, we
empirically and theoretically identify and analyze the model
heterogeneity in DFML. We find that model heterogeneity introduces a
heterogeneity-homogeneity trade-off, where homogeneous models reduce
task conflicts but also increase the overfitting risk. Balancing this
trade-off is crucial for learning shared representations across tasks.
Based on our findings, we propose Task Groupings Regularization that
benefits from model heterogeneity by grouping and aligning conflicting
tasks. Specifically, we embed pre-trained models into a task space to
compute dissimilarity, and group heterogeneous models together based on
this measure. Then, we introduce implicit gradient regularization within
each group to mitigate potential conflicts. By encouraging a gradient
direction suitable for all tasks, the meta-model captures shared
representations that generalize across tasks. Comprehensive experiments
showcase the superiority of our approach in multiple benchmarks,
effectively tackling the model heterogeneity in challenging multi-domain
and multi-architecture scenarios.</p>
</div>
<div class="note primary"><h2
id="icml2024-dfd-distillng-the-feature-disparity-differently-for-detectors"><a
target="_blank" rel="noopener" href="https://openreview.net/forum?id=KI3JKFKciG">ICML2024: DFD:
Distillng the Feature Disparity Differently for Detectors</a></h2>
<h3 id="author-刘康">Author: 刘康</h3>
<h3 id="abstract">Abstract</h3>
<p>Knowledge distillation is a widely adopted model compression
technique that has been successfully applied to object detection. In
feature distillation, it is common practice for the student model to
imitate the feature responses of the teacher model, with the underlying
objective of improving its own abilities by reducing the disparity with
the teacher. However, it is crucial to recognize that the disparities
between the student and teacher are inconsistent, highlighting their
varying abilities. In this paper, we explore the inconsistency in the
disparity between teacher and student feature maps and analyze their
impact on the efficiency of the distillation. We find that regions with
varying degrees of difference should be treated separately, with
different distillation constraints applied accordingly. We introduce our
distillation method called Disparity Feature Distillation(DFD). The core
idea behind DFD is to apply different treatments to regions with varying
learning difficulties, simultaneously incorporating leniency and
strictness. It enables the student to better assimilate the teacher’s
knowledge. Through extensive experiments, we demonstrate the
effectiveness of our proposed DFD in achieving significant improvements.
For instance, when applied to detectors based on ResNet50 such as
RetinaNet, FasterRCNN, and RepPoints, our method enhances their mAP from
37.4%, 38.4%, 38.6% to 41.7%, 42.4%, 42.7%, respectively. Our approach
also demonstrates substantial improvements on YOLO and ViT-based models.
The code is available at https://github.com/luckin99/DFD.</p>
</div>
<div class="note primary"><h2
id="ijcnn2024-integrating-local-global-features-for-estimating-shortest-path-distance-in-large-scale-graphs"><a
target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/10650344">IJCNN2024:
Integrating Local &amp; Global Features for Estimating Shortest-Path
Distance in Large-Scale Graphs</a></h2>
<h3 id="author-王浩宇">Author: 王浩宇</h3>
<h3 id="abstract">Abstract</h3>
<p>We propose an effective hybrid approach jointly leveraging local and
global features for shortest-path (SP) distance estimation in
domain-agnostic large-scale graphs. Previous works struggle to make
estimations either from node-wise local embeddings or by compressing a
global SP distance matrix, causing insufficient learning at some
distance and loss of accuracy. Unlike them, we find a way to better
preserve local distance on node embeddings, and then integrate them with
a global process for accurate estimation at every distance. First, we
propose a distance-consistent embedding method that better preserves the
distance between each node and its local neighbors due to resampling
node occurrence on random walks. Second, we train a feed-forward network
with boosting techniques (FFN-BT) to estimate SP distance from these
embeddings plus existing global features. Experimental results show that
our approach averagely yields 10% improved accuracy and 20% reduced time
when compared to existing methods on a broad class of graphs.</p>
</div>
<div class="note primary"><h2
id="ijcnn2024-mmr-multi-scale-motion-retargeting-between-skeleton-agnostic-characters"><a
target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/10650339">IJCNN2024:
MMR: Multi-Scale Motion Retargeting Between Skeleton-Agnostic
Characters</a></h2>
<h3 id="author-王浩宇">Author: 王浩宇</h3>
<h3 id="abstract">Abstract</h3>
<p>We present a simple yet effective method for skeleton-agnostic motion
retargeting. Previous methods transfer motion between high-resolution
meshes, failing to preserve the inherent local-part motions in the mesh.
Addressing this issue, our proposed method learns the correspondence in
a coarse-to-fine fashion by disentangling the retargeting process within
multi-scale meshes. First, we propose a mesh-pooling module that pools
the mesh representations for better motion transfer. This module
improves the ability to handle small-part motion and preserves the local
motion interdependence between neighboring mesh vertices. Furthermore,
we leverage a multi-scale refinement procedure to complement missing
mesh details by gradually refining the low-resolution mesh output with a
higher-resolution one. We evaluate our method on several well-known 3D
character datasets, and it yields an average improvement of 25% on
point-wise mesh Euclidean distance (PMD) against the start-of-art
method. Qualitative results show that our method is significantly
helpful in preserving the moving consistency of different body parts on
the target character due to disentangling body-part structures and mesh
details in a multi-scale way.</p>
</div>
<div class="note primary"><h2
id="ijcnn2024-error-bound-based-noise-schedule-design-in-diffusion-models"><a
target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/10650926">IJCNN2024:
Error Bound Based Noise Schedule Design in Diffusion Models</a></h2>
<h3 id="author-刘力源">Author: 刘力源</h3>
<h3 id="abstract">Abstract</h3>
<p>Diffusion-based generative model currently serves as a mainstream
generative method. The noise schedule has a significant impact on the
training process of diffusion model, as it affects both the distribution
of the noisy training set and the weights of the objective function at
each noise level. In this paper, we design the noise schedule from the
scope of reducing the final error upper bound of the reverse denoising
process. By examining Monte Carlo training from a theoretical
perspective, we establish an association between noise schedule and the
upper bound of network output error. Furthermore, we derive the
connection between network output and final error through reverse
process. We design our noise schedule with the goal of reducing the
upper bound of error combined with the correlation analysis of network
output. Experimental results demonstrate that our noise schedule
enhances perceptual quality on CIFAR-10, FFHQ-64x64 and AFHQv2-64x64.
Our noise schedule achieves state-of-the-art FID score of 1.70 on
CIFAR-10 unconditional generation task using discriminator guidance
method. On FFHQ/AFHQv2, using our noise schedule to retrain the
pre-trained model can improve the sample quality at little training
cost.</p>
</div>
<div class="note primary"><h2 id="ijcnn2024-noise-weighting-phased-prompt-image-editing"><a
target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/10651075">IJCNN2024:
Noise Weighting Phased Prompt Image Editing</a></h2>
<h3 id="author-徐国炜">Author: 徐国炜</h3>
<h3 id="abstract">Abstract</h3>
<p>The remarkable performance of large-scale Text-to-Image
generation(TI) models is evident in their ability to produce
high-quality and diverse images. However, despite advancements, the
field of image editing still faces challenges. Current methods struggle
to strike a balance between fidelity and powerful editing capabilities.
Moreover, approaches that do not involve fine-tuning fail to produce
diverse editing results. We introduce Noise Weighting Phased Prompt
Image Editing (NWPP), a method that excels in powerful editing, high
fidelity, and diverse results without fine-tuning. Our approach involves
a two-phase generation process. The first phase employs the original
prompt to guide initial image editing, ensuring a layout resembling the
original image. In the second phase, a noise-weighting technique based
on the Cross-Attention map minimizes the impact of the target text on
non-editing regions. Further enhancement is achieved through the
integration of the KV injection module, expanding the editing
capabilities and enabling diverse result generation. Experimental
evaluations, conducted on both generated images and the COCO dataset,
affirm the efficacy of our method.</p>
</div>
<div class="note primary"><h2
id="cvpr2024-distilling-semantic-priors-from-sam-to-efficient-image-restoration-models"><a
target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_Distilling_Semantic_Priors_from_SAM_to_Efficient_Image_Restoration_Models_CVPR_2024_paper.html">CVPR2024:
Distilling Semantic Priors from SAM to Efficient Image Restoration
Models</a></h2>
<h3 id="author-张权">Author: 张权</h3>
<h3 id="abstract">Abstract</h3>
<p>In image restoration (IR) leveraging semantic priors from
segmentation models has been a common approach to improve performance.
The recent segment anything model (SAM) has emerged as a powerful tool
for extracting advanced semantic priors to enhance IR tasks. However the
computational cost of SAM is prohibitive for IR compared to existing
smaller IR models. The incorporation of SAM for extracting semantic
priors considerably hampers the model inference efficiency. To address
this issue we propose a general framework to distill SAM's semantic
knowledge to boost exiting IR models without interfering with their
inference process. Specifically our proposed framework consists of the
semantic priors fusion (SPF) scheme and the semantic priors distillation
(SPD) scheme. SPF fuses two kinds of information between the restored
image predicted by the original IR model and the semantic mask predicted
by SAM for the refined restored image. SPD leverages a self-distillation
manner to distill the fused semantic priors to boost the performance of
original IR models. Additionallywe design a semantic-guided relation
(SGR) module for SPD which ensures semantic feature representation space
consistency to fully distill the priors. We demonstrate the
effectiveness of our framework across multiple IR models and tasks
including deraining deblurring and denoising.</p>
</div>
<div class="note primary"><h2
id="cvpr2024-cricavpr-cross-image-correlation-aware-representation-learning-for-visual-place-recognition"><a
target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2024/html/Lu_CricaVPR_Cross-image_Correlation-aware_Representation_Learning_for_Visual_Place_Recognition_CVPR_2024_paper.html">CVPR2024:
CricaVPR: Cross-image Correlation-aware Representation Learning for
Visual Place Recognition</a></h2>
<h3 id="author-卢锋">Author: 卢锋</h3>
<h3 id="abstract">Abstract</h3>
<p>Over the past decade most methods in visual place recognition (VPR)
have used neural networks to produce feature representations. These
networks typically produce a global representation of a place image
using only this image itself and neglect the cross-image variations
(e.g. viewpoint and illumination) which limits their robustness in
challenging scenes. In this paper we propose a robust global
representation method with cross-image correlation awareness for VPR
named CricaVPR. Our method uses the attention mechanism to correlate
multiple images within a batch. These images can be taken in the same
place with different conditions or viewpoints or even captured from
different places. Therefore our method can utilize the cross-image
variations as a cue to guide the representation learning which ensures
more robust features are produced. To further facilitate the robustness
we propose a multi-scale convolution-enhanced adaptation method to adapt
pre-trained visual foundation models to the VPR task which introduces
the multi-scale local information to further enhance the cross-image
correlation-aware representation. Experimental results show that our
method outperforms state-of-the-art methods by a large margin with
significantly less training time. The code is released at
https://github.com/Lu-Feng/CricaVPR.</p>
</div>
<div class="note primary"><h2
id="cvpr2024-a-free-lunch-for-faster-and-better-data-free-meta-learning"><a
target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2024/html/Wei_FREE_Faster_and_Better_Data-Free_Meta-Learning_CVPR_2024_paper.html">CVPR2024:
A Free Lunch for Faster and Better Data-Free Meta-Learning</a></h2>
<h3 id="author-韦永贤">Author: 韦永贤</h3>
<h3 id="abstract">Abstract</h3>
<p>Data-Free Meta-Learning (DFML) aims to extract knowledge from a
collection of pre-trained models without requiring the original data
presenting practical benefits in contexts constrained by data privacy
concerns. Current DFML methods primarily focus on the data recovery from
these pre-trained models. However they suffer from slow recovery speed
and overlook gaps inherent in heterogeneous pre-trained models. In
response to these challenges we introduce the Faster and Better
Data-Free Meta-Learning (FREE) framework which contains: (i) a
meta-generator for rapidly recovering training tasks from pre-trained
models; and (ii) a meta-learner for generalizing to new unseen tasks.
Specifically within the module Faster Inversion via Meta-Generator each
pre-trained model is perceived as a distinct task. The meta-generator
can rapidly adapt to a specific task in just five steps significantly
accelerating the data recovery. Furthermore we propose Better
Generalization via Meta-Learner and introduce an implicit gradient
alignment algorithm to optimize the meta-learner. This is achieved as
aligned gradient directions alleviate potential conflicts among tasks
from heterogeneous pre-trained models. Empirical experiments on multiple
benchmarks affirm the superiority of our approach marking a notable
speed-up (20x) and performance enhancement (1.42% 4.78%) in comparison
to the state-of-the-art.</p>
</div>
<div class="note primary"><h2
id="ieee-t-mm-negative-sensitive-framework-with-semantic-enhancement-for-composed-image-retrieval"><a
target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/10493853">IEEE T-MM:
Negative-Sensitive Framework with Semantic Enhancement for Composed
Image Retrieval</a></h2>
<h3 id="author-王依凡">Author: 王依凡</h3>
<h3 id="abstract">Abstract</h3>
<p>Composed image retrieval is a challenging task in the field of
multi-modal learning, aiming at measuring the similarities between
target images and query images with modification sentences. Most
previous methods either construct feature composition for the query
image and modification text or concentrate on extracting cross-modal
alignments. However, these methods are prone to neglect the negative
impacts of the mismatched correspondences between the hybrid-modal query
and target, which could be discriminative when comparing similar
instances. Besides, localized textual representations are not fully
explored when learning similarities between the query and the target. To
overcome the above issues, we propose a Negative-Sensitive Framework
with Semantic Enhancement (NSFSE) for mining the adaptive boundaries
between matched and mismatched samples with comprehensive consideration
of positive and negative correspondences. It can optimize the threshold
dynamically based on distributions to explore the intrinsic
characteristics of positive and negative correlations, which could
further facilitate accurate similarity learning. A text-guided attention
mechanism after infusing cross-modal affinities on localized word
features is exploited in NSFSE to explore latent semantic-related visual
similarity and cross-modal similarity simultaneously. The performance of
extensive experiments and comprehensive analysis on three representative
datasets CIRR, FashionIQ, and Fashion200 K demonstrate the effectiveness
of negative mining of similarity with semantic enhancement in the
proposed NSFSE.</p>
</div>
<div class="note primary"><h2
id="iclr2024-convolution-meets-lora-parameter-efficient-finetuning-for-segment-anything-model"><a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2401.17868">ICLR2024: Convolution Meets
LoRA: Parameter Efficient Finetuning for Segment Anything Model</a></h2>
<h3 id="author-钟子涵">Author: 钟子涵</h3>
<h3 id="abstract">Abstract</h3>
<p>The Segment Anything Model (SAM) stands as a foundational framework
for image segmentation. While it exhibits remarkable zero-shot
generalization in typical scenarios, its advantage diminishes when
applied to specialized domains like medical imagery and remote sensing.
To address this limitation, this paper introduces Conv-LoRA, a simple
yet effective parameter-efficient fine-tuning approach. By integrating
ultra-lightweight convolutional parameters into Low-Rank Adaptation
(LoRA), Conv-LoRA can inject image-related inductive biases into the
plain ViT encoder, further reinforcing SAM's local prior assumption.
Notably, Conv-LoRA not only preserves SAM's extensive segmentation
knowledge but also revives its capacity of learning high-level image
semantics, which is constrained by SAM's foreground-background
segmentation pretraining. Comprehensive experimentation across diverse
benchmarks spanning multiple domains underscores Conv-LoRA's superiority
in adapting SAM to real-world semantic segmentation tasks.</p>
</div>
<div class="note primary"><h2
id="iclr2024-towards-seamless-adaptation-of-pre-trained-models-for-visual-place-recognition"><a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2402.14505">ICLR2024: Towards Seamless
Adaptation of Pre-trained Models for Visual Place Recognition</a></h2>
<h3 id="author-卢锋">Author: 卢锋</h3>
<h3 id="abstract">Abstract</h3>
<p>Recent studies show that vision models pre-trained in generic visual
learning tasks with large-scale data can provide useful feature
representations for a wide range of visual perception problems. However,
few attempts have been made to exploit pre-trained foundation models in
visual place recognition (VPR). Due to the inherent difference in
training objectives and data between the tasks of model pre-training and
VPR, how to bridge the gap and fully unleash the capability of
pre-trained models for VPR is still a key issue to address. To this end,
we propose a novel method to realize seamless adaptation of pre-trained
models for VPR. Specifically, to obtain both global and local features
that focus on salient landmarks for discriminating places, we design a
hybrid adaptation method to achieve both global and local adaptation
efficiently, in which only lightweight adapters are tuned without
adjusting the pre-trained model. Besides, to guide effective adaptation,
we propose a mutual nearest neighbor local feature loss, which ensures
proper dense local features are produced for local matching and avoids
time-consuming spatial verification in re-ranking. Experimental results
show that our method outperforms the state-of-the-art methods with less
training data and training time, and uses about only 3% retrieval
runtime of the two-stage VPR methods with RANSAC-based spatial
verification. It ranks 1st on the MSLS challenge leaderboard (at the
time of submission). The code is released at
https://github.com/Lu-Feng/SelaVPR.</p>
</div>
<div class="note primary"><h2
id="aaai2024-efficient-conditional-diffusion-model-with-probability-flow-sampling-for-image-super-resolution"><a
target="_blank" rel="noopener" href="https://ojs.aaai.org/index.php/AAAI/article/view/28511">AAAI2024:
Efficient Conditional Diffusion Model with Probability Flow Sampling for
Image Super-resolution</a></h2>
<h3 id="author-袁宇韬">Author: 袁宇韬</h3>
<h3 id="abstract">Abstract</h3>
<p>Image super-resolution is a fundamentally ill-posed problem because
multiple valid high-resolution images exist for one low-resolution
image. Super-resolution methods based on diffusion probabilistic models
can deal with the ill-posed nature by learning the distribution of
high-resolution images conditioned on low-resolution images, avoiding
the problem of blurry images in PSNR-oriented methods. However, existing
diffusion-based super-resolution methods have high time consumption with
the use of iterative sampling, while the quality and consistency of
generated images are less than ideal due to problems like color
shifting. In this paper, we propose Efficient Conditional Diffusion
Model with Probability Flow Sampling (ECDP) for image super-resolution.
To reduce the time consumption, we design a continuous-time conditional
diffusion model for image super-resolution, which enables the use of
probability flow sampling for efficient generation. Additionally, to
improve the consistency of generated images, we propose a hybrid
parametrization for the denoiser network, which interpolates between the
data-predicting parametrization and the noise-predicting parametrization
for different noise scales. Moreover, we design an image quality loss as
a complement to the score matching loss of diffusion models, further
improving the consistency and quality of super-resolution. Extensive
experiments on DIV2K, ImageNet, and CelebA demonstrate that our method
achieves higher super-resolution quality than existing diffusion-based
image super-resolution methods while having lower time consumption. Our
code is available at https://github.com/Yuan-Yutao/ECDP.</p>
</div>
<div class="note primary"><h2
id="aaai2024-blind-face-restoration-under-extreme-conditions-leveraging-3d-2d-prior-fusion-for-superior-structural-and-texture-recovery"><a
target="_blank" rel="noopener" href="https://ojs.aaai.org/index.php/AAAI/article/view/27889">AAAI2024:
Blind Face Restoration under Extreme Conditions: Leveraging 3D-2D Prior
Fusion for Superior Structural and Texture Recovery</a></h2>
<h3 id="author-袁梓洋">Author: 袁梓洋</h3>
<h3 id="abstract">Abstract</h3>
<p>Blind face restoration under extreme conditions involves
reconstructing high-quality face images from severely degraded inputs.
These input images are often in poor quality and have extreme facial
poses, leading to errors in facial structure and unnatural artifacts
within the restored images. In this paper, we show that utilizing 3D
priors effectively compensates for structure knowledge deficiencies in
2D priors while preserving the texture details. Based on this, we
introduce FREx (Face Restoration under Extreme conditions) that combines
structure-accurate 3D priors and texture-rich 2D priors in pretrained
generative networks for blind face restoration under extreme conditions.
To fuse the different information in 3D and 2D priors, we introduce an
adaptive weight module that adjusts the importance of features based on
the input image's condition. With this approach, our model can restore
structure-accurate and natural-looking faces even when the images have
lost a lot of information due to degradation and extreme pose. Extensive
experimental results on synthetic and real-world datasets validate the
effectiveness of our methods.</p>
</div>
<div class="note primary"><h2
id="aaai2024-mean-teacher-detr-with-masked-feature-alignment-a-robust-domain-adaptive-detection-transformer-framework"><a
target="_blank" rel="noopener" href="https://ojs.aaai.org/index.php/AAAI/article/view/28405">AAAI2024:
Mean Teacher DETR with Masked Feature Alignment: A Robust Domain
Adaptive Detection Transformer Framework</a></h2>
<h3 id="author-翁玮熙">Author: 翁玮熙</h3>
<h3 id="abstract">Abstract</h3>
<p>Unsupervised domain adaptation object detection(UDAOD) research on
Detection Transformer(DETR) mainly focuses on feature alignment and
existing methods can be divided into two kinds, each of which has its
unresolved issues. One-stage feature alignment methods can easily lead
to performance fluctuation and training stagnation. Two-stage feature
alignment method based on mean teacher comprises a pretraining stage
followed by a self-training stage, each facing problems in obtaining
reliable pretrained model and achieving consistent performance gains.
Methods mentioned above have not yet explore how to utilize the third
related domain such as target-like domain to assist adaptation. To
address these issues, we propose a two-stage framework named MTM, i.e.
Mean Teacher-DETR with Masked Feature Alignment. In the pretraining
stage, we utilize labeled target-like images produced by image style
transfer to avoid performance fluctuation. In the self-training stage,
we leverage unlabeled target images by pseudo labels based on mean
teacher and propose a module called Object Queries Knowledge
Transfer(OQKT) to ensure consistent performance gains of the student
model. Most importantly, we propose masked feature alignment methods
including Masked Domain Query-based Feature Alignment(MDQFA) and Masked
Token-wise Feature Alignment(MTWFA) to alleviate domain shift in a more
robust way, which not only prevent training stagnation and lead to a
robust pretrained model in the pretraining stage, but also enhance the
model's target performance in the self-training stage. Experiments on
three challenging scenarios and a theoretical analysis verify the
effectiveness of MTM.</p>
</div>
<div class="note primary"><h2
id="aaai2024-deep-homography-estimation-for-visual-place-recognition"><a
target="_blank" rel="noopener" href="https://ojs.aaai.org/index.php/AAAI/article/view/28901">AAAI2024:
Deep Homography Estimation for Visual Place Recognition</a></h2>
<h3 id="author-卢锋">Author: 卢锋</h3>
<h3 id="abstract">Abstract</h3>
<p>Visual place recognition (VPR) is a fundamental task for many
applications such as robot localization and augmented reality. Recently,
the hierarchical VPR methods have received considerable attention due to
the trade-off between accuracy and efficiency. They usually first use
global features to retrieve the candidate images, then verify the
spatial consistency of matched local features for re-ranking. However,
the latter typically relies on the RANSAC algorithm for fitting
homography, which is time-consuming and non-differentiable. This makes
existing methods compromise to train the network only in global feature
extraction. Here, we propose a transformer-based deep homography
estimation (DHE) network that takes the dense feature map extracted by a
backbone network as input and fits homography for fast and learnable
geometric verification. Moreover, we design a re-projection error of
inliers loss to train the DHE network without additional homography
labels, which can also be jointly trained with the backbone network to
help it extract the features that are more suitable for local matching.
Extensive experiments on benchmark datasets show that our method can
outperform several state-of-the-art methods. And it is more than one
order of magnitude faster than the mainstream hierarchical VPR methods
using RANSAC. The code is released at
https://github.com/Lu-Feng/DHE-VPR.</p>
</div>
<div class="note primary"><h2
id="ieee-t-mm-towards-effective-collaborative-learning-in-long-tailed-recognition"><a
target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/10250973">IEEE T-MM:
Towards Effective Collaborative Learning in Long-Tailed
Recognition</a></h2>
<h3 id="author-许正卓">Author: 许正卓</h3>
<h3 id="abstract">Abstract</h3>
<p>Real-world data usually suffers from severe class imbalance and
long-tailed distributions, where minority classes are significantly
underrepresented compared to the majority ones. Recent research prefers
to utilize multi-expert architectures to mitigate the model uncertainty
on the minority, where collaborative learning is employed to aggregate
the knowledge of experts, i.e., online distillation. In this article, we
observe that the knowledge transfer between experts is imbalanced in
terms of class distribution, which results in limited performance
improvement of the minority classes. To address it, we propose a
re-weighted distillation loss by comparing two classifiers' predictions,
which are supervised by online distillation and label annotations,
respectively. We also emphasize that feature-level distillation will
significantly improve model performance and increase feature robustness.
Finally, we propose an Effective Collaborative Learning (ECL) framework
that integrates a contrastive proxy task branch to further improve
feature quality. Quantitative and qualitative experiments on four
standard datasets demonstrate that ECL achieves state-of-the-art
performance and the detailed ablation studies manifest the effectiveness
of each component in ECL.</p>
</div>
<div class="note primary"><h2
id="acl2023-let-leveraging-error-type-information-for-grammatical-error-correction"><a
target="_blank" rel="noopener" href="https://aclanthology.org/2023.findings-acl.371/">ACL2023: LET:
Leveraging Error Type Information for Grammatical Error
Correction</a></h2>
<h3 id="author-李泓嘉">Author: 李泓嘉</h3>
<h3 id="abstract">Abstract</h3>
<p>Grammatical error correction (GEC) aims to correct errors in given
sentences and is significant to many downstream natural language
understanding tasks. Recent work introduces the idea of grammatical
error detection (GED) to improve the GEC task performance. In contrast,
these explicit multi-stage works propagate and amplify the problem of
misclassification of the GED module. To introduce more convincing error
type information, we propose an end-to-end framework in this paper,
which Leverages Error Type (LET) information in the generation process.
First, the input text is fed into a classification module to obtain the
error type corresponding to each token. Then, we introduce the category
information into the decoder’s input and cross-attention module in two
ways, respectively. Experiments on various datasets show that our
proposed method outperforms existing methods by a clear margin.</p>
</div>
<div class="note primary"><h2
id="acl2023-tailoring-instructions-to-students-learning-levels-boosts-knowledge-distillation"><a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.09651">ACL2023: Tailoring Instructions
to Student's Learning Levels Boosts Knowledge Distillation</a></h2>
<h3 id="author-任昱鑫">Author: 任昱鑫</h3>
<h3 id="abstract">Abstract</h3>
<p>It has been commonly observed that a teacher model with superior
performance does not necessarily result in a stronger student,
highlighting a discrepancy between current teacher training practices
and effective knowledge transfer. In order to enhance the guidance of
the teacher training process, we introduce the concept of distillation
influence to determine the impact of distillation from each training
sample on the student's generalization ability. In this paper, we
propose Learning Good Teacher Matters (LGTM), an efficient training
technique for incorporating distillation influence into the teacher's
learning process. By prioritizing samples that are likely to enhance the
student's generalization ability, our LGTM outperforms 10 common
knowledge distillation baselines on 6 text classification tasks in the
GLUE benchmark.</p>
</div>
<div class="note primary"><h2
id="ieee-t-mm-hhf-hashing-guided-hinge-function-for-deep-hashing-retrieval"><a
target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9953581">IEEE T-MM:
HHF: Hashing-guided Hinge Function for Deep Hashing Retrieval</a></h2>
<h3 id="author-徐呈寅">Author: 徐呈寅</h3>
<h3 id="abstract">Abstract</h3>
<p>Deep hashing has shown promising performance in large-scale image
retrieval. The hashing process utilizes Deep Neural Networks (DNNs) to
embed images into compact continuous latent codes, then map them into
binary codes by hashing function for efficient retrieval. Recent
approaches perform metric loss and quantization loss to supervise the
two procedures that cluster samples with the same categories and
alleviate semantic information loss after binarization in the end-to-end
training framework. However, we observe the incompatible conflict that
the optimal cluster positions are not identical to the ideal hash
positions because of the different objectives of the two loss terms,
which lead to severe ambiguity and error-hashing after the binarization
process. To address the problem, we borrow the Theory of
Minimum-Distance Bounds for Binary Linear Codes to design the inflection
point that depends on the hash bit length and category numbers and
thereby propose Hashing-guided Hinge Function (HHF) to explicitly
enforce the termination of metric loss to prevent the negative pairs
unlimited alienated. Such modification is proven effective and essential
for training, which contributes to proper intra- and inter-distances for
clusters and better hash positions for accurate image retrieval
simultaneously. Extensive experiments in CIFAR-10, CIFAR-100, ImageNet,
and MS-COCO justify that HHF consistently outperforms existing
techniques and is robust and flexible to transplant into other methods.
Code is available at https://github.com/JerryXu0129/HHF .</p>
</div>
<div class="note primary"><h2
id="acm-mm2023-enhanced-image-deblurring-an-efficient-frequency-exploitation-and-preservation-network"><a
target="_blank" rel="noopener" href="https://dl.acm.org/doi/abs/10.1145/3581783.3611976">ACM MM2023:
Enhanced Image Deblurring: An Efficient Frequency Exploitation and
Preservation Network</a></h2>
<h3 id="author-董姝婷">Author: 董姝婷</h3>
<h3 id="abstract">Abstract</h3>
<p>Most of these frequency-based deblurring methods mainly have two
major limitations: (1) insufficient exploitation of frequency
information, (2) inadequate preservation of frequency information. In
this paper, we propose a novel Efficient Frequency Exploitation and
Preservation Network (EFEP) to address these limitations. Firstly, we
propose a novel Frequency-Balanced Exploitation Encoder (FBE-Encoder) to
sufficiently exploit frequency information. We insert a novel
Frequency-Balanced Navigator (FBN) module in the encoder, which
establishes a dynamic balance that adaptively explores and integrates
the correlations between frequency features and other features presented
in the network. And it also can highlight the most important regions in
frequency features. Secondly, considering the limitation that frequency
information is inevitably lost in deep network architectures, we present
an Enhanced Selective Frequency Decoder (ESF-Decoder) that not only
effectively reduces spatial information redundancy, but also fully
explores the different importance of various frequency information to
ensure the supplement of valid spatial information and weaken the
invalid information. Thirdly, each encoder/decoder block of the EFEP
consists of multiple Contrastive Residual Blocks (CRBs), which are
designed to explicitly compute and incorporate feature distinctions.
Powered by the above designs, our EFEP outperforms state-of-the-art
models on both quantitative and qualitative evaluations.</p>
</div>
<div class="note primary"><h2
id="ijcai2023-dfvsr-directional-frequency-video-super-resolution-via-asymmetric-and-enhancement-alignment-network"><a
target="_blank" rel="noopener" href="https://www.ijcai.org/proceedings/2023/0076.pdf">IJCAI2023: DFVSR:
Directional Frequency Video Super-Resolution via Asymmetric and
Enhancement Alignment Network</a></h2>
<h3 id="author-董姝婷">Author: 董姝婷</h3>
<h3 id="abstract">Abstract</h3>
<p>Recently, techniques utilizing frequency-based methods have gained
signifcant attention, as they exhibit exceptional restoration
capabilities for detail and structure in video super-resolution tasks.
However, most of these frequency-based methods mainly have three major
limitations: 1) insuffcient exploration of object motion information, 2)
inadequate enhancement for high-fdelity regions, and 3) loss of spatial
information during convolution. In this paper, we propose a novel
network, Directional Frequency Video Super-Resolution (DFVSR), to
address these limitations. Specifcally, we reconsider object motion from
a new perspective and propose Directional Frequency Representation
(DFR), which not only borrows the property of frequency representation
of detail and structure information but also contains the direction
information of the object motion that is extremely significant in
videos. Based on this representation, we propose a Directional
Frequency-Enhanced Alignment (DFEA) to use double enhancements of
task-related information for ensuring the retention of high-fdelity
frequency regions to generate the high-quality alignment feature.
Furthermore, we design a novel Asymmetrical U-shaped network
architecture to progressively fuse these alignment features and output
the fnal output. This architecture enables the intercommunication of the
same level of resolution in the encoder and decoder to achieve the
supplement of spatial information. Powered by the above designs, our
method achieves superior performance over state-of-the-art models on
both quantitative and qualitative evaluations.</p>
</div>
<div class="note primary"><h2
id="acm-mm2023-adaptive-contrastive-learning-for-learning-robust-representations-under-label-noise"><a
target="_blank" rel="noopener" href="https://dl.acm.org/doi/abs/10.1145/3581783.3612491">ACM MM2023:
Adaptive Contrastive Learning for Learning Robust Representations under
Label Noise</a></h2>
<h3 id="author-王子浩">Author: 王子浩</h3>
<h3 id="abstract">Abstract</h3>
<p>Deep Neural Networks suffer significant performance degeneration when
noisy labels corrupt latent data representations. Previous work has
attempted to alleviate this problem by exploiting contrastive learning,
the pair building of which is critical. However, existing methods either
conduct sample-level processes and then use the resultant subset to
construct pairs or directly perform pair-level selecting using a fixed
threshold, both leading to sub-optimal pairing and subsequent
representation learning. To address this issue, we propose a novel
adaptive contrastive learning method (ACL) working at the pair level to
select contrastive pairs adaptively. Specifically, we consider the
model's learning status to adjust the confidence threshold in a
self-adaptive manner instead of fixing it. Then, towards the
ineffectiveness of the thresholding method on unconfident pairs, we
automatically apply instance-specific temperature to boost the
confidence of accurately-predicted samples and their pairs. We further
introduce temporal cross-ensembling to handle the impact of noisy labels
on model predictions. As a result, diverse pairs are correctly selected
for contrastive learning to induce discriminative representations robust
to various types of label noise. Extensive experimental results on
several standard benchmarks and real-world datasets indicate the
superiority of ACL, especially in extremely noisy scenarios.</p>
</div>
<div class="note primary"><h2
id="iccv2023-hiface-high-fidelity-3d-face-reconstruction-by-learning-static-and-dynamic-details"><a
target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/ICCV2023/html/Chai_HiFace_High-Fidelity_3D_Face_Reconstruction_by_Learning_Static_and_Dynamic_ICCV_2023_paper.html">ICCV2023:
HiFace: High-Fidelity 3D Face Reconstruction by Learning Static and
Dynamic Details</a></h2>
<h3 id="author-柴增豪">Author: 柴增豪</h3>
<h3 id="abstract">Abstract</h3>
<p>3D Morphable Models (3DMMs) demonstrate great potential for
reconstructing faithful and animatable 3D facial surfaces from a single
image. The facial surface is influenced by the coarse shape, as well as
the static detail (e,g., person-specific appearance) and dynamic detail
(e.g., expression-driven wrinkles). Previous work struggles to decouple
the static and dynamic details through image-level supervision, leading
to reconstructions that are not realistic. In this paper, we aim at
high-fidelity 3D face reconstruction and propose HiFace to explicitly
model the static and dynamic details. Specifically, the static detail is
modeled as the linear combination of a displacement basis, while the
dynamic detail is modeled as the linear interpolation of two
displacement maps with polarized expressions. We exploit several loss
functions to jointly learn the coarse shape and fine details with both
synthetic and real-world datasets, which enable HiFace to reconstruct
high-fidelity 3D shapes with animatable details. Extensive quantitative
and qualitative experiments demonstrate that HiFace presents
state-of-the-art reconstruction quality and faithfully recovers both the
static and dynamic details.</p>
</div>
<div class="note primary"><h2
id="iccv2023-accurate-3d-face-reconstruction-with-facial-component-tokens"><a
target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Accurate_3D_Face_Reconstruction_with_Facial_Component_Tokens_ICCV_2023_paper.html">ICCV2023:
Accurate 3D Face Reconstruction with Facial Component Tokens</a></h2>
<h3 id="author-章天珂">Author: 章天珂</h3>
<h3 id="abstract">Abstract</h3>
<p>Accurately reconstructing 3D faces from monocular images and videos
is crucial for various applications, such as digital avatar creation.
However, the current deep learning-based methods face significant
challenges in achieving accurate reconstruction with disentangled facial
parameters and ensuring temporal stability in single-frame methods for
3D face tracking on video data. In this paper, we propose TokenFace, a
transformer-based monocular 3D face reconstruction model. TokenFace uses
separate tokens for different facial components to capture information
about different facial parameters and employs temporal transformers to
capture temporal information from video data. This design can naturally
disentangle different facial components and is flexible to both 2D and
3D training data. Trained on hybrid 2D and 3D data, our model shows its
power in accurately reconstructing faces from images and producing
stable results for video data. Experimental results on popular
benchmarks NoW and Stirling demonstrate that TokenFace achieves
state-of-the-art performance, outperforming existing methods on all
metrics by a large margin.</p>
</div>
<div class="note primary"><h2
id="iccvw2023-effective-whole-body-pose-estimation-with-two-stages-distillation"><a
target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/ICCV2023W/CV4Metaverse/html/Yang_Effective_Whole-Body_Pose_Estimation_with_Two-Stages_Distillation_ICCVW_2023_paper.html">ICCVW2023:
Effective Whole-body Pose Estimation with Two-stages
Distillation</a></h2>
<h3 id="author-杨震东">Author: 杨震东</h3>
<h3 id="abstract">Abstract</h3>
<p>Whole-body pose estimation localizes the human body, hand, face, and
foot keypoints in an image. This task is challenging due to multi-scale
body parts, fine-grained localization for low-resolution regions, and
data scarcity. Meanwhile, applying a highly efficient and accurate pose
estimator to widely human-centric understanding and generation tasks is
urgent. In this work, we present a two-stage pose Distillation for
Whole-body Pose estimators, named DWPose, to improve their effectiveness
and efficiency. The first-stage distillation designs a weight-decay
strategy while utilizing a teacher's intermediate feature and final
logits with both visible and invisible keypoints to supervise the
student from scratch. The second stage distills the student model itself
to further improve performance. Different from the previous
self-knowledge distillation, this stage finetunes the student's head
with only 20% training time as a plug-and-play training strategy. For
data limitations, we explore the UBody dataset that contains diverse
facial expressions and hand gestures for real-life applications.
Comprehensive experiments show the superiority of our proposed simple
yet effective methods. We achieve new state-of-the-art performance on
COCO-WholeBody, significantly boosting the whole-body AP of RTMPose-l
from 64.8% to 66.5%, even surpassing RTMPose-x teacher with 65.3% AP. We
release a series of models with different sizes, from tiny to large, for
satisfying various downstream tasks. Our code and models are available
at https://github.com/IDEA-Research/DWPose.</p>
</div>
<div class="note primary"><h2
id="iccv2023-from-knowledge-distillation-to-self-knowledge-distillation-a-unified-approach-with-normalized-loss-and-customized-soft-labels"><a
target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/ICCV2023/html/Yang_From_Knowledge_Distillation_to_Self-Knowledge_Distillation_A_Unified_Approach_with_ICCV_2023_paper.html">ICCV2023:
From Knowledge Distillation to Self-Knowledge Distillation: A Unified
Approach with Normalized Loss and Customized Soft Labels</a></h2>
<h3 id="author-杨震东">Author: 杨震东</h3>
<h3 id="abstract">Abstract</h3>
<p>Knowledge Distillation (KD) uses the teacher's prediction logits as
soft labels to guide the student, while self-KD does not need a real
teacher to require the soft labels. This work unifies the formulations
of the two tasks by decomposing and reorganizing the generic KD loss
into a Normalized KD (NKD) loss and customized soft labels for both
target class (image's category) and non-target classes named Universal
Self-Knowledge Distillation (USKD). We decompose the KD loss and find
the non-target loss from it forces the student's non-target logits to
match the teacher's, but the sum of the two non-target logits is
different, preventing them from being identical. NKD normalizes the
non-target logits to equalize their sum. It can be generally used for KD
and self-KD to better use the soft labels for distillation loss. USKD
generates customized soft labels for both target and non-target classes
without a teacher. It smooths the target logit of the student as the
soft target label and uses the rank of the intermediate feature to
generate the soft non-target labels with Zipf's law. For KD with
teachers, our NKD achieves state-of-the-art performance on CIFAR-100 and
ImageNet datasets, boosting the ImageNet Top-1 accuracy of ResNet18 from
69.90% to 71.96% with a ResNet-34 teacher. For self-KD without teachers,
USKD is the first self-KD method that can be effectively applied to both
CNN and ViT models with negligible additional time and memory cost,
resulting in new state-of-the-art results, such as 1.17% and 0.55%
accuracy gains on ImageNet for MobileNet and DeiT-Tiny, respectively.
Code is available at https://github.com/yzd-v/cls_KD.</p>
</div>
<div class="note primary"><h2
id="iccv2023-make-encoder-great-again-in-3d-gan-inversion-through-geometry-and-occlusion-aware-encoding"><a
target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/ICCV2023/html/Yuan_Make_Encoder_Great_Again_in_3D_GAN_Inversion_through_Geometry_ICCV_2023_paper.html">ICCV2023:
Make Encoder Great Again in 3D GAN Inversion through Geometry and
Occlusion-Aware Encoding</a></h2>
<h3 id="author-袁梓洋">Author: 袁梓洋</h3>
<h3 id="abstract">Abstract</h3>
<p>3D GAN inversion aims to achieve high reconstruction fidelity and
reasonable 3D geometry simultaneously from a single image input.
However, existing 3D GAN inversion methods rely on time-consuming
optimization for each individual case. In this work, we introduce a
novel encoder-based inversion framework based on EG3D, one of the most
widely-used 3D GAN models. We leverage the inherent properties of EG3D's
latent space to design a discriminator and a background depth
regularization. This enables us to train a geometry-aware encoder
capable of converting the input image into corresponding latent code.
Additionally, we explore the feature space of EG3D and develop an
adaptive refinement stage that improves the representation ability of
features in EG3D to enhance the recovery of fine-grained textural
details. Finally, we propose an occlusion-aware fusion operation to
prevent distortion in unobserved regions. Our method achieves impressive
results comparable to optimization-based methods while operating up to
500 times faster. Our framework is well-suited for applications such as
semantic editing.</p>
</div>
<div class="note primary"><h2
id="icml2023-upop-unified-and-progressive-pruning-for-compressing-vision-language-transformers"><a
target="_blank" rel="noopener" href="https://proceedings.mlr.press/v202/shi23e.html">ICML2023: UPop:
Unified and Progressive Pruning for Compressing Vision-Language
Transformers</a></h2>
<h3 id="author-石大川">Author: 石大川</h3>
<h3 id="abstract">Abstract</h3>
<p>Real-world data contains a vast amount of multimodal information,
among which vision and language are the two most representative
modalities. Moreover, increasingly heavier models, e.g., Transformers,
have attracted the attention of researchers to model compression.
However, how to compress multimodal models, especially vison-language
Transformers, is still under-explored. This paper proposes the Unified
and Progressive Pruning (UPop) as a universal vison-language Transformer
compression framework, which incorporates 1) unifiedly searching
multimodal subnets in a continuous optimization space from the original
model, which enables automatic assignment of pruning ratios among
compressible modalities and structures; 2) progressively searching and
retraining the subnet, which maintains convergence between the search
and retrain to attain higher compression ratios. Experiments on various
tasks, datasets, and model architectures demonstrate the effectiveness
and versatility of the proposed UPop framework. The code is available at
https://github.com/sdc17/UPop.</p>
</div>
<div class="note primary"><h2
id="icml2023-learning-to-learn-from-apis-black-box-data-free-meta-learning"><a
target="_blank" rel="noopener" href="https://proceedings.mlr.press/v202/hu23g.html">ICML2023: Learning
to Learn from APIs: Black-Box Data-Free Meta-Learning</a></h2>
<h3 id="author-胡梓轩">Author: 胡梓轩</h3>
<h3 id="abstract">Abstract</h3>
<p>Data-free meta-learning (DFML) aims to enable efficient learning of
new tasks by meta-learning from a collection of pre-trained models
without access to the training data. Existing DFML work can only
meta-learn from (i) white-box and (ii) small-scale pre-trained models
(iii) with the same architecture, neglecting the more practical setting
where the users only have inference access to the APIs with arbitrary
model architectures and model scale inside. To solve this issue, we
propose a Bi-level Data-free Meta Knowledge Distillation (BiDf-MKD)
framework to transfer more general meta knowledge from a collection of
black-box APIs to one single meta model. Specifically, by just querying
APIs, we inverse each API to recover its training data via a zero-order
gradient estimator and then perform meta-learning via a novel bi-level
meta knowledge distillation structure, in which we design a boundary
query set recovery technique to recover a more informative query set
near the decision boundary. In addition, to encourage better
generalization within the setting of limited API budgets, we propose
task memory replay to diversify the underlying task distribution by
covering more interpolated tasks. Extensive experiments in various
real-world scenarios show the superior performance of our BiDf-MKD
framework.</p>
</div>
<div class="note primary"><h2
id="icra2023-aanet-aggregation-and-alignment-network-with-semi-hard-positive-sample-mining-for-hierarchical-place-recognition"><a
target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/10160734">ICRA2023:
AANet: Aggregation and Alignment Network with Semi-hard Positive Sample
Mining for Hierarchical Place Recognition</a></h2>
<h3 id="author-卢锋">Author: 卢锋</h3>
<h3 id="abstract">Abstract</h3>
<p>Visual place recognition (VPR) is one of the research hotspots in
robotics, which uses visual information to locate robots. Recently, the
hierarchical two-stage VPR methods have become popular in this field due
to the trade-off between accuracy and efficiency. These methods retrieve
the top-k candidate images using the global features in the first stage,
then re-rank the candidates by matching the local features in the second
stage. However, they usually require additional al-gorithms (e.g.
RANSAC) for geometric consistency verification in re-ranking, which is
time-consuming. Here we propose a Dynamically Aligning Local Features
(DALF) algorithm to align the local features under spatial constraints.
It is significantly more efficient than the methods that need geometric
consistency verification. We present a unified network capable of
extracting global features for retrieving candidates via an aggregation
module and aligning local features for re-ranking via the DALF alignment
module. We call this network AANet. Meanwhile, many works use the
simplest positive samples in triplet for weakly supervised training,
which limits the ability of the network to recognize harder positive
pairs. To address this issue, we propose a Semi-hard Positive Sample
Mining (ShPSM) strategy to select appropriate hard positive images for
training more robust VPR networks. Extensive experiments on four
benchmark VPR datasets show that the proposed AANet can outperform
several state-of-the-art methods with less time consumption. The code is
released at https://github.com/Lu-Feng/AANet.</p>
</div>
<div class="note primary"><h2
id="cvpr2023-high-fidelity-facial-avatar-reconstruction-from-monocular-video-with-generative-priors"><a
target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2023/html/Bai_High-Fidelity_Facial_Avatar_Reconstruction_From_Monocular_Video_With_Generative_Priors_CVPR_2023_paper.html">CVPR2023:
High-fidelity Facial Avatar Reconstruction from Monocular Video with
Generative Priors</a></h2>
<h3 id="author-白云鹏">Author: 白云鹏</h3>
<h3 id="abstract">Abstract</h3>
<p>High-fidelity facial avatar reconstruction from a monocular video is
a significant research problem in computer graphics and computer vision.
Recently, Neural Radiance Field (NeRF) has shown impressive novel view
rendering results and has been considered for facial avatar
reconstruction. However, the complex facial dynamics and missing 3D
information in monocular videos raise significant challenges for
faithful facial reconstruction. In this work, we propose a new method
for NeRF-based facial avatar reconstruction that utilizes 3D-aware
generative prior. Different from existing works that depend on a
conditional deformation field for dynamic modeling, we propose to learn
a personalized generative prior, which is formulated as a local and low
dimensional subspace in the latent space of 3D-GAN. We propose an
efficient method to construct the personalized generative prior based on
a small set of facial images of a given individual. After learning, it
allows for photo-realistic rendering with novel views, and the face
reenactment can be realized by performing navigation in the latent
space. Our proposed method is applicable for different driven signals,
including RGB images, 3DMM coefficients, and audio. Compared with
existing works, we obtain superior novel view synthesis results and
faithfully face reenactment performance. The code is available here
https://github.com/bbaaii/HFA-GP.</p>
</div>
<div class="note primary"><h2 id="cvpr2023-learning-imbalanced-data-with-vision-transformers"><a
target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2023/html/Xu_Learning_Imbalanced_Data_With_Vision_Transformers_CVPR_2023_paper.html">CVPR2023:
Learning Imbalanced Data with Vision Transformers</a></h2>
<h3 id="author-许正卓">Author: 许正卓</h3>
<h3 id="abstract">Abstract</h3>
<p>The real-world data tends to be heavily imbalanced and severely skew
the data-driven deep neural networks, which makes Long-Tailed
Recognition (LTR) a massive challenging task. Existing LTR methods
seldom train Vision Transformers (ViTs) with Long-Tailed (LT) data,
while the off-the-shelf pretrain weight of ViTs always leads to unfair
comparisons. In this paper, we systematically investigate the ViTs'
performance in LTR and propose LiVT to train ViTs from scratch only with
LT data. With the observation that ViTs suffer more severe LTR problems,
we conduct Masked Generative Pretraining (MGP) to learn generalized
features. With ample and solid evidence, we show that MGP is more robust
than supervised manners. Although Binary Cross Entropy (BCE) loss
performs well with ViTs, it struggles on the LTR tasks. We further
propose the balanced BCE to ameliorate it with strong theoretical
groundings. Specially, we derive the unbiased extension of Sigmoid and
compensate extra logit margins for deploying it. Our Bal-BCE contributes
to the quick convergence of ViTs in just a few epochs. Extensive
experiments demonstrate that with MGP and Bal-BCE, LiVT successfully
trains ViTs well without any additional data and outperforms comparable
state-of-the-art methods significantly, e.g., our ViT-B achieves 81.0%
Top-1 accuracy in iNaturalist 2018 without bells and whistles. Code is
available at https://github.com/XuZhengzhuo/LiVT.</p>
</div>
<div class="note primary"><h2
id="ieee-tcsvt-task-adaptive-feature-disentanglement-and-hallucination-for-few-shot-classification"><a
target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/10024322">IEEE
TCSVT: Task-adaptive Feature Disentanglement and Hallucination for
Few-shot Classification</a></h2>
<h3 id="author-胡梓轩">Author: 胡梓轩</h3>
<h3 id="abstract">Abstract</h3>
<p>Few-shot classification is a challenging task of computer vision and
is critical to the data-sparse scenario like rare disease diagnosis.
Feature augmentation is a straightforward way to alleviate the
data-sparse issue in few-shot classification. However, mimicking the
original feature distribution from a small amount of data is
challenging. Existing augmentation-based methods are task-agnostic: the
augmented feature is not with optimal intra-class diversity and
inter-class discriminability concerning a certain task. To address this
drawback, we propose a novel Task-adaptive Feature Disentanglement and
Hallucination framework, dubbed TaFDH. Concretely, we first perceive the
task information to disentangle the original feature into two
components: class-irrelevant and class-specific features. Then more
class-irrelevant features are decoded from a learned variational
distribution, fused with the class-specific feature to get the augmented
features. Finally, a generalized prior distribution over a quadratic
classifier is meta-learned, which can be fast adapted to the
class-specific posterior, thus further alleviating the inadequacy and
uncertainty of feature hallucination via the nature of Bayesian
inference. In this way, we construct a more discriminable embedding
space with reasonable intra-class diversity instead of simply restoring
the original embedding space, which can lead to a more precise decision
boundary. We obtain the augmented features equipped with enhanced
inter-class discriminability by highlighting the most discriminable part
while boosting the intra-class diversity by fusing with the diverse
generated class-irrelevant parts. Experiments on five multi-grained
few-shot classification datasets demonstrate the superiority of our
method.</p>
</div>
<div class="note primary"><h2
id="cvpr2023-architecture-dataset-and-model-scale-agnostic-data-free-meta-learning"><a
target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2023/html/Hu_Architecture_Dataset_and_Model-Scale_Agnostic_Data-Free_Meta-Learning_CVPR_2023_paper.html">CVPR2023:
Architecture, Dataset and Model-Scale Agnostic Data-free
Meta-Learning</a></h2>
<h3 id="author-胡梓轩">Author: 胡梓轩</h3>
<h3 id="abstract">Abstract</h3>
<p>The goal of data-free meta-learning is to learn useful prior
knowledge from a collection of pre-trained models without accessing
their training data. However, existing works only solve the problem in
parameter space, which (i) ignore the fruitful data knowledge contained
in the pre-trained models; (ii) can not scale to large-scale pre-trained
models; (iii) can only meta-learn pre-trained models with the same
network architecture. To address those issues, we propose a unified
framework, dubbed PURER, which contains: (1) ePisode cUrriculum
inveRsion (ECI) during data-free meta training; and (2) invErsion
calibRation following inner loop (ICFIL) during meta testing. During
meta training, we propose ECI to perform pseudo episode training for
learning to adapt fast to new unseen tasks. Specifically, we
progressively synthesize a sequence of pseudo episodes by distilling the
training data from each pre-trained model. The ECI adaptively increases
the difficulty level of pseudo episodes according to the real-time
feedback of the meta model. We formulate the optimization process of
meta training with ECI as an adversarial form in an end-to-end manner.
During meta testing, we further propose a simple plug-and-play
supplement--ICFIL--only used during meta testing to narrow the gap
between meta training and meta testing task distribution. Extensive
experiments in various real-world scenarios show the superior
performance of ours.</p>
</div>
<div class="note primary"><h2
id="cvpr2023-uni-perceiver-v2-a-generalist-model-for-large-scale-vision-and-vision-language-tasks"><a
target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2023/html/Li_Uni-Perceiver_v2_A_Generalist_Model_for_Large-Scale_Vision_and_Vision-Language_CVPR_2023_paper.html">CVPR2023:
Uni-Perceiver v2: A Generalist Model for Large-Scale Vision and
Vision-Language Tasks</a></h2>
<h3 id="author-江晓湖">Author: 江晓湖</h3>
<h3 id="abstract">Abstract</h3>
<p>Despite the remarkable success of foundation models, their
task-specific fine-tuning paradigm makes them inconsistent with the goal
of general perception modeling. The key to eliminating this
inconsistency is to use generalist models for general task modeling.
However, existing attempts at generalist models are inadequate in both
versatility and performance. In this paper, we propose Uni-Perceiver v2,
which is the first generalist model capable of handling major
large-scale vision and vision-language tasks with competitive
performance. Specifically, images are encoded as general region
proposals, while texts are encoded via a Transformer-based language
model. The encoded representations are transformed by a task-agnostic
decoder. Different tasks are formulated as a unified maximum likelihood
estimation problem. We further propose an effective optimization
technique named Task-Balanced Gradient Normalization to ensure stable
multi-task learning with an unmixed sampling strategy, which is helpful
for tasks requiring large batch-size training. After being jointly
trained on various tasks, Uni-Perceiver v2 is capable of directly
handling downstream tasks without any task-specific adaptation. Results
show that Uni-Perceiver v2 outperforms all existing generalist models in
both versatility and performance. Meanwhile, compared with the
commonly-recognized strong baselines that require tasks-specific
fine-tuning, Uni-Perceiver v2 achieves competitive performance on a
broad range of vision and vision-language tasks.</p>
</div>
<div class="note primary"><h2
id="icassp2023-frequency-reciprocal-action-and-fusion-for-single-image-super-resolution"><a
target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/10096598">ICASSP2023:
FREQUENCY RECIPROCAL ACTION AND FUSION FOR SINGLE IMAGE
SUPER-RESOLUTION</a></h2>
<h3 id="author-董姝婷">Author: 董姝婷</h3>
<h3 id="abstract">Abstract</h3>
<p>Frequency-based methods have recently received much attention due to
their impressive restoration of detail and structure in single image
super-resolution (SISR). However, most of these methods mainly use
frequency information as auxiliary means but ignore exploring the
correlations and pixel distribution differences among various
frequencies. To address the limitations, we propose a novel Frequency
Reciprocal Action and Fusion Network (FRAF) that explores various
frequency correlations and differences. Specifically, we design a
Frequency Reciprocal Action (FRA) module, which safely enhances valid
spatial information and decreases un-necessary repetition by reciprocal
action among various spatial frequencies, to generate refined high- and
low-frequency features. These refined frequency features are then
progressively to guide the details and structure recovery, respectively.
Furthermore, we develop a Detail and Structure Fusion (DSF) module to
adaptively select, enhance and fuse the features to output the final HR
image. This way ensures the final image is a high-quality product with
rich details and a clear structure. Experimental results demonstrate
that our method achieves superior performance over state-of-the-art
(SOTA) approaches on both quantitative and qualitative evaluations.</p>
</div>
<div class="note primary"><h2
id="icassp2023-rethink-long-tailed-recognition-with-vision-transforms"><a
target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/10097154">ICASSP2023:
Rethink Long-Tailed Recognition With Vision Transforms</a></h2>
<h3 id="author-许正卓">Author: 许正卓</h3>
<h3 id="abstract">Abstract</h3>
<p>In the real world, data tends to follow long-tailed distributions
w.r.t. class or attribution, motivating the challenging Long-Tailed
Recognition (LTR) problem. In this paper, we revisit recent LTR methods
with promising Vision Transformers (ViT). We figure out that 1) ViT is
hard to train with longtailed data. 2) ViT learns generalized features
in an unsupervised manner, like mask generative training, either on
longtailed or balanced datasets. Hence, we propose to adopt unsupervised
learning to utilize long-tailed data. Furthermore, we propose the
Predictive Distribution Calibration (PDC) as a novel metric for LTR,
where the model tends to simply classify inputs into common classes. Our
PDC can measure the model calibration of predictive preferences
quantitatively. On this basis, we find many LTR approaches alleviate it
slightly, despite the accuracy improvement. Extensive experiments on
benchmark datasets validate that PDC reflects the model’s predictive
preference precisely, which is consistent with the visualization.</p>
</div>
<div class="note primary"><h2
id="ieee-tnnls-patchnet-maximize-the-exploration-of-congeneric-semantics-for-weakly-supervised-semantic-segmentation"><a
target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/10153071">IEEE
TNNLS: PatchNet: Maximize the Exploration of Congeneric Semantics for
Weakly Supervised Semantic Segmentation</a></h2>
<h3 id="author-张可">Author: 张可</h3>
<h3 id="abstract">Abstract</h3>
<p>With the increase in the number of image data and the lack of
corresponding labels, weakly supervised learning has drawn a lot of
attention recently in computer vision tasks, especially in the
fine-grained semantic segmentation problem. To alleviate human efforts
from expensive pixel-by-pixel annotations, our method focuses on weakly
supervised semantic segmentation (WSSS) with image-level labels, which
are much easier to obtain. As a considerable gap exists between
pixel-level segmentation and image-level labels, how to reflect the
image-level semantic information on each pixel is an important question.
To explore the congeneric semantic regions from the same class to the
maximum, we construct the patch-level semantic augmentation network
(PatchNet) based on the self-detected patches from different images that
contain the same class labels. Patches can frame the objects as much as
possible and include as little background as possible. The patch-level
semantic augmentation network that is established with patches as the
nodes can maximize the mutual learning of similar objects. We regard the
embedding vectors of patches as nodes and use a transformer-based
complementary learning module to construct weighted edges according to
the embedding similarity between different nodes. Moreover, to better
supplement semantic information, we propose softcomplementary loss
functions matched with the whole network structure. We conduct
experiments on the popular PASCAL VOC 2012 and MS COCO 2014 benchmarks,
and our model yields the state-of-the-art performance.</p>
</div>
<div class="note primary"><h2
id="aaai2023-truncate-split-contrast-a-framework-for-learning-from-mislabeled-videos"><a
target="_blank" rel="noopener" href="https://ojs.aaai.org/index.php/AAAI/article/view/25375">AAAI2023:
Truncate-Split-Contrast: A Framework for Learning from Mislabeled
Videos</a></h2>
<h3 id="author-王子啸">Author: 王子啸</h3>
<h3 id="abstract">Abstract</h3>
<p>Learning with noisy label is a classic problem that has been
extensively studied for image tasks, but much less for video in the
literature. A straightforward migration from images to videos without
considering temporal semantics and computational cost is not a sound
choice. In this paper, we propose two new strategies for video analysis
with noisy labels: 1) a lightweight channel selection method dubbed as
Channel Truncation for feature-based label noise detection. This method
selects the most discriminative channels to split clean and noisy
instances in each category. 2) A novel contrastive strategy dubbed as
Noise Contrastive Learning, which constructs the relationship between
clean and noisy instances to regularize model training. Experiments on
three well-known benchmark datasets for video classification show that
our proposed truNcatE-split-contrAsT (NEAT) significantly outperforms
the existing baselines. By reducing the dimension to 10% of it, our
method achieves over 0.4 noise detection F1-score and 5% classification
accuracy improvement on Mini-Kinetics dataset under severe noise
(symmetric-80%). Thanks to Noise Contrastive Learning, the average
classification accuracy improvement on Mini-Kinetics and Sth-Sth-V1 is
over 1.6%.</p>
</div>
<div class="note primary"><h2
id="aaai2023-darwinian-model-upgrades-model-evolving-with-selective-compatibility"><a
target="_blank" rel="noopener" href="https://ojs.aaai.org/index.php/AAAI/article/view/25447">AAAI2023:
Darwinian Model Upgrades: Model Evolving with Selective
Compatibility</a></h2>
<h3 id="author-张斌杰">Author: 张斌杰</h3>
<h3 id="abstract">Abstract</h3>
<p>The traditional model upgrading paradigm for retrieval requires
recomputing all gallery embeddings before deploying the new model
(dubbed as "backfilling"), which is quite expensive and time-consuming
considering billions of instances in industrial applications. BCT
presents the first step towards backward-compatible model upgrades to
get rid of backfilling. It is workable but leaves the new model in a
dilemma between new feature discriminativeness and new-to-old
compatibility due to the undifferentiated compatibility constraints. In
this work, we propose Darwinian Model Upgrades (DMU), which disentangle
the inheritance and variation in the model evolving with selective
backward compatibility and forward adaptation, respectively. The
old-to-new heritable knowledge is measured by old feature
discriminativeness, and the gallery features, especially those of poor
quality, are evolved in a lightweight manner to become more adaptive in
the new latent space. We demonstrate the superiority of DMU through
comprehensive experiments on large-scale landmark retrieval and face
recognition benchmarks. DMU effectively alleviates the new-to-new
degradation at the same time improving new-to-old compatibility,
rendering a more proper model upgrading paradigm in large-scale
retrieval systems.Code:
https://github.com/TencentARC/OpenCompatible.</p>
</div>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="followme">
  <span>Welcome to my other publishing channels</span>

  <div class="social-list">

      <div class="social-item">
          <a target="_blank" class="social-link" href="https://scholar.google.com/citations?hl=en&user=fYdxi2sAAAAJ">
            <span class="icon">
              <i class=""></i>
            </span>

            <span class="label">Google Scholar</span>
          </a>
      </div>
  </div>
</div>


        

    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">袁春</span>
  </div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>







  





</body>
</html>
